{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abacofs import ABACOFeatureSelector\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.294244  0.705043  0.183047  0.610883  0.287570  0.806475  0.179009   \n",
      "1  0.315761  0.143673  0.263651  0.559548  0.330241  0.630611  0.174344   \n",
      "2  0.586875  0.397716  0.413937  0.667351  0.285714  0.785136  0.078134   \n",
      "3  0.481442  0.255947  0.506500  0.377823  0.270872  0.506990  0.232653   \n",
      "4  0.603012  0.574691  0.433177  0.394251  0.319109  0.431199  0.274052   \n",
      "\n",
      "          7         8         9  ...        61        62        63        64  \\\n",
      "0  0.499009  0.890763  0.184259  ...  0.378264  0.459459  0.300824  0.339323   \n",
      "1  0.522299  0.433735  0.462037  ...  0.606916  0.529530  0.333791  0.389081   \n",
      "2  0.570367  0.506827  0.612037  ...  0.448836  0.623624  0.515110  0.374568   \n",
      "3  0.484143  0.623293  0.378704  ...  0.576570  0.377377  0.220467  0.291638   \n",
      "4  0.471754  0.575100  0.686111  ...  0.486944  0.438438  0.695055  0.379406   \n",
      "\n",
      "         65        66        67        68        69  label  \n",
      "0  0.180758  0.348815  0.324151  0.526825  0.128294    0.0  \n",
      "1  0.332945  0.448341  0.433056  0.543741  0.221914    0.0  \n",
      "2  0.360933  0.509005  0.315823  0.435476  0.147712    0.0  \n",
      "3  0.143440  0.403791  0.410634  0.517641  0.126907    0.0  \n",
      "4  0.456560  0.371564  0.553491  0.523925  0.415395    1.0  \n",
      "\n",
      "[5 rows x 71 columns]\n"
     ]
    }
   ],
   "source": [
    "# ACO CALL\n",
    "fs = ABACOFeatureSelector(dtype='csv', data_training_name='./rtfDataSet.csv',\n",
    "                          numberAnts=20, iterations=30, n_features=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colony 0 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [57, 41, 36, 49, 37, 16, 63, 11, 44, 59, 10, 51, 38, 30, 29]\n",
      "\t\tAccuracy: 0.6970621468926554\n",
      "\tAnt 1 :\n",
      "\t\tPath: [21, 49, 65, 58, 38, 25, 44, 57, 43, 28, 9, 63, 12, 4, 10]\n",
      "\t\tAccuracy: 0.7072881355932203\n",
      "\tAnt 2 :\n",
      "\t\tPath: [19, 11, 67, 57, 53, 52, 16, 23, 51, 9, 18, 29, 5, 31, 63]\n",
      "\t\tAccuracy: 0.683502824858757\n",
      "\tAnt 3 :\n",
      "\t\tPath: [12, 42, 25, 49, 9, 29, 63, 16, 10, 59, 51, 57, 41, 37, 6]\n",
      "\t\tAccuracy: 0.6935593220338984\n",
      "\tAnt 4 :\n",
      "\t\tPath: [18, 59, 49, 11, 32, 14, 60, 65, 6, 43, 37, 19, 27, 9, 15]\n",
      "\t\tAccuracy: 0.7205649717514124\n",
      "\tAnt 5 :\n",
      "\t\tPath: [11, 49, 10, 25, 27, 59, 6, 22, 24, 12, 65, 34, 67, 44, 54]\n",
      "\t\tAccuracy: 0.6868361581920903\n",
      "\tAnt 6 :\n",
      "\t\tPath: [12, 18, 44, 67, 63, 53, 52, 30, 50, 65, 19, 10, 64, 59, 8]\n",
      "\t\tAccuracy: 0.6431073446327684\n",
      "\tAnt 7 :\n",
      "\t\tPath: [51, 10, 59, 28, 63, 22, 33, 53, 41, 12, 0, 30, 58, 11, 50]\n",
      "\t\tAccuracy: 0.6697175141242938\n",
      "\tAnt 8 :\n",
      "\t\tPath: [49, 44, 42, 65, 67, 36, 57, 14, 37, 12, 18, 3, 59, 9, 52]\n",
      "\t\tAccuracy: 0.6698305084745763\n",
      "\tAnt 9 :\n",
      "\t\tPath: [6, 51, 48, 68, 53, 34, 43, 58, 28, 19, 27, 15, 44, 12, 56]\n",
      "\t\tAccuracy: 0.6630508474576271\n",
      "\tAnt 10 :\n",
      "\t\tPath: [33, 2, 11, 23, 12, 10, 5, 28, 52, 50, 8, 18, 38, 61, 30]\n",
      "\t\tAccuracy: 0.7105084745762712\n",
      "\tAnt 11 :\n",
      "\t\tPath: [10, 9, 59, 65, 49, 37, 38, 14, 30, 11, 19, 21, 6, 20, 41]\n",
      "\t\tAccuracy: 0.6968926553672316\n",
      "\tAnt 12 :\n",
      "\t\tPath: [51, 42, 23, 57, 9, 8, 58, 40, 56, 10, 53, 48, 3, 38, 67]\n",
      "\t\tAccuracy: 0.6668926553672316\n",
      "\tAnt 13 :\n",
      "\t\tPath: [58, 63, 9, 67, 65, 29, 49, 2, 11, 27, 10, 51, 38, 43, 8]\n",
      "\t\tAccuracy: 0.676949152542373\n",
      "\tAnt 14 :\n",
      "\t\tPath: [13, 42, 50, 53, 48, 63, 67, 55, 57, 60, 19, 47, 37, 49, 12]\n",
      "\t\tAccuracy: 0.7003389830508474\n",
      "\tAnt 15 :\n",
      "\t\tPath: [5, 10, 53, 15, 9, 37, 44, 28, 58, 52, 7, 67, 61, 6, 51]\n",
      "\t\tAccuracy: 0.6801129943502826\n",
      "\tAnt 16 :\n",
      "\t\tPath: [11, 37, 63, 2, 57, 9, 10, 38, 16, 67, 14, 40, 0, 46, 44]\n",
      "\t\tAccuracy: 0.7072881355932205\n",
      "\tAnt 17 :\n",
      "\t\tPath: [9, 30, 16, 63, 57, 67, 25, 49, 14, 11, 18, 37, 59, 12, 36]\n",
      "\t\tAccuracy: 0.6936723163841808\n",
      "\tAnt 18 :\n",
      "\t\tPath: [31, 11, 51, 10, 63, 20, 9, 44, 22, 16, 5, 30, 7, 40, 14]\n",
      "\t\tAccuracy: 0.6868926553672317\n",
      "\tAnt 19 :\n",
      "\t\tPath: [63, 59, 24, 33, 19, 67, 34, 36, 14, 44, 42, 11, 54, 25, 60]\n",
      "\t\tAccuracy: 0.7036158192090395\n",
      "Colony 1 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [56, 38, 34, 31, 57, 2, 44, 52, 18, 25, 42, 50, 28, 33, 48]\n",
      "\t\tAccuracy: 0.7204519774011299\n",
      "\tAnt 1 :\n",
      "\t\tPath: [9, 63, 37, 10, 20, 65, 59, 30, 47, 57, 15, 12, 38, 16, 43]\n",
      "\t\tAccuracy: 0.7034463276836158\n",
      "\tAnt 2 :\n",
      "\t\tPath: [56, 59, 24, 30, 67, 38, 44, 6, 33, 14, 8, 37, 68, 11, 43]\n",
      "\t\tAccuracy: 0.7206214689265537\n",
      "\tAnt 3 :\n",
      "\t\tPath: [48, 67, 38, 53, 11, 50, 15, 59, 21, 46, 41, 49, 37, 19, 29]\n",
      "\t\tAccuracy: 0.6833898305084746\n",
      "\tAnt 4 :\n",
      "\t\tPath: [28, 25, 50, 67, 9, 49, 12, 38, 53, 10, 5, 43, 37, 22, 34]\n",
      "\t\tAccuracy: 0.7074576271186441\n",
      "\tAnt 5 :\n",
      "\t\tPath: [18, 61, 63, 6, 42, 45, 58, 8, 67, 28, 16, 14, 44, 51, 19]\n",
      "\t\tAccuracy: 0.6766101694915254\n",
      "\tAnt 6 :\n",
      "\t\tPath: [37, 8, 49, 52, 47, 59, 60, 15, 11, 67, 50, 38, 34, 10, 43]\n",
      "\t\tAccuracy: 0.7138983050847457\n",
      "\tAnt 7 :\n",
      "\t\tPath: [46, 19, 65, 14, 43, 50, 12, 63, 44, 52, 31, 59, 41, 8, 47]\n",
      "\t\tAccuracy: 0.7033333333333334\n",
      "\tAnt 8 :\n",
      "\t\tPath: [30, 57, 38, 10, 54, 34, 37, 59, 42, 46, 60, 31, 52, 19, 6]\n",
      "\t\tAccuracy: 0.6905084745762711\n",
      "\tAnt 9 :\n",
      "\t\tPath: [30, 42, 27, 32, 28, 58, 67, 52, 65, 33, 29, 16, 14, 31, 25]\n",
      "\t\tAccuracy: 0.6563276836158193\n",
      "\tAnt 10 :\n",
      "\t\tPath: [63, 50, 65, 13, 6, 37, 43, 38, 28, 51, 49, 10, 16, 29, 33]\n",
      "\t\tAccuracy: 0.697231638418079\n",
      "\tAnt 11 :\n",
      "\t\tPath: [9, 54, 15, 21, 18, 58, 20, 41, 59, 7, 16, 10, 38, 63, 49]\n",
      "\t\tAccuracy: 0.7038418079096045\n",
      "\tAnt 12 :\n",
      "\t\tPath: [57, 52, 25, 18, 63, 28, 59, 12, 37, 10, 22, 67, 43, 38, 29]\n",
      "\t\tAccuracy: 0.7038983050847458\n",
      "\tAnt 13 :\n",
      "\t\tPath: [52, 27, 31, 11, 25, 49, 15, 51, 34, 43, 50, 53, 40, 19, 2]\n",
      "\t\tAccuracy: 0.6970621468926553\n",
      "\tAnt 14 :\n",
      "\t\tPath: [37, 8, 15, 57, 7, 42, 41, 25, 34, 2, 52, 14, 27, 12, 9]\n",
      "\t\tAccuracy: 0.6868361581920903\n",
      "\tAnt 15 :\n",
      "\t\tPath: [32, 44, 10, 30, 49, 59, 14, 16, 58, 53, 51, 31, 18, 11, 43]\n",
      "\t\tAccuracy: 0.6971751412429379\n",
      "\tAnt 16 :\n",
      "\t\tPath: [38, 8, 14, 60, 68, 57, 51, 59, 67, 33, 46, 27, 63, 11, 6]\n",
      "\t\tAccuracy: 0.6869491525423729\n",
      "\tAnt 17 :\n",
      "\t\tPath: [29, 53, 28, 16, 43, 10, 42, 38, 67, 59, 18, 49, 65, 33, 14]\n",
      "\t\tAccuracy: 0.7005649717514123\n",
      "\tAnt 18 :\n",
      "\t\tPath: [12, 49, 44, 22, 65, 25, 56, 50, 9, 36, 57, 63, 5, 20, 27]\n",
      "\t\tAccuracy: 0.7408474576271187\n",
      "\tAnt 19 :\n",
      "\t\tPath: [6, 9, 59, 49, 37, 12, 48, 67, 44, 28, 50, 57, 10, 29, 43]\n",
      "\t\tAccuracy: 0.7473446327683616\n",
      "Colony 2 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [53, 2, 16, 36, 22, 0, 19, 57, 52, 49, 50, 67, 60, 18, 12]\n",
      "\t\tAccuracy: 0.6970056497175141\n",
      "\tAnt 1 :\n",
      "\t\tPath: [53, 26, 51, 6, 16, 12, 42, 2, 67, 19, 10, 30, 44, 63, 20]\n",
      "\t\tAccuracy: 0.6732768361581921\n",
      "\tAnt 2 :\n",
      "\t\tPath: [51, 50, 65, 67, 59, 10, 44, 42, 18, 9, 41, 19, 63, 43, 6]\n",
      "\t\tAccuracy: 0.666497175141243\n",
      "\tAnt 3 :\n",
      "\t\tPath: [38, 21, 65, 42, 22, 59, 9, 12, 34, 44, 7, 11, 57, 63, 48]\n",
      "\t\tAccuracy: 0.6463276836158192\n",
      "\tAnt 4 :\n",
      "\t\tPath: [36, 16, 56, 51, 67, 19, 22, 25, 2, 37, 50, 58, 23, 28, 52]\n",
      "\t\tAccuracy: 0.6799999999999999\n",
      "\tAnt 5 :\n",
      "\t\tPath: [22, 16, 59, 57, 14, 9, 30, 25, 19, 8, 49, 11, 42, 52, 24]\n",
      "\t\tAccuracy: 0.7438983050847459\n",
      "\tAnt 6 :\n",
      "\t\tPath: [68, 12, 44, 8, 38, 18, 47, 42, 6, 10, 50, 57, 11, 30, 19]\n",
      "\t\tAccuracy: 0.6665536723163842\n",
      "\tAnt 7 :\n",
      "\t\tPath: [42, 13, 57, 0, 50, 53, 10, 16, 30, 23, 34, 65, 12, 46, 28]\n",
      "\t\tAccuracy: 0.7102824858757062\n",
      "\tAnt 8 :\n",
      "\t\tPath: [54, 63, 11, 38, 25, 49, 29, 16, 14, 67, 51, 57, 19, 65, 12]\n",
      "\t\tAccuracy: 0.6496610169491526\n",
      "\tAnt 9 :\n",
      "\t\tPath: [54, 65, 57, 63, 19, 18, 51, 50, 16, 8, 38, 9, 46, 43, 67]\n",
      "\t\tAccuracy: 0.6936158192090395\n",
      "\tAnt 10 :\n",
      "\t\tPath: [16, 20, 33, 53, 6, 68, 63, 36, 21, 67, 60, 59, 25, 57, 14]\n",
      "\t\tAccuracy: 0.7174011299435028\n",
      "\tAnt 11 :\n",
      "\t\tPath: [47, 57, 59, 51, 63, 21, 52, 28, 44, 9, 54, 12, 38, 43, 37]\n",
      "\t\tAccuracy: 0.7242372881355933\n",
      "\tAnt 12 :\n",
      "\t\tPath: [52, 9, 60, 59, 65, 53, 10, 27, 37, 48, 69, 35, 50, 23, 68]\n",
      "\t\tAccuracy: 0.693502824858757\n",
      "\tAnt 13 :\n",
      "\t\tPath: [2, 58, 9, 59, 22, 8, 52, 56, 11, 15, 60, 14, 10, 57, 38]\n",
      "\t\tAccuracy: 0.7105084745762712\n",
      "\tAnt 14 :\n",
      "\t\tPath: [16, 2, 10, 22, 42, 59, 63, 14, 43, 13, 37, 60, 6, 68, 57]\n",
      "\t\tAccuracy: 0.7372881355932203\n",
      "\tAnt 15 :\n",
      "\t\tPath: [14, 65, 31, 63, 54, 43, 40, 6, 24, 67, 25, 38, 44, 10, 53]\n",
      "\t\tAccuracy: 0.6868361581920903\n",
      "\tAnt 16 :\n",
      "\t\tPath: [38, 53, 67, 50, 14, 65, 27, 52, 12, 48, 63, 37, 54, 57, 0]\n",
      "\t\tAccuracy: 0.6666666666666666\n",
      "\tAnt 17 :\n",
      "\t\tPath: [63, 11, 67, 29, 50, 57, 12, 30, 48, 27, 52, 68, 49, 38, 9]\n",
      "\t\tAccuracy: 0.6834463276836158\n",
      "\tAnt 18 :\n",
      "\t\tPath: [13, 2, 57, 16, 34, 63, 42, 52, 19, 44, 31, 15, 14, 67, 11]\n",
      "\t\tAccuracy: 0.6465536723163842\n",
      "\tAnt 19 :\n",
      "\t\tPath: [23, 51, 65, 44, 50, 15, 2, 63, 9, 42, 8, 29, 57, 38, 22]\n",
      "\t\tAccuracy: 0.6900564971751412\n",
      "Colony 3 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [19, 50, 22, 10, 9, 65, 0, 38, 43, 54, 59, 7, 56, 11, 64]\n",
      "\t\tAccuracy: 0.7003389830508475\n",
      "\tAnt 1 :\n",
      "\t\tPath: [67, 18, 46, 50, 57, 22, 38, 0, 65, 11, 9, 59, 63, 58, 8]\n",
      "\t\tAccuracy: 0.6966666666666667\n",
      "\tAnt 2 :\n",
      "\t\tPath: [48, 51, 55, 10, 12, 11, 28, 59, 21, 37, 19, 18, 57, 25, 60]\n",
      "\t\tAccuracy: 0.6701694915254237\n",
      "\tAnt 3 :\n",
      "\t\tPath: [59, 16, 57, 15, 43, 12, 36, 63, 13, 58, 19, 23, 37, 11, 5]\n",
      "\t\tAccuracy: 0.6903954802259887\n",
      "\tAnt 4 :\n",
      "\t\tPath: [33, 14, 57, 15, 38, 63, 2, 16, 9, 10, 67, 6, 12, 46, 54]\n",
      "\t\tAccuracy: 0.7105649717514124\n",
      "\tAnt 5 :\n",
      "\t\tPath: [14, 10, 18, 38, 60, 56, 67, 19, 12, 37, 34, 30, 63, 44, 46]\n",
      "\t\tAccuracy: 0.6968926553672317\n",
      "\tAnt 6 :\n",
      "\t\tPath: [59, 18, 14, 63, 65, 22, 12, 10, 30, 44, 28, 9, 41, 6, 53]\n",
      "\t\tAccuracy: 0.6698305084745761\n",
      "\tAnt 7 :\n",
      "\t\tPath: [10, 34, 65, 50, 33, 12, 56, 37, 9, 42, 59, 63, 53, 31, 8]\n",
      "\t\tAccuracy: 0.6902824858757063\n",
      "\tAnt 8 :\n",
      "\t\tPath: [30, 14, 18, 63, 60, 7, 65, 56, 15, 25, 10, 11, 59, 27, 37]\n",
      "\t\tAccuracy: 0.7175141242937852\n",
      "\tAnt 9 :\n",
      "\t\tPath: [11, 24, 36, 19, 25, 22, 57, 16, 12, 31, 42, 30, 68, 18, 43]\n",
      "\t\tAccuracy: 0.6933898305084746\n",
      "\tAnt 10 :\n",
      "\t\tPath: [12, 2, 29, 63, 67, 68, 49, 60, 40, 25, 36, 30, 13, 15, 14]\n",
      "\t\tAccuracy: 0.6463276836158192\n",
      "\tAnt 11 :\n",
      "\t\tPath: [59, 30, 50, 65, 25, 16, 11, 15, 14, 67, 52, 57, 40, 64, 27]\n",
      "\t\tAccuracy: 0.6867796610169491\n",
      "\tAnt 12 :\n",
      "\t\tPath: [37, 58, 11, 8, 59, 53, 50, 38, 57, 10, 21, 14, 23, 49, 44]\n",
      "\t\tAccuracy: 0.7001129943502825\n",
      "\tAnt 13 :\n",
      "\t\tPath: [54, 12, 49, 9, 48, 51, 31, 19, 58, 38, 43, 10, 21, 28, 50]\n",
      "\t\tAccuracy: 0.6968361581920903\n",
      "\tAnt 14 :\n",
      "\t\tPath: [40, 42, 5, 65, 57, 23, 18, 25, 44, 0, 53, 63, 15, 21, 30]\n",
      "\t\tAccuracy: 0.6666666666666667\n",
      "\tAnt 15 :\n",
      "\t\tPath: [6, 23, 34, 59, 67, 22, 57, 52, 14, 18, 50, 38, 8, 68, 29]\n",
      "\t\tAccuracy: 0.6868361581920904\n",
      "\tAnt 16 :\n",
      "\t\tPath: [14, 36, 13, 42, 65, 49, 31, 9, 8, 25, 68, 4, 50, 44, 16]\n",
      "\t\tAccuracy: 0.6799999999999999\n",
      "\tAnt 17 :\n",
      "\t\tPath: [6, 63, 22, 10, 65, 57, 59, 44, 48, 12, 46, 2, 52, 33, 11]\n",
      "\t\tAccuracy: 0.6867796610169491\n",
      "\tAnt 18 :\n",
      "\t\tPath: [30, 29, 42, 43, 12, 25, 51, 18, 22, 14, 65, 67, 57, 59, 2]\n",
      "\t\tAccuracy: 0.6664971751412428\n",
      "\tAnt 19 :\n",
      "\t\tPath: [43, 31, 63, 30, 56, 57, 40, 19, 50, 21, 18, 60, 68, 15, 22]\n",
      "\t\tAccuracy: 0.6697740112994349\n",
      "Colony 4 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [52, 25, 6, 53, 10, 30, 42, 12, 63, 50, 59, 56, 57, 44, 11]\n",
      "\t\tAccuracy: 0.700225988700565\n",
      "\tAnt 1 :\n",
      "\t\tPath: [37, 56, 10, 28, 9, 59, 12, 46, 43, 44, 15, 54, 38, 57, 42]\n",
      "\t\tAccuracy: 0.6936723163841808\n",
      "\tAnt 2 :\n",
      "\t\tPath: [12, 9, 50, 29, 2, 38, 46, 54, 63, 59, 49, 31, 60, 25, 40]\n",
      "\t\tAccuracy: 0.7172881355932204\n",
      "\tAnt 3 :\n",
      "\t\tPath: [2, 38, 12, 50, 46, 65, 41, 14, 53, 37, 63, 49, 40, 10, 44]\n",
      "\t\tAccuracy: 0.7140677966101695\n",
      "\tAnt 4 :\n",
      "\t\tPath: [67, 18, 12, 47, 44, 29, 38, 53, 37, 59, 10, 20, 9, 43, 14]\n",
      "\t\tAccuracy: 0.7441242937853106\n",
      "\tAnt 5 :\n",
      "\t\tPath: [37, 67, 11, 50, 29, 63, 31, 47, 2, 65, 57, 44, 58, 28, 12]\n",
      "\t\tAccuracy: 0.6834463276836159\n",
      "\tAnt 6 :\n",
      "\t\tPath: [46, 44, 33, 56, 7, 16, 65, 18, 36, 67, 54, 57, 12, 9, 42]\n",
      "\t\tAccuracy: 0.6663841807909604\n",
      "\tAnt 7 :\n",
      "\t\tPath: [69, 43, 6, 16, 25, 14, 59, 37, 9, 49, 57, 18, 51, 67, 63]\n",
      "\t\tAccuracy: 0.7036158192090396\n",
      "\tAnt 8 :\n",
      "\t\tPath: [59, 63, 50, 58, 22, 13, 65, 15, 48, 40, 29, 38, 2, 44, 53]\n",
      "\t\tAccuracy: 0.6933898305084745\n",
      "\tAnt 9 :\n",
      "\t\tPath: [25, 68, 37, 9, 14, 51, 48, 8, 22, 65, 59, 30, 10, 50, 12]\n",
      "\t\tAccuracy: 0.7103389830508474\n",
      "\tAnt 10 :\n",
      "\t\tPath: [10, 52, 49, 22, 9, 18, 38, 8, 67, 60, 7, 29, 64, 53, 36]\n",
      "\t\tAccuracy: 0.6634463276836158\n",
      "\tAnt 11 :\n",
      "\t\tPath: [27, 59, 10, 38, 46, 2, 42, 44, 9, 6, 65, 48, 49, 67, 57]\n",
      "\t\tAccuracy: 0.7140677966101695\n",
      "\tAnt 12 :\n",
      "\t\tPath: [11, 67, 14, 54, 20, 58, 59, 21, 52, 53, 38, 19, 47, 27, 18]\n",
      "\t\tAccuracy: 0.6935593220338983\n",
      "\tAnt 13 :\n",
      "\t\tPath: [43, 63, 19, 38, 44, 57, 22, 10, 59, 37, 11, 33, 0, 12, 47]\n",
      "\t\tAccuracy: 0.6968361581920904\n",
      "\tAnt 14 :\n",
      "\t\tPath: [20, 41, 59, 52, 10, 25, 19, 38, 44, 8, 47, 65, 13, 57, 51]\n",
      "\t\tAccuracy: 0.6698870056497175\n",
      "\tAnt 15 :\n",
      "\t\tPath: [2, 68, 9, 49, 57, 15, 59, 12, 44, 63, 41, 43, 16, 11, 0]\n",
      "\t\tAccuracy: 0.7103954802259886\n",
      "\tAnt 16 :\n",
      "\t\tPath: [59, 65, 18, 38, 57, 63, 30, 36, 44, 37, 49, 52, 42, 14, 9]\n",
      "\t\tAccuracy: 0.7138418079096044\n",
      "\tAnt 17 :\n",
      "\t\tPath: [51, 11, 63, 6, 14, 42, 59, 58, 12, 60, 9, 18, 10, 15, 43]\n",
      "\t\tAccuracy: 0.6835593220338982\n",
      "\tAnt 18 :\n",
      "\t\tPath: [43, 21, 8, 38, 44, 36, 22, 45, 2, 10, 14, 58, 59, 20, 67]\n",
      "\t\tAccuracy: 0.7372881355932204\n",
      "\tAnt 19 :\n",
      "\t\tPath: [12, 38, 9, 14, 24, 36, 54, 63, 25, 44, 22, 52, 19, 46, 58]\n",
      "\t\tAccuracy: 0.7001129943502825\n",
      "Colony 5 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [2, 59, 56, 63, 67, 14, 25, 41, 19, 28, 40, 30, 16, 64, 43]\n",
      "\t\tAccuracy: 0.713954802259887\n",
      "\tAnt 1 :\n",
      "\t\tPath: [65, 43, 44, 49, 16, 38, 67, 57, 10, 41, 42, 33, 58, 9, 37]\n",
      "\t\tAccuracy: 0.690225988700565\n",
      "\tAnt 2 :\n",
      "\t\tPath: [26, 63, 44, 38, 54, 53, 42, 46, 23, 52, 47, 16, 22, 68, 19]\n",
      "\t\tAccuracy: 0.70045197740113\n",
      "\tAnt 3 :\n",
      "\t\tPath: [57, 53, 49, 19, 44, 9, 18, 52, 16, 63, 50, 40, 11, 37, 51]\n",
      "\t\tAccuracy: 0.6801694915254238\n",
      "\tAnt 4 :\n",
      "\t\tPath: [2, 27, 67, 63, 53, 5, 59, 57, 16, 23, 29, 11, 52, 30, 38]\n",
      "\t\tAccuracy: 0.6835593220338982\n",
      "\tAnt 5 :\n",
      "\t\tPath: [67, 43, 25, 28, 58, 68, 57, 21, 56, 52, 44, 38, 51, 48, 14]\n",
      "\t\tAccuracy: 0.7070621468926553\n",
      "\tAnt 6 :\n",
      "\t\tPath: [0, 38, 43, 59, 33, 14, 44, 42, 50, 52, 65, 8, 20, 16, 25]\n",
      "\t\tAccuracy: 0.7071751412429379\n",
      "\tAnt 7 :\n",
      "\t\tPath: [67, 12, 9, 38, 44, 8, 57, 45, 48, 63, 10, 49, 52, 43, 60]\n",
      "\t\tAccuracy: 0.7275141242937853\n",
      "\tAnt 8 :\n",
      "\t\tPath: [59, 38, 20, 40, 9, 37, 22, 28, 8, 33, 49, 65, 60, 51, 11]\n",
      "\t\tAccuracy: 0.727683615819209\n",
      "\tAnt 9 :\n",
      "\t\tPath: [48, 63, 54, 52, 12, 14, 47, 13, 15, 2, 53, 25, 38, 31, 64]\n",
      "\t\tAccuracy: 0.6431638418079096\n",
      "\tAnt 10 :\n",
      "\t\tPath: [21, 41, 11, 58, 69, 6, 27, 63, 46, 23, 14, 29, 22, 44, 54]\n",
      "\t\tAccuracy: 0.6701694915254237\n",
      "\tAnt 11 :\n",
      "\t\tPath: [11, 33, 63, 8, 57, 10, 28, 22, 2, 38, 6, 18, 59, 9, 0]\n",
      "\t\tAccuracy: 0.7275141242937853\n",
      "\tAnt 12 :\n",
      "\t\tPath: [68, 14, 9, 10, 13, 53, 42, 20, 38, 57, 2, 63, 49, 28, 44]\n",
      "\t\tAccuracy: 0.6935593220338983\n",
      "\tAnt 13 :\n",
      "\t\tPath: [28, 50, 12, 52, 44, 57, 63, 22, 14, 59, 15, 56, 31, 53, 38]\n",
      "\t\tAccuracy: 0.703502824858757\n",
      "\tAnt 14 :\n",
      "\t\tPath: [67, 18, 33, 49, 38, 22, 42, 29, 56, 58, 44, 19, 48, 63, 12]\n",
      "\t\tAccuracy: 0.6834463276836159\n",
      "\tAnt 15 :\n",
      "\t\tPath: [28, 63, 22, 9, 52, 25, 59, 19, 30, 45, 37, 65, 57, 67, 14]\n",
      "\t\tAccuracy: 0.7040112994350283\n",
      "\tAnt 16 :\n",
      "\t\tPath: [53, 19, 29, 43, 12, 23, 10, 63, 44, 40, 48, 65, 61, 59, 9]\n",
      "\t\tAccuracy: 0.6496610169491526\n",
      "\tAnt 17 :\n",
      "\t\tPath: [33, 44, 25, 15, 63, 53, 59, 50, 23, 67, 12, 9, 38, 52, 5]\n",
      "\t\tAccuracy: 0.6934463276836158\n",
      "\tAnt 18 :\n",
      "\t\tPath: [11, 52, 57, 27, 44, 40, 29, 14, 67, 43, 10, 6, 42, 18, 31]\n",
      "\t\tAccuracy: 0.6498870056497175\n",
      "\tAnt 19 :\n",
      "\t\tPath: [23, 30, 63, 57, 44, 60, 19, 28, 53, 51, 43, 38, 11, 65, 22]\n",
      "\t\tAccuracy: 0.7072316384180791\n",
      "Colony 6 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [12, 56, 10, 18, 33, 19, 40, 31, 58, 57, 37, 38, 30, 65, 47]\n",
      "\t\tAccuracy: 0.7137853107344634\n",
      "\tAnt 1 :\n",
      "\t\tPath: [49, 36, 50, 54, 58, 16, 38, 9, 12, 63, 44, 37, 0, 30, 43]\n",
      "\t\tAccuracy: 0.7003389830508475\n",
      "\tAnt 2 :\n",
      "\t\tPath: [15, 2, 37, 48, 27, 20, 52, 28, 44, 12, 25, 16, 38, 9, 51]\n",
      "\t\tAccuracy: 0.7003389830508475\n",
      "\tAnt 3 :\n",
      "\t\tPath: [0, 10, 53, 22, 59, 21, 57, 67, 9, 16, 41, 11, 32, 2, 12]\n",
      "\t\tAccuracy: 0.7137853107344634\n",
      "\tAnt 4 :\n",
      "\t\tPath: [50, 44, 67, 63, 38, 41, 18, 17, 10, 14, 40, 23, 65, 36, 59]\n",
      "\t\tAccuracy: 0.7171186440677966\n",
      "\tAnt 5 :\n",
      "\t\tPath: [40, 42, 38, 9, 50, 12, 69, 11, 15, 53, 63, 68, 65, 16, 57]\n",
      "\t\tAccuracy: 0.6631638418079097\n",
      "\tAnt 6 :\n",
      "\t\tPath: [13, 23, 60, 63, 9, 10, 49, 18, 57, 2, 50, 12, 38, 58, 44]\n",
      "\t\tAccuracy: 0.7172881355932204\n",
      "\tAnt 7 :\n",
      "\t\tPath: [56, 9, 34, 67, 38, 36, 30, 14, 16, 50, 44, 59, 52, 32, 19]\n",
      "\t\tAccuracy: 0.6765536723163843\n",
      "\tAnt 8 :\n",
      "\t\tPath: [50, 53, 59, 14, 25, 30, 43, 63, 33, 57, 21, 29, 51, 65, 19]\n",
      "\t\tAccuracy: 0.7035593220338983\n",
      "\tAnt 9 :\n",
      "\t\tPath: [24, 67, 23, 63, 9, 6, 38, 10, 42, 37, 14, 25, 3, 57, 48]\n",
      "\t\tAccuracy: 0.7238418079096045\n",
      "\tAnt 10 :\n",
      "\t\tPath: [14, 45, 49, 56, 38, 42, 53, 59, 46, 19, 11, 33, 63, 2, 67]\n",
      "\t\tAccuracy: 0.703502824858757\n",
      "\tAnt 11 :\n",
      "\t\tPath: [10, 21, 33, 69, 49, 38, 9, 59, 12, 27, 11, 6, 50, 14, 63]\n",
      "\t\tAccuracy: 0.7070621468926553\n",
      "\tAnt 12 :\n",
      "\t\tPath: [46, 10, 18, 25, 63, 19, 50, 11, 2, 30, 58, 49, 51, 14, 59]\n",
      "\t\tAccuracy: 0.6699435028248588\n",
      "\tAnt 13 :\n",
      "\t\tPath: [9, 57, 54, 49, 51, 10, 12, 37, 28, 40, 13, 65, 48, 2, 46]\n",
      "\t\tAccuracy: 0.7137853107344633\n",
      "\tAnt 14 :\n",
      "\t\tPath: [49, 52, 19, 42, 10, 6, 60, 57, 15, 28, 44, 63, 48, 69, 30]\n",
      "\t\tAccuracy: 0.7105649717514124\n",
      "\tAnt 15 :\n",
      "\t\tPath: [33, 15, 30, 8, 44, 67, 59, 49, 68, 22, 40, 37, 2, 60, 16]\n",
      "\t\tAccuracy: 0.7106214689265536\n",
      "\tAnt 16 :\n",
      "\t\tPath: [8, 50, 38, 21, 25, 2, 52, 10, 18, 12, 28, 41, 67, 9, 48]\n",
      "\t\tAccuracy: 0.7105649717514124\n",
      "\tAnt 17 :\n",
      "\t\tPath: [49, 12, 43, 0, 50, 14, 58, 25, 37, 38, 30, 57, 9, 68, 52]\n",
      "\t\tAccuracy: 0.7136723163841807\n",
      "\tAnt 18 :\n",
      "\t\tPath: [10, 42, 60, 12, 34, 65, 28, 51, 14, 50, 43, 18, 56, 53, 38]\n",
      "\t\tAccuracy: 0.6805084745762712\n",
      "\tAnt 19 :\n",
      "\t\tPath: [31, 44, 2, 37, 3, 43, 10, 6, 15, 25, 67, 56, 9, 23, 38]\n",
      "\t\tAccuracy: 0.6801694915254237\n",
      "Colony 7 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [45, 52, 36, 11, 63, 65, 44, 59, 2, 20, 68, 38, 14, 43, 25]\n",
      "\t\tAccuracy: 0.7103954802259886\n",
      "\tAnt 1 :\n",
      "\t\tPath: [16, 40, 27, 31, 37, 38, 15, 51, 57, 59, 33, 67, 63, 30, 25]\n",
      "\t\tAccuracy: 0.6903389830508475\n",
      "\tAnt 2 :\n",
      "\t\tPath: [59, 63, 9, 14, 53, 29, 42, 44, 30, 48, 57, 12, 54, 52, 37]\n",
      "\t\tAccuracy: 0.7001129943502825\n",
      "\tAnt 3 :\n",
      "\t\tPath: [11, 20, 0, 14, 63, 29, 8, 6, 12, 54, 67, 52, 59, 38, 42]\n",
      "\t\tAccuracy: 0.6867796610169491\n",
      "\tAnt 4 :\n",
      "\t\tPath: [45, 18, 9, 67, 63, 50, 38, 46, 37, 23, 65, 8, 14, 29, 60]\n",
      "\t\tAccuracy: 0.7140677966101695\n",
      "\tAnt 5 :\n",
      "\t\tPath: [63, 30, 67, 34, 35, 19, 9, 38, 44, 25, 28, 65, 59, 46, 68]\n",
      "\t\tAccuracy: 0.703728813559322\n",
      "\tAnt 6 :\n",
      "\t\tPath: [11, 58, 22, 10, 42, 12, 38, 34, 50, 56, 8, 43, 21, 14, 16]\n",
      "\t\tAccuracy: 0.6967796610169491\n",
      "\tAnt 7 :\n",
      "\t\tPath: [63, 19, 23, 30, 67, 52, 14, 65, 56, 44, 12, 9, 47, 18, 53]\n",
      "\t\tAccuracy: 0.6698305084745763\n",
      "\tAnt 8 :\n",
      "\t\tPath: [12, 37, 53, 50, 46, 67, 49, 58, 10, 63, 11, 9, 29, 18, 60]\n",
      "\t\tAccuracy: 0.6733333333333333\n",
      "\tAnt 9 :\n",
      "\t\tPath: [63, 57, 20, 67, 23, 44, 56, 8, 14, 43, 33, 52, 9, 25, 49]\n",
      "\t\tAccuracy: 0.6937853107344633\n",
      "\tAnt 10 :\n",
      "\t\tPath: [31, 65, 68, 37, 50, 67, 25, 14, 13, 53, 52, 59, 2, 49, 22]\n",
      "\t\tAccuracy: 0.7171751412429379\n",
      "\tAnt 11 :\n",
      "\t\tPath: [50, 46, 60, 53, 2, 12, 22, 57, 63, 52, 43, 65, 14, 25, 11]\n",
      "\t\tAccuracy: 0.7306779661016949\n",
      "\tAnt 12 :\n",
      "\t\tPath: [63, 12, 10, 42, 19, 25, 49, 8, 9, 57, 5, 51, 7, 56, 43]\n",
      "\t\tAccuracy: 0.737457627118644\n",
      "\tAnt 13 :\n",
      "\t\tPath: [54, 14, 36, 40, 8, 18, 59, 15, 38, 11, 22, 44, 34, 50, 57]\n",
      "\t\tAccuracy: 0.6734463276836158\n",
      "\tAnt 14 :\n",
      "\t\tPath: [63, 10, 30, 37, 36, 67, 9, 5, 15, 31, 25, 59, 58, 49, 24]\n",
      "\t\tAccuracy: 0.6737853107344633\n",
      "\tAnt 15 :\n",
      "\t\tPath: [49, 67, 9, 38, 47, 16, 34, 63, 5, 43, 40, 18, 30, 19, 10]\n",
      "\t\tAccuracy: 0.6870621468926553\n",
      "\tAnt 16 :\n",
      "\t\tPath: [30, 63, 65, 44, 38, 10, 31, 12, 59, 57, 22, 9, 46, 60, 43]\n",
      "\t\tAccuracy: 0.7206214689265537\n",
      "\tAnt 17 :\n",
      "\t\tPath: [16, 63, 6, 30, 37, 38, 52, 10, 59, 5, 12, 46, 19, 33, 58]\n",
      "\t\tAccuracy: 0.7207344632768361\n",
      "\tAnt 18 :\n",
      "\t\tPath: [49, 24, 63, 36, 67, 10, 19, 57, 50, 65, 12, 53, 29, 1, 14]\n",
      "\t\tAccuracy: 0.6869491525423729\n",
      "\tAnt 19 :\n",
      "\t\tPath: [31, 12, 57, 52, 50, 38, 19, 2, 20, 53, 29, 4, 51, 59, 58]\n",
      "\t\tAccuracy: 0.703728813559322\n",
      "Colony 8 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [10, 33, 18, 67, 42, 59, 19, 12, 60, 57, 2, 9, 6, 50, 41]\n",
      "\t\tAccuracy: 0.700225988700565\n",
      "\tAnt 1 :\n",
      "\t\tPath: [30, 2, 15, 37, 38, 9, 28, 57, 67, 54, 11, 40, 44, 29, 63]\n",
      "\t\tAccuracy: 0.703954802259887\n",
      "\tAnt 2 :\n",
      "\t\tPath: [43, 42, 8, 65, 57, 2, 67, 59, 53, 9, 19, 20, 50, 12, 44]\n",
      "\t\tAccuracy: 0.6968926553672316\n",
      "\tAnt 3 :\n",
      "\t\tPath: [9, 14, 18, 59, 44, 50, 51, 38, 20, 65, 37, 22, 43, 48, 47]\n",
      "\t\tAccuracy: 0.733954802259887\n",
      "\tAnt 4 :\n",
      "\t\tPath: [67, 58, 14, 36, 49, 23, 37, 13, 9, 51, 43, 2, 57, 8, 46]\n",
      "\t\tAccuracy: 0.7374576271186442\n",
      "\tAnt 5 :\n",
      "\t\tPath: [44, 52, 9, 11, 49, 63, 22, 64, 30, 50, 59, 29, 33, 56, 20]\n",
      "\t\tAccuracy: 0.7068361581920903\n",
      "\tAnt 6 :\n",
      "\t\tPath: [11, 59, 9, 56, 63, 31, 37, 43, 27, 10, 68, 60, 35, 28, 42]\n",
      "\t\tAccuracy: 0.6768926553672315\n",
      "\tAnt 7 :\n",
      "\t\tPath: [49, 38, 60, 63, 31, 51, 59, 23, 43, 37, 65, 14, 42, 12, 6]\n",
      "\t\tAccuracy: 0.7238418079096045\n",
      "\tAnt 8 :\n",
      "\t\tPath: [21, 14, 59, 31, 38, 51, 9, 67, 18, 52, 37, 30, 20, 27, 46]\n",
      "\t\tAccuracy: 0.7202824858757062\n",
      "\tAnt 9 :\n",
      "\t\tPath: [67, 43, 38, 30, 50, 45, 59, 8, 20, 44, 16, 57, 14, 12, 58]\n",
      "\t\tAccuracy: 0.7034463276836158\n",
      "\tAnt 10 :\n",
      "\t\tPath: [10, 48, 65, 58, 50, 34, 63, 66, 0, 37, 25, 36, 11, 28, 68]\n",
      "\t\tAccuracy: 0.6766666666666666\n",
      "\tAnt 11 :\n",
      "\t\tPath: [42, 53, 38, 65, 58, 15, 25, 52, 50, 57, 63, 14, 56, 36, 9]\n",
      "\t\tAccuracy: 0.6969491525423729\n",
      "\tAnt 12 :\n",
      "\t\tPath: [44, 21, 22, 43, 9, 15, 67, 53, 13, 18, 30, 49, 27, 11, 23]\n",
      "\t\tAccuracy: 0.6837288135593219\n",
      "\tAnt 13 :\n",
      "\t\tPath: [65, 18, 54, 10, 25, 16, 49, 57, 19, 2, 59, 28, 52, 14, 5]\n",
      "\t\tAccuracy: 0.67\n",
      "\tAnt 14 :\n",
      "\t\tPath: [44, 49, 50, 67, 63, 7, 25, 12, 14, 29, 37, 53, 11, 43, 69]\n",
      "\t\tAccuracy: 0.6633333333333333\n",
      "\tAnt 15 :\n",
      "\t\tPath: [21, 54, 37, 2, 42, 59, 10, 38, 53, 56, 57, 15, 41, 43, 44]\n",
      "\t\tAccuracy: 0.7071751412429379\n",
      "\tAnt 16 :\n",
      "\t\tPath: [11, 37, 51, 38, 16, 33, 29, 50, 14, 42, 44, 9, 25, 18, 4]\n",
      "\t\tAccuracy: 0.6870621468926553\n",
      "\tAnt 17 :\n",
      "\t\tPath: [60, 12, 33, 57, 10, 23, 50, 19, 18, 2, 32, 53, 45, 48, 59]\n",
      "\t\tAccuracy: 0.6900564971751413\n",
      "\tAnt 18 :\n",
      "\t\tPath: [10, 63, 16, 2, 52, 29, 43, 46, 37, 57, 34, 14, 53, 19, 38]\n",
      "\t\tAccuracy: 0.7102824858757062\n",
      "\tAnt 19 :\n",
      "\t\tPath: [9, 6, 50, 12, 54, 42, 10, 30, 23, 51, 63, 11, 22, 0, 37]\n",
      "\t\tAccuracy: 0.7005084745762712\n",
      "Colony 9 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [67, 63, 41, 49, 42, 19, 9, 25, 44, 48, 52, 27, 51, 22, 30]\n",
      "\t\tAccuracy: 0.7002824858757062\n",
      "\tAnt 1 :\n",
      "\t\tPath: [16, 57, 19, 38, 65, 37, 43, 34, 3, 22, 12, 60, 2, 58, 50]\n",
      "\t\tAccuracy: 0.7171751412429379\n",
      "\tAnt 2 :\n",
      "\t\tPath: [46, 18, 19, 48, 37, 2, 30, 16, 67, 50, 5, 13, 9, 59, 44]\n",
      "\t\tAccuracy: 0.6938983050847457\n",
      "\tAnt 3 :\n",
      "\t\tPath: [56, 10, 57, 30, 22, 50, 18, 59, 12, 65, 40, 27, 9, 7, 11]\n",
      "\t\tAccuracy: 0.6699435028248588\n",
      "\tAnt 4 :\n",
      "\t\tPath: [59, 51, 38, 8, 43, 25, 46, 31, 24, 67, 16, 56, 57, 48, 2]\n",
      "\t\tAccuracy: 0.7240112994350282\n",
      "\tAnt 5 :\n",
      "\t\tPath: [49, 19, 28, 59, 15, 38, 45, 23, 52, 18, 10, 13, 43, 57, 14]\n",
      "\t\tAccuracy: 0.7272316384180791\n",
      "\tAnt 6 :\n",
      "\t\tPath: [52, 59, 48, 14, 19, 15, 51, 18, 65, 44, 38, 10, 30, 24, 9]\n",
      "\t\tAccuracy: 0.7004519774011299\n",
      "\tAnt 7 :\n",
      "\t\tPath: [43, 45, 67, 38, 28, 37, 42, 50, 57, 59, 41, 11, 19, 10, 52]\n",
      "\t\tAccuracy: 0.6800564971751413\n",
      "\tAnt 8 :\n",
      "\t\tPath: [59, 22, 53, 37, 10, 44, 18, 51, 11, 41, 52, 40, 2, 33, 14]\n",
      "\t\tAccuracy: 0.7172881355932204\n",
      "\tAnt 9 :\n",
      "\t\tPath: [52, 7, 6, 65, 18, 54, 10, 21, 60, 67, 16, 9, 23, 29, 11]\n",
      "\t\tAccuracy: 0.7105084745762712\n",
      "\tAnt 10 :\n",
      "\t\tPath: [16, 12, 59, 15, 29, 49, 11, 67, 54, 50, 30, 46, 19, 14, 36]\n",
      "\t\tAccuracy: 0.68\n",
      "\tAnt 11 :\n",
      "\t\tPath: [28, 22, 18, 9, 6, 54, 43, 25, 27, 59, 57, 14, 51, 2, 63]\n",
      "\t\tAccuracy: 0.7306214689265537\n",
      "\tAnt 12 :\n",
      "\t\tPath: [65, 3, 18, 14, 12, 59, 30, 68, 19, 44, 23, 46, 25, 48, 38]\n",
      "\t\tAccuracy: 0.7036723163841807\n",
      "\tAnt 13 :\n",
      "\t\tPath: [16, 22, 63, 37, 57, 2, 59, 43, 44, 69, 10, 19, 30, 65, 45]\n",
      "\t\tAccuracy: 0.7205649717514124\n",
      "\tAnt 14 :\n",
      "\t\tPath: [9, 52, 51, 46, 59, 28, 10, 5, 38, 14, 8, 12, 34, 18, 19]\n",
      "\t\tAccuracy: 0.6939548022598869\n",
      "\tAnt 15 :\n",
      "\t\tPath: [46, 37, 11, 50, 18, 38, 57, 14, 33, 41, 25, 67, 0, 44, 28]\n",
      "\t\tAccuracy: 0.7238418079096045\n",
      "\tAnt 16 :\n",
      "\t\tPath: [59, 20, 15, 44, 49, 57, 63, 27, 12, 58, 9, 10, 65, 41, 22]\n",
      "\t\tAccuracy: 0.6967796610169492\n",
      "\tAnt 17 :\n",
      "\t\tPath: [52, 50, 27, 56, 30, 11, 57, 59, 32, 40, 28, 18, 67, 19, 51]\n",
      "\t\tAccuracy: 0.6801129943502825\n",
      "\tAnt 18 :\n",
      "\t\tPath: [41, 11, 14, 48, 58, 37, 50, 31, 6, 67, 23, 38, 9, 60, 65]\n",
      "\t\tAccuracy: 0.6868361581920903\n",
      "\tAnt 19 :\n",
      "\t\tPath: [14, 23, 37, 53, 66, 21, 44, 9, 33, 36, 38, 57, 49, 65, 64]\n",
      "\t\tAccuracy: 0.7071186440677966\n",
      "Colony 10 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [24, 58, 16, 8, 63, 42, 57, 38, 19, 44, 28, 22, 48, 37, 9]\n",
      "\t\tAccuracy: 0.6835593220338982\n",
      "\tAnt 1 :\n",
      "\t\tPath: [22, 50, 59, 63, 43, 44, 14, 37, 58, 15, 65, 38, 60, 30, 52]\n",
      "\t\tAccuracy: 0.723954802259887\n",
      "\tAnt 2 :\n",
      "\t\tPath: [43, 49, 3, 11, 53, 14, 29, 30, 52, 16, 10, 57, 48, 63, 15]\n",
      "\t\tAccuracy: 0.6702259887005649\n",
      "\tAnt 3 :\n",
      "\t\tPath: [59, 58, 42, 12, 65, 63, 67, 18, 43, 30, 68, 31, 33, 38, 60]\n",
      "\t\tAccuracy: 0.6734463276836158\n",
      "\tAnt 4 :\n",
      "\t\tPath: [14, 56, 57, 44, 49, 50, 18, 63, 12, 38, 58, 41, 9, 30, 60]\n",
      "\t\tAccuracy: 0.7034463276836158\n",
      "\tAnt 5 :\n",
      "\t\tPath: [15, 46, 67, 40, 25, 30, 59, 28, 9, 63, 11, 20, 38, 65, 64]\n",
      "\t\tAccuracy: 0.7206214689265537\n",
      "\tAnt 6 :\n",
      "\t\tPath: [50, 6, 53, 14, 11, 44, 57, 67, 63, 20, 22, 25, 48, 2, 23]\n",
      "\t\tAccuracy: 0.7069491525423729\n",
      "\tAnt 7 :\n",
      "\t\tPath: [10, 46, 14, 25, 49, 59, 2, 15, 33, 65, 4, 13, 42, 57, 22]\n",
      "\t\tAccuracy: 0.7374011299435027\n",
      "\tAnt 8 :\n",
      "\t\tPath: [34, 38, 59, 49, 64, 67, 57, 19, 63, 22, 52, 14, 58, 48, 15]\n",
      "\t\tAccuracy: 0.7205084745762711\n",
      "\tAnt 9 :\n",
      "\t\tPath: [14, 21, 45, 16, 68, 24, 12, 52, 19, 69, 44, 22, 5, 38, 54]\n",
      "\t\tAccuracy: 0.6896610169491526\n",
      "\tAnt 10 :\n",
      "\t\tPath: [51, 63, 18, 68, 37, 43, 65, 12, 49, 48, 9, 42, 30, 22, 14]\n",
      "\t\tAccuracy: 0.6534463276836158\n",
      "\tAnt 11 :\n",
      "\t\tPath: [12, 51, 50, 15, 44, 52, 33, 19, 46, 57, 30, 11, 31, 67, 8]\n",
      "\t\tAccuracy: 0.7068361581920903\n",
      "\tAnt 12 :\n",
      "\t\tPath: [16, 51, 19, 30, 46, 34, 57, 2, 53, 52, 10, 48, 7, 11, 59]\n",
      "\t\tAccuracy: 0.67\n",
      "\tAnt 13 :\n",
      "\t\tPath: [59, 67, 65, 9, 49, 30, 25, 68, 43, 5, 44, 63, 10, 56, 57]\n",
      "\t\tAccuracy: 0.7170056497175141\n",
      "\tAnt 14 :\n",
      "\t\tPath: [14, 36, 57, 20, 48, 1, 34, 69, 8, 58, 38, 60, 59, 37, 63]\n",
      "\t\tAccuracy: 0.693728813559322\n",
      "\tAnt 15 :\n",
      "\t\tPath: [22, 30, 48, 42, 19, 38, 15, 44, 18, 63, 6, 11, 57, 2, 13]\n",
      "\t\tAccuracy: 0.6767231638418079\n",
      "\tAnt 16 :\n",
      "\t\tPath: [52, 59, 13, 38, 57, 14, 43, 63, 54, 19, 16, 37, 58, 6, 30]\n",
      "\t\tAccuracy: 0.7170621468926553\n",
      "\tAnt 17 :\n",
      "\t\tPath: [25, 10, 8, 43, 50, 40, 38, 56, 12, 18, 63, 58, 36, 28, 54]\n",
      "\t\tAccuracy: 0.6833898305084746\n",
      "\tAnt 18 :\n",
      "\t\tPath: [6, 37, 16, 11, 20, 64, 10, 59, 40, 31, 48, 65, 42, 14, 50]\n",
      "\t\tAccuracy: 0.6802824858757062\n",
      "\tAnt 19 :\n",
      "\t\tPath: [18, 30, 52, 38, 50, 59, 5, 63, 57, 6, 25, 22, 11, 68, 37]\n",
      "\t\tAccuracy: 0.6971186440677967\n",
      "Colony 11 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [10, 14, 9, 63, 49, 51, 33, 59, 28, 18, 68, 54, 22, 13, 12]\n",
      "\t\tAccuracy: 0.6700000000000002\n",
      "\tAnt 1 :\n",
      "\t\tPath: [27, 19, 53, 43, 60, 10, 63, 2, 28, 14, 40, 12, 51, 41, 11]\n",
      "\t\tAccuracy: 0.6733898305084747\n",
      "\tAnt 2 :\n",
      "\t\tPath: [16, 40, 12, 45, 41, 48, 25, 44, 38, 46, 28, 50, 60, 19, 43]\n",
      "\t\tAccuracy: 0.70045197740113\n",
      "\tAnt 3 :\n",
      "\t\tPath: [9, 63, 10, 69, 67, 46, 65, 14, 57, 27, 50, 37, 52, 56, 12]\n",
      "\t\tAccuracy: 0.6833898305084746\n",
      "\tAnt 4 :\n",
      "\t\tPath: [15, 42, 21, 9, 57, 49, 2, 51, 6, 12, 44, 20, 14, 31, 18]\n",
      "\t\tAccuracy: 0.7068926553672317\n",
      "\tAnt 5 :\n",
      "\t\tPath: [65, 58, 0, 2, 67, 53, 28, 14, 43, 6, 25, 48, 9, 10, 22]\n",
      "\t\tAccuracy: 0.7273446327683615\n",
      "\tAnt 6 :\n",
      "\t\tPath: [24, 53, 11, 58, 8, 33, 43, 25, 44, 9, 52, 46, 19, 38, 49]\n",
      "\t\tAccuracy: 0.7105084745762712\n",
      "\tAnt 7 :\n",
      "\t\tPath: [44, 37, 52, 57, 12, 9, 51, 67, 30, 59, 38, 60, 54, 56, 63]\n",
      "\t\tAccuracy: 0.6836158192090395\n",
      "\tAnt 8 :\n",
      "\t\tPath: [49, 59, 12, 9, 58, 51, 11, 30, 64, 63, 44, 57, 1, 50, 38]\n",
      "\t\tAccuracy: 0.6901129943502824\n",
      "\tAnt 9 :\n",
      "\t\tPath: [44, 57, 63, 38, 12, 27, 42, 18, 25, 58, 46, 52, 48, 64, 36]\n",
      "\t\tAccuracy: 0.6666101694915254\n",
      "\tAnt 10 :\n",
      "\t\tPath: [23, 19, 44, 7, 27, 9, 59, 49, 31, 8, 25, 30, 52, 11, 18]\n",
      "\t\tAccuracy: 0.7071186440677966\n",
      "\tAnt 11 :\n",
      "\t\tPath: [10, 0, 19, 63, 56, 48, 52, 12, 18, 51, 28, 54, 60, 50, 49]\n",
      "\t\tAccuracy: 0.723954802259887\n",
      "\tAnt 12 :\n",
      "\t\tPath: [46, 12, 25, 8, 60, 51, 57, 11, 38, 48, 42, 67, 65, 63, 43]\n",
      "\t\tAccuracy: 0.6971186440677967\n",
      "\tAnt 13 :\n",
      "\t\tPath: [58, 43, 10, 65, 20, 30, 57, 59, 5, 9, 37, 54, 15, 49, 50]\n",
      "\t\tAccuracy: 0.7272881355932203\n",
      "\tAnt 14 :\n",
      "\t\tPath: [10, 9, 18, 38, 14, 60, 57, 56, 15, 12, 19, 28, 13, 68, 31]\n",
      "\t\tAccuracy: 0.6732203389830509\n",
      "\tAnt 15 :\n",
      "\t\tPath: [27, 24, 63, 56, 57, 54, 58, 16, 50, 52, 10, 6, 11, 28, 38]\n",
      "\t\tAccuracy: 0.683276836158192\n",
      "\tAnt 16 :\n",
      "\t\tPath: [2, 33, 57, 6, 59, 48, 16, 11, 67, 9, 54, 14, 23, 46, 22]\n",
      "\t\tAccuracy: 0.6768361581920903\n",
      "\tAnt 17 :\n",
      "\t\tPath: [42, 59, 63, 44, 23, 26, 20, 56, 10, 50, 9, 28, 22, 36, 57]\n",
      "\t\tAccuracy: 0.6798870056497175\n",
      "\tAnt 18 :\n",
      "\t\tPath: [33, 65, 68, 31, 25, 19, 46, 50, 53, 15, 52, 48, 67, 38, 34]\n",
      "\t\tAccuracy: 0.6767231638418079\n",
      "\tAnt 19 :\n",
      "\t\tPath: [2, 59, 43, 52, 33, 63, 30, 54, 38, 36, 37, 22, 18, 24, 44]\n",
      "\t\tAccuracy: 0.7677966101694915\n",
      "Colony 12 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [14, 10, 38, 50, 34, 27, 59, 12, 67, 63, 52, 9, 41, 44, 60]\n",
      "\t\tAccuracy: 0.7374011299435027\n",
      "\tAnt 1 :\n",
      "\t\tPath: [42, 63, 21, 59, 48, 33, 49, 10, 58, 31, 1, 9, 43, 19, 53]\n",
      "\t\tAccuracy: 0.713954802259887\n",
      "\tAnt 2 :\n",
      "\t\tPath: [51, 2, 14, 57, 15, 43, 25, 59, 52, 48, 68, 9, 19, 38, 49]\n",
      "\t\tAccuracy: 0.7307344632768361\n",
      "\tAnt 3 :\n",
      "\t\tPath: [19, 59, 0, 65, 57, 11, 10, 46, 63, 58, 36, 9, 14, 38, 33]\n",
      "\t\tAccuracy: 0.6566101694915254\n",
      "\tAnt 4 :\n",
      "\t\tPath: [46, 25, 38, 10, 65, 14, 50, 27, 49, 59, 52, 47, 43, 48, 28]\n",
      "\t\tAccuracy: 0.7071751412429379\n",
      "\tAnt 5 :\n",
      "\t\tPath: [31, 21, 52, 16, 34, 67, 65, 49, 48, 58, 37, 25, 14, 29, 11]\n",
      "\t\tAccuracy: 0.6766101694915254\n",
      "\tAnt 6 :\n",
      "\t\tPath: [50, 38, 11, 12, 31, 60, 14, 59, 54, 6, 0, 21, 52, 18, 49]\n",
      "\t\tAccuracy: 0.6800564971751413\n",
      "\tAnt 7 :\n",
      "\t\tPath: [16, 19, 50, 30, 18, 43, 15, 22, 14, 9, 44, 11, 38, 57, 60]\n",
      "\t\tAccuracy: 0.7102824858757062\n",
      "\tAnt 8 :\n",
      "\t\tPath: [2, 44, 41, 63, 11, 59, 30, 38, 37, 57, 10, 48, 12, 64, 6]\n",
      "\t\tAccuracy: 0.7071186440677966\n",
      "\tAnt 9 :\n",
      "\t\tPath: [50, 10, 30, 34, 53, 13, 14, 33, 5, 0, 9, 64, 68, 57, 48]\n",
      "\t\tAccuracy: 0.6936158192090395\n",
      "\tAnt 10 :\n",
      "\t\tPath: [47, 9, 42, 14, 65, 33, 37, 44, 60, 2, 6, 38, 12, 49, 51]\n",
      "\t\tAccuracy: 0.7005649717514124\n",
      "\tAnt 11 :\n",
      "\t\tPath: [41, 53, 19, 29, 50, 9, 52, 64, 33, 65, 44, 58, 8, 18, 38]\n",
      "\t\tAccuracy: 0.6361581920903956\n",
      "\tAnt 12 :\n",
      "\t\tPath: [40, 13, 47, 63, 48, 57, 14, 16, 67, 38, 54, 37, 56, 11, 50]\n",
      "\t\tAccuracy: 0.7138983050847457\n",
      "\tAnt 13 :\n",
      "\t\tPath: [45, 11, 19, 63, 38, 14, 29, 58, 60, 31, 6, 16, 8, 9, 59]\n",
      "\t\tAccuracy: 0.6901694915254237\n",
      "\tAnt 14 :\n",
      "\t\tPath: [2, 9, 41, 15, 22, 45, 30, 49, 46, 57, 8, 18, 56, 48, 11]\n",
      "\t\tAccuracy: 0.7206779661016949\n",
      "\tAnt 15 :\n",
      "\t\tPath: [9, 16, 20, 51, 50, 59, 12, 49, 38, 30, 37, 15, 58, 63, 40]\n",
      "\t\tAccuracy: 0.7272881355932204\n",
      "\tAnt 16 :\n",
      "\t\tPath: [9, 42, 31, 65, 33, 59, 50, 43, 27, 22, 25, 12, 63, 37, 67]\n",
      "\t\tAccuracy: 0.6865536723163842\n",
      "\tAnt 17 :\n",
      "\t\tPath: [14, 19, 28, 11, 45, 46, 68, 33, 31, 49, 25, 23, 22, 59, 53]\n",
      "\t\tAccuracy: 0.6936158192090395\n",
      "\tAnt 18 :\n",
      "\t\tPath: [51, 16, 63, 57, 2, 65, 58, 11, 38, 44, 5, 9, 64, 33, 59]\n",
      "\t\tAccuracy: 0.7574011299435028\n",
      "\tAnt 19 :\n",
      "\t\tPath: [14, 67, 15, 9, 60, 65, 11, 49, 46, 20, 58, 48, 57, 10, 18]\n",
      "\t\tAccuracy: 0.7138418079096045\n",
      "Colony 13 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [33, 12, 31, 56, 50, 52, 63, 34, 59, 54, 37, 27, 6, 38, 8]\n",
      "\t\tAccuracy: 0.7036723163841808\n",
      "\tAnt 1 :\n",
      "\t\tPath: [59, 11, 37, 57, 38, 67, 50, 33, 29, 42, 52, 64, 10, 9, 48]\n",
      "\t\tAccuracy: 0.6804519774011298\n",
      "\tAnt 2 :\n",
      "\t\tPath: [14, 67, 45, 51, 63, 22, 18, 43, 15, 48, 7, 12, 29, 41, 50]\n",
      "\t\tAccuracy: 0.6498870056497175\n",
      "\tAnt 3 :\n",
      "\t\tPath: [43, 31, 54, 48, 64, 41, 8, 9, 44, 21, 46, 12, 16, 15, 63]\n",
      "\t\tAccuracy: 0.6835593220338982\n",
      "\tAnt 4 :\n",
      "\t\tPath: [25, 44, 19, 20, 42, 59, 10, 63, 52, 15, 47, 65, 49, 22, 60]\n",
      "\t\tAccuracy: 0.7309039548022599\n",
      "\tAnt 5 :\n",
      "\t\tPath: [13, 14, 54, 34, 49, 40, 67, 51, 24, 23, 46, 65, 18, 63, 5]\n",
      "\t\tAccuracy: 0.6972881355932202\n",
      "\tAnt 6 :\n",
      "\t\tPath: [38, 46, 56, 8, 63, 45, 57, 52, 22, 44, 14, 37, 54, 16, 19]\n",
      "\t\tAccuracy: 0.7238983050847457\n",
      "\tAnt 7 :\n",
      "\t\tPath: [42, 65, 13, 64, 11, 38, 49, 67, 10, 14, 52, 43, 57, 25, 28]\n",
      "\t\tAccuracy: 0.6870621468926554\n",
      "\tAnt 8 :\n",
      "\t\tPath: [53, 54, 30, 12, 18, 64, 40, 8, 38, 42, 68, 43, 22, 37, 23]\n",
      "\t\tAccuracy: 0.6735593220338982\n",
      "\tAnt 9 :\n",
      "\t\tPath: [25, 37, 36, 43, 33, 12, 13, 59, 22, 38, 53, 41, 11, 30, 2]\n",
      "\t\tAccuracy: 0.690225988700565\n",
      "\tAnt 10 :\n",
      "\t\tPath: [5, 20, 60, 10, 51, 48, 21, 14, 7, 65, 11, 59, 49, 9, 50]\n",
      "\t\tAccuracy: 0.7036723163841807\n",
      "\tAnt 11 :\n",
      "\t\tPath: [63, 20, 10, 14, 18, 43, 65, 54, 48, 58, 25, 44, 38, 59, 57]\n",
      "\t\tAccuracy: 0.6801694915254237\n",
      "\tAnt 12 :\n",
      "\t\tPath: [27, 19, 12, 9, 42, 38, 53, 44, 54, 10, 68, 36, 31, 58, 63]\n",
      "\t\tAccuracy: 0.6598870056497175\n",
      "\tAnt 13 :\n",
      "\t\tPath: [0, 42, 44, 65, 21, 67, 52, 14, 29, 49, 63, 12, 57, 43, 51]\n",
      "\t\tAccuracy: 0.6799999999999999\n",
      "\tAnt 14 :\n",
      "\t\tPath: [2, 42, 47, 60, 50, 12, 49, 10, 57, 44, 23, 36, 25, 15, 18]\n",
      "\t\tAccuracy: 0.6700564971751412\n",
      "\tAnt 15 :\n",
      "\t\tPath: [50, 42, 12, 22, 65, 59, 43, 10, 54, 38, 23, 63, 49, 6, 30]\n",
      "\t\tAccuracy: 0.7068926553672317\n",
      "\tAnt 16 :\n",
      "\t\tPath: [29, 28, 50, 41, 27, 63, 59, 24, 9, 19, 11, 38, 33, 18, 6]\n",
      "\t\tAccuracy: 0.6868361581920903\n",
      "\tAnt 17 :\n",
      "\t\tPath: [63, 37, 43, 56, 11, 44, 38, 46, 34, 52, 0, 25, 14, 16, 50]\n",
      "\t\tAccuracy: 0.6802259887005649\n",
      "\tAnt 18 :\n",
      "\t\tPath: [50, 44, 48, 25, 38, 58, 8, 59, 12, 23, 57, 14, 42, 68, 46]\n",
      "\t\tAccuracy: 0.6936158192090396\n",
      "\tAnt 19 :\n",
      "\t\tPath: [42, 21, 67, 33, 54, 36, 25, 31, 49, 63, 19, 0, 53, 48, 68]\n",
      "\t\tAccuracy: 0.632994350282486\n",
      "Colony 14 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [44, 34, 49, 50, 63, 28, 10, 16, 52, 43, 38, 8, 12, 65, 53]\n",
      "\t\tAccuracy: 0.7037853107344633\n",
      "\tAnt 1 :\n",
      "\t\tPath: [69, 50, 59, 2, 22, 20, 60, 56, 11, 8, 65, 48, 21, 19, 38]\n",
      "\t\tAccuracy: 0.7238983050847457\n",
      "\tAnt 2 :\n",
      "\t\tPath: [28, 63, 13, 5, 65, 22, 57, 0, 20, 33, 9, 10, 67, 14, 6]\n",
      "\t\tAccuracy: 0.7238983050847457\n",
      "\tAnt 3 :\n",
      "\t\tPath: [34, 50, 21, 14, 6, 43, 16, 46, 57, 58, 27, 53, 63, 52, 59]\n",
      "\t\tAccuracy: 0.7068361581920903\n",
      "\tAnt 4 :\n",
      "\t\tPath: [12, 56, 33, 25, 6, 38, 50, 63, 67, 28, 27, 3, 43, 30, 65]\n",
      "\t\tAccuracy: 0.6970056497175141\n",
      "\tAnt 5 :\n",
      "\t\tPath: [9, 19, 50, 59, 38, 0, 36, 47, 11, 21, 37, 29, 27, 49, 67]\n",
      "\t\tAccuracy: 0.7004519774011299\n",
      "\tAnt 6 :\n",
      "\t\tPath: [22, 65, 24, 9, 59, 38, 58, 37, 63, 19, 57, 15, 25, 12, 49]\n",
      "\t\tAccuracy: 0.7273446327683615\n",
      "\tAnt 7 :\n",
      "\t\tPath: [27, 68, 52, 42, 38, 46, 63, 30, 34, 59, 7, 12, 22, 65, 14]\n",
      "\t\tAccuracy: 0.6968926553672316\n",
      "\tAnt 8 :\n",
      "\t\tPath: [12, 42, 43, 9, 25, 19, 33, 48, 5, 49, 24, 44, 37, 53, 47]\n",
      "\t\tAccuracy: 0.6870056497175141\n",
      "\tAnt 9 :\n",
      "\t\tPath: [13, 50, 23, 9, 11, 4, 30, 18, 5, 59, 63, 45, 67, 24, 27]\n",
      "\t\tAccuracy: 0.656497175141243\n",
      "\tAnt 10 :\n",
      "\t\tPath: [22, 44, 12, 2, 8, 51, 0, 63, 9, 18, 48, 43, 11, 57, 7]\n",
      "\t\tAccuracy: 0.693502824858757\n",
      "\tAnt 11 :\n",
      "\t\tPath: [59, 57, 14, 12, 11, 1, 34, 38, 52, 65, 63, 18, 44, 31, 10]\n",
      "\t\tAccuracy: 0.7003954802259887\n",
      "\tAnt 12 :\n",
      "\t\tPath: [12, 37, 50, 42, 2, 57, 25, 19, 22, 67, 15, 18, 14, 20, 46]\n",
      "\t\tAccuracy: 0.7204519774011299\n",
      "\tAnt 13 :\n",
      "\t\tPath: [11, 56, 25, 10, 19, 58, 49, 22, 65, 27, 37, 33, 54, 2, 24]\n",
      "\t\tAccuracy: 0.6870621468926553\n",
      "\tAnt 14 :\n",
      "\t\tPath: [2, 63, 42, 45, 20, 41, 50, 38, 49, 37, 31, 58, 33, 11, 25]\n",
      "\t\tAccuracy: 0.6968926553672317\n",
      "\tAnt 15 :\n",
      "\t\tPath: [40, 52, 15, 1, 34, 59, 19, 45, 43, 8, 51, 63, 6, 44, 28]\n",
      "\t\tAccuracy: 0.7038418079096045\n",
      "\tAnt 16 :\n",
      "\t\tPath: [42, 16, 44, 65, 23, 57, 30, 14, 31, 63, 37, 8, 41, 56, 67]\n",
      "\t\tAccuracy: 0.6998870056497175\n",
      "\tAnt 17 :\n",
      "\t\tPath: [65, 12, 25, 9, 8, 49, 33, 14, 37, 27, 19, 53, 51, 18, 63]\n",
      "\t\tAccuracy: 0.693728813559322\n",
      "\tAnt 18 :\n",
      "\t\tPath: [57, 67, 10, 22, 50, 38, 63, 15, 12, 30, 7, 9, 19, 44, 18]\n",
      "\t\tAccuracy: 0.7038983050847458\n",
      "\tAnt 19 :\n",
      "\t\tPath: [12, 24, 31, 52, 36, 67, 48, 58, 69, 51, 16, 18, 59, 64, 37]\n",
      "\t\tAccuracy: 0.6833333333333333\n",
      "Colony 15 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [19, 67, 65, 49, 68, 33, 14, 63, 52, 54, 38, 44, 12, 13, 43]\n",
      "\t\tAccuracy: 0.6530508474576272\n",
      "\tAnt 1 :\n",
      "\t\tPath: [46, 57, 40, 65, 67, 47, 33, 21, 9, 31, 52, 38, 63, 30, 60]\n",
      "\t\tAccuracy: 0.7105084745762711\n",
      "\tAnt 2 :\n",
      "\t\tPath: [63, 0, 59, 67, 46, 27, 6, 23, 14, 18, 9, 56, 16, 60, 50]\n",
      "\t\tAccuracy: 0.710225988700565\n",
      "\tAnt 3 :\n",
      "\t\tPath: [47, 29, 51, 60, 33, 50, 57, 52, 56, 43, 9, 34, 8, 21, 2]\n",
      "\t\tAccuracy: 0.703728813559322\n",
      "\tAnt 4 :\n",
      "\t\tPath: [63, 50, 32, 30, 10, 7, 42, 38, 21, 28, 19, 29, 51, 44, 41]\n",
      "\t\tAccuracy: 0.6837288135593219\n",
      "\tAnt 5 :\n",
      "\t\tPath: [30, 67, 52, 56, 23, 29, 36, 15, 10, 45, 20, 49, 14, 28, 19]\n",
      "\t\tAccuracy: 0.7002259887005648\n",
      "\tAnt 6 :\n",
      "\t\tPath: [63, 49, 28, 40, 65, 59, 11, 22, 29, 57, 25, 30, 48, 16, 18]\n",
      "\t\tAccuracy: 0.6767796610169492\n",
      "\tAnt 7 :\n",
      "\t\tPath: [2, 27, 31, 33, 11, 24, 63, 30, 46, 18, 13, 14, 57, 6, 42]\n",
      "\t\tAccuracy: 0.7000564971751413\n",
      "\tAnt 8 :\n",
      "\t\tPath: [22, 44, 10, 2, 50, 34, 12, 15, 53, 49, 27, 14, 19, 6, 59]\n",
      "\t\tAccuracy: 0.7241242937853107\n",
      "\tAnt 9 :\n",
      "\t\tPath: [7, 8, 49, 57, 58, 48, 2, 44, 63, 29, 30, 24, 20, 18, 38]\n",
      "\t\tAccuracy: 0.7340677966101694\n",
      "\tAnt 10 :\n",
      "\t\tPath: [10, 56, 32, 37, 14, 52, 19, 16, 65, 43, 15, 42, 67, 53, 2]\n",
      "\t\tAccuracy: 0.7542372881355931\n",
      "\tAnt 11 :\n",
      "\t\tPath: [42, 44, 43, 33, 37, 46, 59, 20, 53, 12, 34, 18, 49, 57, 6]\n",
      "\t\tAccuracy: 0.7171186440677966\n",
      "\tAnt 12 :\n",
      "\t\tPath: [38, 31, 9, 69, 37, 60, 27, 30, 63, 18, 50, 59, 14, 49, 43]\n",
      "\t\tAccuracy: 0.7136723163841807\n",
      "\tAnt 13 :\n",
      "\t\tPath: [25, 15, 63, 12, 52, 58, 20, 16, 38, 51, 18, 8, 50, 22, 34]\n",
      "\t\tAccuracy: 0.6733333333333333\n",
      "\tAnt 14 :\n",
      "\t\tPath: [11, 38, 40, 63, 33, 42, 10, 69, 30, 48, 52, 27, 65, 43, 37]\n",
      "\t\tAccuracy: 0.6735028248587571\n",
      "\tAnt 15 :\n",
      "\t\tPath: [2, 31, 52, 67, 18, 33, 36, 59, 38, 21, 29, 43, 30, 14, 51]\n",
      "\t\tAccuracy: 0.6903954802259886\n",
      "\tAnt 16 :\n",
      "\t\tPath: [44, 8, 48, 6, 18, 58, 15, 42, 63, 9, 16, 67, 51, 22, 49]\n",
      "\t\tAccuracy: 0.6767796610169492\n",
      "\tAnt 17 :\n",
      "\t\tPath: [50, 52, 10, 18, 42, 7, 49, 34, 56, 24, 64, 9, 43, 11, 37]\n",
      "\t\tAccuracy: 0.6872881355932202\n",
      "\tAnt 18 :\n",
      "\t\tPath: [12, 34, 44, 59, 36, 54, 11, 21, 27, 18, 2, 57, 67, 49, 52]\n",
      "\t\tAccuracy: 0.6903954802259886\n",
      "\tAnt 19 :\n",
      "\t\tPath: [46, 63, 12, 65, 61, 10, 27, 43, 69, 57, 52, 22, 38, 56, 32]\n",
      "\t\tAccuracy: 0.6903954802259886\n",
      "Colony 16 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [30, 14, 10, 49, 44, 35, 63, 67, 25, 37, 50, 2, 22, 65, 11]\n",
      "\t\tAccuracy: 0.6902824858757063\n",
      "\tAnt 1 :\n",
      "\t\tPath: [59, 2, 57, 60, 64, 37, 63, 67, 40, 49, 18, 9, 34, 51, 29]\n",
      "\t\tAccuracy: 0.7241242937853107\n",
      "\tAnt 2 :\n",
      "\t\tPath: [68, 65, 52, 9, 57, 15, 12, 25, 36, 28, 11, 40, 48, 18, 45]\n",
      "\t\tAccuracy: 0.659774011299435\n",
      "\tAnt 3 :\n",
      "\t\tPath: [7, 19, 12, 9, 6, 52, 65, 56, 16, 8, 11, 25, 2, 36, 15]\n",
      "\t\tAccuracy: 0.690225988700565\n",
      "\tAnt 4 :\n",
      "\t\tPath: [18, 51, 49, 58, 31, 44, 63, 37, 38, 46, 42, 34, 28, 29, 59]\n",
      "\t\tAccuracy: 0.7341242937853107\n",
      "\tAnt 5 :\n",
      "\t\tPath: [13, 59, 36, 63, 46, 21, 14, 67, 8, 34, 12, 25, 49, 43, 16]\n",
      "\t\tAccuracy: 0.7003389830508475\n",
      "\tAnt 6 :\n",
      "\t\tPath: [59, 2, 28, 52, 11, 38, 22, 56, 46, 42, 63, 54, 49, 24, 53]\n",
      "\t\tAccuracy: 0.7408474576271187\n",
      "\tAnt 7 :\n",
      "\t\tPath: [14, 59, 47, 41, 38, 33, 34, 24, 65, 12, 63, 9, 11, 37, 50]\n",
      "\t\tAccuracy: 0.7237853107344632\n",
      "\tAnt 8 :\n",
      "\t\tPath: [48, 43, 10, 37, 57, 25, 63, 9, 19, 21, 28, 2, 30, 42, 5]\n",
      "\t\tAccuracy: 0.6531638418079095\n",
      "\tAnt 9 :\n",
      "\t\tPath: [9, 40, 41, 11, 65, 10, 2, 59, 67, 49, 1, 54, 12, 63, 18]\n",
      "\t\tAccuracy: 0.6803389830508474\n",
      "\tAnt 10 :\n",
      "\t\tPath: [15, 46, 25, 10, 48, 42, 53, 43, 37, 49, 65, 23, 67, 59, 18]\n",
      "\t\tAccuracy: 0.7307909604519774\n",
      "\tAnt 11 :\n",
      "\t\tPath: [67, 12, 14, 11, 50, 25, 44, 53, 18, 34, 0, 38, 59, 51, 29]\n",
      "\t\tAccuracy: 0.707231638418079\n",
      "\tAnt 12 :\n",
      "\t\tPath: [29, 30, 43, 51, 37, 13, 12, 33, 57, 11, 56, 38, 63, 14, 6]\n",
      "\t\tAccuracy: 0.7035593220338983\n",
      "\tAnt 13 :\n",
      "\t\tPath: [49, 34, 47, 59, 6, 54, 42, 8, 28, 19, 36, 37, 60, 38, 65]\n",
      "\t\tAccuracy: 0.6970621468926553\n",
      "\tAnt 14 :\n",
      "\t\tPath: [9, 30, 68, 2, 10, 60, 18, 63, 28, 15, 8, 12, 59, 16, 0]\n",
      "\t\tAccuracy: 0.6835593220338982\n",
      "\tAnt 15 :\n",
      "\t\tPath: [8, 11, 37, 57, 58, 12, 49, 42, 14, 19, 50, 18, 59, 44, 16]\n",
      "\t\tAccuracy: 0.6934463276836158\n",
      "\tAnt 16 :\n",
      "\t\tPath: [7, 57, 63, 46, 37, 18, 20, 11, 14, 24, 65, 43, 51, 58, 56]\n",
      "\t\tAccuracy: 0.7272881355932203\n",
      "\tAnt 17 :\n",
      "\t\tPath: [60, 42, 59, 44, 37, 38, 36, 52, 27, 41, 11, 43, 9, 65, 29]\n",
      "\t\tAccuracy: 0.6938983050847456\n",
      "\tAnt 18 :\n",
      "\t\tPath: [67, 23, 54, 21, 52, 68, 16, 59, 19, 65, 29, 14, 37, 30, 49]\n",
      "\t\tAccuracy: 0.6900000000000001\n",
      "\tAnt 19 :\n",
      "\t\tPath: [67, 44, 37, 42, 53, 6, 18, 52, 38, 64, 56, 31, 19, 63, 30]\n",
      "\t\tAccuracy: 0.6669491525423729\n",
      "Colony 17 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [34, 63, 50, 28, 52, 11, 12, 57, 38, 31, 65, 45, 49, 44, 59]\n",
      "\t\tAccuracy: 0.713728813559322\n",
      "\tAnt 1 :\n",
      "\t\tPath: [67, 43, 57, 7, 9, 59, 10, 37, 19, 2, 21, 34, 65, 11, 38]\n",
      "\t\tAccuracy: 0.7068361581920903\n",
      "\tAnt 2 :\n",
      "\t\tPath: [12, 10, 22, 36, 52, 29, 27, 59, 63, 23, 40, 37, 14, 25, 49]\n",
      "\t\tAccuracy: 0.7071186440677966\n",
      "\tAnt 3 :\n",
      "\t\tPath: [49, 63, 21, 8, 14, 53, 56, 13, 9, 44, 57, 11, 67, 65, 42]\n",
      "\t\tAccuracy: 0.6767231638418079\n",
      "\tAnt 4 :\n",
      "\t\tPath: [12, 65, 54, 44, 30, 49, 10, 20, 56, 48, 29, 9, 15, 60, 52]\n",
      "\t\tAccuracy: 0.733954802259887\n",
      "\tAnt 5 :\n",
      "\t\tPath: [57, 59, 19, 34, 41, 11, 12, 40, 53, 63, 52, 44, 14, 9, 21]\n",
      "\t\tAccuracy: 0.6631638418079097\n",
      "\tAnt 6 :\n",
      "\t\tPath: [12, 38, 50, 56, 15, 36, 43, 52, 67, 46, 14, 8, 11, 63, 45]\n",
      "\t\tAccuracy: 0.6966101694915254\n",
      "\tAnt 7 :\n",
      "\t\tPath: [50, 60, 15, 40, 18, 51, 10, 19, 67, 63, 38, 65, 47, 3, 36]\n",
      "\t\tAccuracy: 0.6970621468926554\n",
      "\tAnt 8 :\n",
      "\t\tPath: [2, 57, 37, 8, 38, 18, 10, 21, 19, 65, 50, 64, 34, 27, 12]\n",
      "\t\tAccuracy: 0.7476271186440677\n",
      "\tAnt 9 :\n",
      "\t\tPath: [23, 31, 29, 65, 11, 59, 43, 38, 14, 3, 63, 37, 53, 5, 33]\n",
      "\t\tAccuracy: 0.6970056497175141\n",
      "\tAnt 10 :\n",
      "\t\tPath: [59, 15, 11, 49, 0, 54, 60, 6, 2, 50, 31, 52, 56, 63, 44]\n",
      "\t\tAccuracy: 0.6767231638418079\n",
      "\tAnt 11 :\n",
      "\t\tPath: [13, 58, 33, 57, 48, 9, 44, 42, 24, 63, 52, 22, 38, 2, 12]\n",
      "\t\tAccuracy: 0.737231638418079\n",
      "\tAnt 12 :\n",
      "\t\tPath: [25, 63, 44, 67, 68, 10, 31, 64, 7, 33, 42, 54, 23, 38, 12]\n",
      "\t\tAccuracy: 0.663502824858757\n",
      "\tAnt 13 :\n",
      "\t\tPath: [15, 31, 43, 57, 68, 2, 12, 67, 51, 19, 44, 20, 58, 59, 23]\n",
      "\t\tAccuracy: 0.6666666666666666\n",
      "\tAnt 14 :\n",
      "\t\tPath: [65, 38, 18, 30, 58, 16, 10, 44, 15, 51, 53, 54, 11, 43, 57]\n",
      "\t\tAccuracy: 0.6428813559322035\n",
      "\tAnt 15 :\n",
      "\t\tPath: [65, 10, 14, 43, 44, 11, 9, 67, 30, 59, 23, 38, 56, 8, 18]\n",
      "\t\tAccuracy: 0.6968926553672316\n",
      "\tAnt 16 :\n",
      "\t\tPath: [36, 65, 23, 11, 57, 51, 46, 50, 19, 18, 33, 48, 59, 54, 25]\n",
      "\t\tAccuracy: 0.6735028248587571\n",
      "\tAnt 17 :\n",
      "\t\tPath: [57, 14, 9, 2, 10, 22, 12, 59, 19, 0, 63, 67, 46, 42, 56]\n",
      "\t\tAccuracy: 0.6969491525423728\n",
      "\tAnt 18 :\n",
      "\t\tPath: [20, 63, 42, 32, 67, 48, 11, 46, 36, 53, 19, 38, 52, 33, 2]\n",
      "\t\tAccuracy: 0.7238418079096045\n",
      "\tAnt 19 :\n",
      "\t\tPath: [40, 51, 38, 10, 28, 63, 18, 12, 30, 29, 53, 9, 59, 8, 68]\n",
      "\t\tAccuracy: 0.6668361581920903\n",
      "Colony 18 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [44, 19, 8, 14, 23, 67, 15, 56, 65, 38, 16, 27, 46, 50, 22]\n",
      "\t\tAccuracy: 0.7003954802259887\n",
      "\tAnt 1 :\n",
      "\t\tPath: [2, 59, 67, 14, 65, 56, 22, 41, 30, 10, 19, 57, 58, 52, 63]\n",
      "\t\tAccuracy: 0.7103389830508474\n",
      "\tAnt 2 :\n",
      "\t\tPath: [65, 44, 14, 30, 64, 12, 59, 10, 53, 31, 57, 52, 48, 15, 7]\n",
      "\t\tAccuracy: 0.663276836158192\n",
      "\tAnt 3 :\n",
      "\t\tPath: [25, 24, 12, 16, 7, 8, 41, 52, 50, 58, 38, 59, 48, 22, 36]\n",
      "\t\tAccuracy: 0.713728813559322\n",
      "\tAnt 4 :\n",
      "\t\tPath: [42, 15, 53, 11, 38, 51, 8, 65, 50, 28, 34, 58, 18, 23, 9]\n",
      "\t\tAccuracy: 0.6533333333333333\n",
      "\tAnt 5 :\n",
      "\t\tPath: [21, 42, 49, 44, 11, 65, 57, 43, 6, 40, 59, 14, 9, 37, 60]\n",
      "\t\tAccuracy: 0.7204519774011299\n",
      "\tAnt 6 :\n",
      "\t\tPath: [37, 58, 53, 46, 57, 65, 2, 59, 50, 42, 41, 16, 67, 52, 11]\n",
      "\t\tAccuracy: 0.6496610169491526\n",
      "\tAnt 7 :\n",
      "\t\tPath: [25, 2, 60, 12, 38, 58, 21, 11, 10, 37, 44, 27, 28, 41, 68]\n",
      "\t\tAccuracy: 0.713954802259887\n",
      "\tAnt 8 :\n",
      "\t\tPath: [60, 14, 25, 38, 8, 49, 22, 57, 23, 64, 52, 48, 20, 43, 67]\n",
      "\t\tAccuracy: 0.7073446327683616\n",
      "\tAnt 9 :\n",
      "\t\tPath: [57, 59, 65, 42, 9, 37, 12, 19, 67, 53, 25, 29, 52, 40, 27]\n",
      "\t\tAccuracy: 0.6866666666666668\n",
      "\tAnt 10 :\n",
      "\t\tPath: [2, 42, 6, 9, 12, 59, 36, 65, 63, 38, 50, 41, 48, 20, 19]\n",
      "\t\tAccuracy: 0.737231638418079\n",
      "\tAnt 11 :\n",
      "\t\tPath: [15, 29, 33, 45, 25, 50, 14, 57, 20, 40, 63, 22, 7, 2, 43]\n",
      "\t\tAccuracy: 0.7340112994350283\n",
      "\tAnt 12 :\n",
      "\t\tPath: [25, 59, 16, 63, 20, 43, 18, 45, 21, 48, 0, 9, 14, 44, 41]\n",
      "\t\tAccuracy: 0.7002824858757062\n",
      "\tAnt 13 :\n",
      "\t\tPath: [44, 67, 49, 14, 5, 58, 59, 0, 38, 25, 8, 2, 54, 52, 11]\n",
      "\t\tAccuracy: 0.6799999999999999\n",
      "\tAnt 14 :\n",
      "\t\tPath: [58, 14, 37, 38, 9, 6, 20, 50, 11, 10, 53, 12, 57, 63, 40]\n",
      "\t\tAccuracy: 0.7135593220338983\n",
      "\tAnt 15 :\n",
      "\t\tPath: [67, 53, 38, 68, 46, 34, 48, 24, 8, 14, 28, 57, 16, 47, 44]\n",
      "\t\tAccuracy: 0.7070056497175141\n",
      "\tAnt 16 :\n",
      "\t\tPath: [57, 11, 40, 8, 59, 6, 43, 34, 53, 22, 50, 58, 14, 25, 41]\n",
      "\t\tAccuracy: 0.69\n",
      "\tAnt 17 :\n",
      "\t\tPath: [44, 63, 50, 38, 14, 43, 52, 12, 67, 33, 51, 59, 57, 10, 11]\n",
      "\t\tAccuracy: 0.7174576271186441\n",
      "\tAnt 18 :\n",
      "\t\tPath: [25, 6, 22, 43, 51, 49, 41, 5, 29, 69, 27, 36, 9, 16, 21]\n",
      "\t\tAccuracy: 0.6394350282485876\n",
      "\tAnt 19 :\n",
      "\t\tPath: [24, 63, 47, 30, 50, 25, 21, 60, 41, 12, 44, 11, 58, 51, 52]\n",
      "\t\tAccuracy: 0.6732203389830508\n",
      "Colony 19 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [42, 56, 59, 19, 63, 49, 14, 38, 12, 6, 2, 9, 67, 43, 68]\n",
      "\t\tAccuracy: 0.723728813559322\n",
      "\tAnt 1 :\n",
      "\t\tPath: [57, 6, 14, 50, 37, 38, 44, 8, 18, 51, 15, 20, 28, 42, 59]\n",
      "\t\tAccuracy: 0.7405649717514124\n",
      "\tAnt 2 :\n",
      "\t\tPath: [23, 27, 43, 63, 8, 16, 57, 30, 59, 53, 37, 48, 12, 19, 52]\n",
      "\t\tAccuracy: 0.6732768361581921\n",
      "\tAnt 3 :\n",
      "\t\tPath: [37, 44, 28, 27, 49, 43, 56, 9, 14, 57, 40, 25, 8, 38, 65]\n",
      "\t\tAccuracy: 0.7342937853107345\n",
      "\tAnt 4 :\n",
      "\t\tPath: [59, 10, 18, 47, 9, 50, 51, 63, 58, 49, 22, 28, 43, 67, 57]\n",
      "\t\tAccuracy: 0.713728813559322\n",
      "\tAnt 5 :\n",
      "\t\tPath: [6, 9, 14, 25, 27, 58, 34, 63, 56, 46, 67, 37, 28, 57, 49]\n",
      "\t\tAccuracy: 0.771186440677966\n",
      "\tAnt 6 :\n",
      "\t\tPath: [56, 22, 49, 19, 29, 12, 60, 46, 67, 8, 37, 18, 41, 14, 63]\n",
      "\t\tAccuracy: 0.7103389830508474\n",
      "\tAnt 7 :\n",
      "\t\tPath: [57, 38, 33, 67, 10, 48, 45, 37, 65, 9, 12, 21, 44, 49, 59]\n",
      "\t\tAccuracy: 0.6567796610169492\n",
      "\tAnt 8 :\n",
      "\t\tPath: [69, 58, 53, 63, 28, 42, 36, 31, 38, 8, 33, 14, 15, 43, 12]\n",
      "\t\tAccuracy: 0.6766101694915253\n",
      "\tAnt 9 :\n",
      "\t\tPath: [0, 49, 50, 57, 9, 22, 65, 38, 53, 27, 18, 7, 67, 12, 16]\n",
      "\t\tAccuracy: 0.7033898305084746\n",
      "\tAnt 10 :\n",
      "\t\tPath: [9, 52, 68, 38, 65, 22, 67, 56, 30, 59, 28, 64, 63, 11, 42]\n",
      "\t\tAccuracy: 0.6900564971751413\n",
      "\tAnt 11 :\n",
      "\t\tPath: [59, 38, 48, 57, 22, 14, 41, 36, 66, 28, 56, 67, 50, 27, 2]\n",
      "\t\tAccuracy: 0.7306214689265537\n",
      "\tAnt 12 :\n",
      "\t\tPath: [10, 53, 46, 47, 36, 14, 38, 37, 29, 24, 19, 2, 59, 63, 51]\n",
      "\t\tAccuracy: 0.7711864406779662\n",
      "\tAnt 13 :\n",
      "\t\tPath: [67, 9, 31, 11, 49, 29, 50, 3, 53, 65, 1, 12, 38, 18, 14]\n",
      "\t\tAccuracy: 0.6363841807909605\n",
      "\tAnt 14 :\n",
      "\t\tPath: [14, 19, 37, 34, 52, 57, 63, 2, 8, 44, 0, 36, 48, 9, 22]\n",
      "\t\tAccuracy: 0.7171751412429379\n",
      "\tAnt 15 :\n",
      "\t\tPath: [57, 28, 34, 63, 37, 11, 49, 64, 14, 12, 59, 2, 38, 25, 43]\n",
      "\t\tAccuracy: 0.6836158192090395\n",
      "\tAnt 16 :\n",
      "\t\tPath: [49, 60, 43, 18, 16, 30, 28, 59, 57, 50, 63, 15, 12, 21, 42]\n",
      "\t\tAccuracy: 0.6968361581920903\n",
      "\tAnt 17 :\n",
      "\t\tPath: [7, 63, 59, 52, 58, 12, 11, 42, 36, 40, 24, 54, 6, 10, 38]\n",
      "\t\tAccuracy: 0.683276836158192\n",
      "\tAnt 18 :\n",
      "\t\tPath: [67, 22, 63, 18, 12, 49, 43, 10, 34, 50, 57, 14, 56, 9, 33]\n",
      "\t\tAccuracy: 0.6701129943502824\n",
      "\tAnt 19 :\n",
      "\t\tPath: [14, 13, 21, 24, 38, 31, 58, 40, 64, 63, 51, 65, 34, 43, 10]\n",
      "\t\tAccuracy: 0.6598870056497176\n",
      "Colony 20 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [19, 50, 27, 16, 14, 40, 30, 52, 6, 38, 49, 56, 28, 0, 36]\n",
      "\t\tAccuracy: 0.7070621468926553\n",
      "\tAnt 1 :\n",
      "\t\tPath: [12, 0, 6, 44, 59, 15, 30, 38, 37, 64, 65, 50, 57, 53, 25]\n",
      "\t\tAccuracy: 0.7003389830508474\n",
      "\tAnt 2 :\n",
      "\t\tPath: [19, 18, 30, 65, 11, 59, 50, 43, 37, 3, 68, 38, 41, 67, 44]\n",
      "\t\tAccuracy: 0.7205649717514124\n",
      "\tAnt 3 :\n",
      "\t\tPath: [37, 56, 14, 21, 18, 57, 24, 6, 10, 44, 38, 52, 63, 43, 9]\n",
      "\t\tAccuracy: 0.7174011299435028\n",
      "\tAnt 4 :\n",
      "\t\tPath: [50, 51, 33, 12, 63, 38, 19, 56, 14, 46, 59, 10, 65, 41, 48]\n",
      "\t\tAccuracy: 0.6866101694915254\n",
      "\tAnt 5 :\n",
      "\t\tPath: [14, 67, 63, 54, 60, 58, 25, 8, 57, 28, 65, 68, 50, 18, 41]\n",
      "\t\tAccuracy: 0.6833333333333333\n",
      "\tAnt 6 :\n",
      "\t\tPath: [56, 38, 43, 59, 14, 13, 16, 9, 58, 65, 63, 19, 12, 30, 42]\n",
      "\t\tAccuracy: 0.6867796610169491\n",
      "\tAnt 7 :\n",
      "\t\tPath: [50, 65, 6, 31, 9, 0, 23, 52, 57, 64, 59, 25, 67, 34, 16]\n",
      "\t\tAccuracy: 0.6835028248587571\n",
      "\tAnt 8 :\n",
      "\t\tPath: [49, 22, 59, 6, 38, 43, 25, 44, 15, 46, 12, 58, 8, 14, 67]\n",
      "\t\tAccuracy: 0.7238418079096045\n",
      "\tAnt 9 :\n",
      "\t\tPath: [11, 18, 68, 59, 38, 63, 50, 22, 6, 10, 56, 37, 54, 23, 47]\n",
      "\t\tAccuracy: 0.7241242937853107\n",
      "\tAnt 10 :\n",
      "\t\tPath: [59, 12, 57, 16, 33, 19, 37, 50, 52, 43, 28, 54, 6, 9, 49]\n",
      "\t\tAccuracy: 0.7270621468926554\n",
      "\tAnt 11 :\n",
      "\t\tPath: [2, 63, 25, 60, 9, 29, 38, 19, 6, 67, 41, 21, 15, 59, 31]\n",
      "\t\tAccuracy: 0.6903389830508474\n",
      "\tAnt 12 :\n",
      "\t\tPath: [11, 65, 12, 8, 57, 16, 2, 31, 22, 60, 10, 46, 49, 18, 9]\n",
      "\t\tAccuracy: 0.7373446327683615\n",
      "\tAnt 13 :\n",
      "\t\tPath: [27, 38, 65, 43, 50, 67, 10, 19, 31, 52, 15, 5, 28, 22, 8]\n",
      "\t\tAccuracy: 0.703954802259887\n",
      "\tAnt 14 :\n",
      "\t\tPath: [9, 63, 57, 67, 42, 25, 68, 2, 29, 10, 52, 60, 20, 6, 50]\n",
      "\t\tAccuracy: 0.7238418079096045\n",
      "\tAnt 15 :\n",
      "\t\tPath: [50, 9, 38, 33, 14, 57, 67, 2, 47, 18, 19, 31, 37, 60, 12]\n",
      "\t\tAccuracy: 0.7375706214689266\n",
      "\tAnt 16 :\n",
      "\t\tPath: [57, 21, 20, 43, 12, 53, 2, 28, 37, 14, 38, 34, 9, 45, 25]\n",
      "\t\tAccuracy: 0.7138983050847457\n",
      "\tAnt 17 :\n",
      "\t\tPath: [57, 14, 18, 49, 16, 37, 43, 48, 46, 59, 19, 67, 30, 22, 38]\n",
      "\t\tAccuracy: 0.727231638418079\n",
      "\tAnt 18 :\n",
      "\t\tPath: [8, 63, 57, 18, 20, 65, 44, 14, 43, 50, 53, 25, 38, 41, 48]\n",
      "\t\tAccuracy: 0.7137853107344633\n",
      "\tAnt 19 :\n",
      "\t\tPath: [22, 38, 15, 53, 44, 50, 59, 46, 48, 12, 68, 67, 42, 16, 47]\n",
      "\t\tAccuracy: 0.6899435028248588\n",
      "Colony 21 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [33, 46, 0, 44, 34, 29, 59, 27, 57, 14, 25, 54, 48, 30, 52]\n",
      "\t\tAccuracy: 0.6935593220338984\n",
      "\tAnt 1 :\n",
      "\t\tPath: [51, 25, 56, 9, 50, 58, 19, 30, 22, 27, 38, 10, 15, 65, 12]\n",
      "\t\tAccuracy: 0.6666666666666666\n",
      "\tAnt 2 :\n",
      "\t\tPath: [14, 57, 49, 18, 52, 54, 23, 42, 58, 29, 21, 67, 19, 38, 9]\n",
      "\t\tAccuracy: 0.6903954802259887\n",
      "\tAnt 3 :\n",
      "\t\tPath: [43, 12, 56, 14, 63, 44, 38, 49, 29, 2, 8, 51, 59, 36, 57]\n",
      "\t\tAccuracy: 0.7240112994350282\n",
      "\tAnt 4 :\n",
      "\t\tPath: [57, 31, 24, 0, 59, 36, 67, 5, 37, 2, 29, 47, 43, 25, 23]\n",
      "\t\tAccuracy: 0.7270621468926552\n",
      "\tAnt 5 :\n",
      "\t\tPath: [10, 14, 63, 44, 23, 19, 61, 50, 34, 0, 9, 43, 18, 67, 28]\n",
      "\t\tAccuracy: 0.700225988700565\n",
      "\tAnt 6 :\n",
      "\t\tPath: [34, 63, 6, 57, 10, 59, 13, 67, 29, 54, 37, 7, 65, 52, 38]\n",
      "\t\tAccuracy: 0.7173446327683616\n",
      "\tAnt 7 :\n",
      "\t\tPath: [22, 23, 15, 37, 51, 13, 57, 49, 33, 8, 67, 18, 44, 19, 12]\n",
      "\t\tAccuracy: 0.7036723163841808\n",
      "\tAnt 8 :\n",
      "\t\tPath: [10, 50, 19, 9, 5, 29, 25, 22, 28, 16, 65, 57, 27, 42, 56]\n",
      "\t\tAccuracy: 0.6970056497175141\n",
      "\tAnt 9 :\n",
      "\t\tPath: [2, 22, 9, 44, 12, 14, 20, 59, 63, 37, 19, 25, 16, 27, 43]\n",
      "\t\tAccuracy: 0.7071186440677966\n",
      "\tAnt 10 :\n",
      "\t\tPath: [14, 8, 51, 57, 50, 49, 52, 38, 10, 37, 36, 12, 0, 65, 25]\n",
      "\t\tAccuracy: 0.6971186440677967\n",
      "\tAnt 11 :\n",
      "\t\tPath: [58, 44, 49, 56, 11, 34, 25, 14, 46, 27, 52, 21, 28, 59, 60]\n",
      "\t\tAccuracy: 0.7241242937853107\n",
      "\tAnt 12 :\n",
      "\t\tPath: [13, 10, 19, 25, 41, 52, 38, 9, 0, 2, 65, 33, 14, 18, 67]\n",
      "\t\tAccuracy: 0.7172881355932204\n",
      "\tAnt 13 :\n",
      "\t\tPath: [9, 63, 57, 11, 19, 38, 49, 14, 37, 48, 54, 44, 59, 12, 25]\n",
      "\t\tAccuracy: 0.6832203389830509\n",
      "\tAnt 14 :\n",
      "\t\tPath: [14, 19, 9, 21, 34, 51, 25, 59, 33, 57, 37, 43, 6, 41, 28]\n",
      "\t\tAccuracy: 0.717457627118644\n",
      "\tAnt 15 :\n",
      "\t\tPath: [14, 57, 30, 59, 34, 38, 67, 12, 24, 53, 23, 51, 68, 63, 36]\n",
      "\t\tAccuracy: 0.6667796610169491\n",
      "\tAnt 16 :\n",
      "\t\tPath: [10, 6, 33, 37, 9, 49, 60, 12, 25, 11, 56, 43, 50, 59, 18]\n",
      "\t\tAccuracy: 0.7107344632768362\n",
      "\tAnt 17 :\n",
      "\t\tPath: [56, 67, 52, 1, 14, 37, 38, 65, 59, 50, 12, 10, 33, 57, 43]\n",
      "\t\tAccuracy: 0.7205084745762712\n",
      "\tAnt 18 :\n",
      "\t\tPath: [65, 42, 5, 58, 38, 24, 37, 69, 22, 36, 63, 20, 2, 48, 28]\n",
      "\t\tAccuracy: 0.737457627118644\n",
      "\tAnt 19 :\n",
      "\t\tPath: [43, 38, 63, 50, 11, 41, 60, 67, 44, 52, 46, 54, 22, 18, 57]\n",
      "\t\tAccuracy: 0.7005084745762712\n",
      "Colony 22 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [53, 25, 54, 28, 9, 21, 14, 38, 50, 42, 22, 19, 27, 65, 36]\n",
      "\t\tAccuracy: 0.690225988700565\n",
      "\tAnt 1 :\n",
      "\t\tPath: [10, 23, 36, 33, 6, 9, 63, 14, 60, 31, 57, 69, 44, 43, 40]\n",
      "\t\tAccuracy: 0.7036723163841808\n",
      "\tAnt 2 :\n",
      "\t\tPath: [10, 25, 57, 47, 44, 65, 21, 11, 24, 53, 67, 12, 51, 63, 48]\n",
      "\t\tAccuracy: 0.6834463276836158\n",
      "\tAnt 3 :\n",
      "\t\tPath: [58, 10, 9, 59, 30, 20, 14, 44, 68, 57, 29, 65, 21, 38, 43]\n",
      "\t\tAccuracy: 0.7070056497175141\n",
      "\tAnt 4 :\n",
      "\t\tPath: [42, 12, 37, 22, 25, 68, 58, 59, 46, 2, 67, 19, 50, 11, 30]\n",
      "\t\tAccuracy: 0.7305084745762711\n",
      "\tAnt 5 :\n",
      "\t\tPath: [48, 65, 13, 33, 57, 60, 43, 37, 63, 42, 28, 12, 46, 11, 20]\n",
      "\t\tAccuracy: 0.7307344632768361\n",
      "\tAnt 6 :\n",
      "\t\tPath: [42, 52, 27, 14, 63, 9, 37, 36, 65, 19, 59, 60, 49, 56, 67]\n",
      "\t\tAccuracy: 0.7170056497175141\n",
      "\tAnt 7 :\n",
      "\t\tPath: [65, 23, 51, 66, 14, 58, 37, 44, 63, 53, 57, 59, 28, 9, 12]\n",
      "\t\tAccuracy: 0.7070621468926553\n",
      "\tAnt 8 :\n",
      "\t\tPath: [9, 10, 40, 63, 2, 58, 8, 38, 51, 59, 65, 46, 36, 44, 48]\n",
      "\t\tAccuracy: 0.7338418079096044\n",
      "\tAnt 9 :\n",
      "\t\tPath: [43, 64, 69, 50, 21, 67, 65, 3, 31, 49, 38, 22, 42, 52, 14]\n",
      "\t\tAccuracy: 0.71045197740113\n",
      "\tAnt 10 :\n",
      "\t\tPath: [65, 40, 6, 59, 18, 37, 69, 2, 51, 25, 9, 44, 57, 50, 11]\n",
      "\t\tAccuracy: 0.6801129943502825\n",
      "\tAnt 11 :\n",
      "\t\tPath: [12, 29, 38, 1, 8, 40, 60, 15, 63, 44, 59, 36, 10, 46, 52]\n",
      "\t\tAccuracy: 0.7137853107344633\n",
      "\tAnt 12 :\n",
      "\t\tPath: [23, 53, 6, 67, 59, 12, 38, 49, 60, 63, 18, 57, 10, 19, 48]\n",
      "\t\tAccuracy: 0.6903389830508474\n",
      "\tAnt 13 :\n",
      "\t\tPath: [23, 43, 14, 22, 12, 57, 63, 50, 38, 53, 6, 47, 19, 33, 44]\n",
      "\t\tAccuracy: 0.7169491525423728\n",
      "\tAnt 14 :\n",
      "\t\tPath: [65, 69, 18, 51, 9, 43, 67, 33, 52, 50, 38, 63, 49, 15, 14]\n",
      "\t\tAccuracy: 0.6733333333333333\n",
      "\tAnt 15 :\n",
      "\t\tPath: [34, 59, 12, 9, 10, 43, 63, 65, 6, 0, 53, 37, 51, 42, 56]\n",
      "\t\tAccuracy: 0.6899435028248588\n",
      "\tAnt 16 :\n",
      "\t\tPath: [58, 50, 11, 40, 57, 44, 29, 20, 9, 30, 59, 63, 25, 43, 65]\n",
      "\t\tAccuracy: 0.7171751412429378\n",
      "\tAnt 17 :\n",
      "\t\tPath: [20, 38, 49, 52, 9, 5, 18, 44, 50, 42, 63, 47, 37, 10, 69]\n",
      "\t\tAccuracy: 0.6974011299435027\n",
      "\tAnt 18 :\n",
      "\t\tPath: [14, 57, 42, 29, 67, 31, 7, 65, 43, 23, 53, 51, 50, 9, 25]\n",
      "\t\tAccuracy: 0.6633898305084746\n",
      "\tAnt 19 :\n",
      "\t\tPath: [68, 8, 43, 67, 20, 57, 51, 41, 50, 49, 33, 0, 9, 46, 19]\n",
      "\t\tAccuracy: 0.7205084745762711\n",
      "Colony 23 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [12, 10, 14, 51, 25, 63, 30, 11, 65, 41, 9, 47, 18, 50, 31]\n",
      "\t\tAccuracy: 0.6326553672316384\n",
      "\tAnt 1 :\n",
      "\t\tPath: [18, 12, 67, 7, 65, 30, 52, 57, 15, 63, 36, 33, 11, 37, 6]\n",
      "\t\tAccuracy: 0.6868926553672317\n",
      "\tAnt 2 :\n",
      "\t\tPath: [44, 37, 52, 41, 15, 13, 57, 21, 30, 53, 8, 38, 43, 36, 20]\n",
      "\t\tAccuracy: 0.713728813559322\n",
      "\tAnt 3 :\n",
      "\t\tPath: [14, 12, 0, 43, 22, 25, 59, 53, 63, 38, 2, 5, 67, 9, 51]\n",
      "\t\tAccuracy: 0.6968361581920903\n",
      "\tAnt 4 :\n",
      "\t\tPath: [59, 18, 14, 50, 56, 37, 16, 31, 19, 60, 33, 67, 45, 21, 68]\n",
      "\t\tAccuracy: 0.6766101694915254\n",
      "\tAnt 5 :\n",
      "\t\tPath: [6, 57, 25, 50, 31, 63, 56, 19, 14, 46, 59, 24, 8, 45, 51]\n",
      "\t\tAccuracy: 0.7036723163841807\n",
      "\tAnt 6 :\n",
      "\t\tPath: [5, 6, 49, 48, 37, 41, 47, 18, 27, 40, 57, 21, 44, 65, 19]\n",
      "\t\tAccuracy: 0.6868926553672317\n",
      "\tAnt 7 :\n",
      "\t\tPath: [2, 54, 11, 37, 43, 14, 23, 49, 47, 59, 18, 50, 25, 57, 36]\n",
      "\t\tAccuracy: 0.7138418079096045\n",
      "\tAnt 8 :\n",
      "\t\tPath: [11, 12, 58, 57, 6, 13, 52, 59, 36, 56, 65, 38, 49, 25, 41]\n",
      "\t\tAccuracy: 0.7036723163841808\n",
      "\tAnt 9 :\n",
      "\t\tPath: [2, 59, 53, 12, 24, 54, 38, 52, 30, 9, 36, 8, 63, 57, 51]\n",
      "\t\tAccuracy: 0.7105649717514124\n",
      "\tAnt 10 :\n",
      "\t\tPath: [27, 52, 43, 57, 25, 58, 9, 48, 63, 60, 59, 44, 38, 56, 28]\n",
      "\t\tAccuracy: 0.6970621468926553\n",
      "\tAnt 11 :\n",
      "\t\tPath: [48, 9, 37, 29, 59, 12, 45, 44, 14, 53, 67, 33, 57, 27, 10]\n",
      "\t\tAccuracy: 0.7271751412429378\n",
      "\tAnt 12 :\n",
      "\t\tPath: [37, 11, 41, 12, 57, 19, 6, 20, 52, 58, 5, 54, 8, 42, 50]\n",
      "\t\tAccuracy: 0.6866666666666666\n",
      "\tAnt 13 :\n",
      "\t\tPath: [29, 2, 58, 14, 50, 6, 33, 37, 53, 30, 34, 19, 38, 21, 16]\n",
      "\t\tAccuracy: 0.713728813559322\n",
      "\tAnt 14 :\n",
      "\t\tPath: [11, 43, 9, 44, 56, 52, 28, 16, 46, 59, 49, 50, 10, 33, 48]\n",
      "\t\tAccuracy: 0.7004519774011299\n",
      "\tAnt 15 :\n",
      "\t\tPath: [6, 38, 44, 63, 52, 19, 11, 15, 42, 46, 22, 40, 57, 60, 33]\n",
      "\t\tAccuracy: 0.7107344632768362\n",
      "\tAnt 16 :\n",
      "\t\tPath: [10, 56, 22, 49, 2, 15, 28, 34, 12, 67, 40, 52, 68, 19, 21]\n",
      "\t\tAccuracy: 0.6598305084745764\n",
      "\tAnt 17 :\n",
      "\t\tPath: [31, 12, 57, 68, 67, 44, 50, 36, 63, 65, 37, 11, 54, 30, 46]\n",
      "\t\tAccuracy: 0.6563276836158192\n",
      "\tAnt 18 :\n",
      "\t\tPath: [38, 63, 25, 14, 8, 44, 18, 19, 59, 9, 27, 60, 0, 49, 50]\n",
      "\t\tAccuracy: 0.7340677966101694\n",
      "\tAnt 19 :\n",
      "\t\tPath: [12, 65, 29, 30, 46, 51, 38, 9, 59, 21, 67, 57, 43, 58, 44]\n",
      "\t\tAccuracy: 0.7241807909604521\n",
      "Colony 24 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [42, 65, 16, 11, 18, 58, 46, 67, 56, 36, 10, 52, 15, 22, 37]\n",
      "\t\tAccuracy: 0.6429943502824859\n",
      "\tAnt 1 :\n",
      "\t\tPath: [28, 16, 14, 58, 38, 12, 59, 44, 25, 42, 11, 43, 9, 15, 37]\n",
      "\t\tAccuracy: 0.6834463276836158\n",
      "\tAnt 2 :\n",
      "\t\tPath: [25, 22, 19, 43, 33, 50, 40, 52, 12, 51, 18, 67, 44, 31, 11]\n",
      "\t\tAccuracy: 0.6769491525423729\n",
      "\tAnt 3 :\n",
      "\t\tPath: [25, 63, 52, 2, 50, 30, 33, 15, 10, 8, 14, 56, 16, 12, 41]\n",
      "\t\tAccuracy: 0.6901129943502825\n",
      "\tAnt 4 :\n",
      "\t\tPath: [33, 38, 34, 11, 14, 67, 64, 31, 57, 52, 63, 25, 9, 8, 51]\n",
      "\t\tAccuracy: 0.6498870056497175\n",
      "\tAnt 5 :\n",
      "\t\tPath: [14, 34, 30, 22, 57, 67, 2, 52, 29, 9, 15, 28, 42, 27, 5]\n",
      "\t\tAccuracy: 0.7274576271186441\n",
      "\tAnt 6 :\n",
      "\t\tPath: [10, 31, 8, 38, 59, 37, 60, 16, 12, 67, 51, 63, 48, 9, 65]\n",
      "\t\tAccuracy: 0.6734463276836158\n",
      "\tAnt 7 :\n",
      "\t\tPath: [52, 68, 11, 18, 37, 14, 21, 63, 2, 44, 38, 41, 60, 31, 7]\n",
      "\t\tAccuracy: 0.7476836158192091\n",
      "\tAnt 8 :\n",
      "\t\tPath: [16, 49, 53, 63, 24, 14, 58, 54, 19, 21, 5, 38, 9, 43, 13]\n",
      "\t\tAccuracy: 0.6563841807909604\n",
      "\tAnt 9 :\n",
      "\t\tPath: [37, 52, 16, 49, 58, 15, 68, 43, 28, 31, 46, 44, 14, 9, 30]\n",
      "\t\tAccuracy: 0.6768361581920904\n",
      "\tAnt 10 :\n",
      "\t\tPath: [49, 65, 12, 54, 44, 59, 50, 19, 64, 67, 6, 28, 10, 16, 63]\n",
      "\t\tAccuracy: 0.6532203389830509\n",
      "\tAnt 11 :\n",
      "\t\tPath: [6, 28, 63, 0, 50, 25, 43, 19, 48, 37, 51, 31, 27, 60, 16]\n",
      "\t\tAccuracy: 0.7002824858757062\n",
      "\tAnt 12 :\n",
      "\t\tPath: [16, 28, 50, 12, 44, 0, 51, 38, 40, 1, 9, 11, 33, 65, 19]\n",
      "\t\tAccuracy: 0.6801694915254237\n",
      "\tAnt 13 :\n",
      "\t\tPath: [50, 68, 29, 63, 53, 57, 48, 59, 65, 18, 33, 64, 12, 19, 16]\n",
      "\t\tAccuracy: 0.6667796610169491\n",
      "\tAnt 14 :\n",
      "\t\tPath: [27, 25, 63, 65, 30, 42, 19, 4, 37, 43, 12, 38, 24, 23, 10]\n",
      "\t\tAccuracy: 0.7005084745762712\n",
      "\tAnt 15 :\n",
      "\t\tPath: [11, 49, 63, 43, 33, 52, 59, 48, 67, 44, 41, 57, 10, 12, 53]\n",
      "\t\tAccuracy: 0.6835593220338982\n",
      "\tAnt 16 :\n",
      "\t\tPath: [33, 18, 43, 36, 57, 52, 30, 14, 50, 67, 44, 19, 12, 34, 38]\n",
      "\t\tAccuracy: 0.6699435028248587\n",
      "\tAnt 17 :\n",
      "\t\tPath: [22, 12, 56, 18, 57, 59, 19, 14, 24, 44, 25, 33, 38, 9, 53]\n",
      "\t\tAccuracy: 0.7169491525423728\n",
      "\tAnt 18 :\n",
      "\t\tPath: [16, 10, 59, 52, 57, 38, 25, 9, 24, 11, 21, 51, 63, 2, 6]\n",
      "\t\tAccuracy: 0.710225988700565\n",
      "\tAnt 19 :\n",
      "\t\tPath: [18, 44, 63, 9, 65, 37, 45, 19, 14, 28, 64, 59, 10, 20, 38]\n",
      "\t\tAccuracy: 0.7271751412429379\n",
      "Colony 25 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [11, 42, 31, 15, 14, 44, 53, 30, 8, 5, 57, 33, 63, 23, 65]\n",
      "\t\tAccuracy: 0.6867796610169491\n",
      "\tAnt 1 :\n",
      "\t\tPath: [11, 52, 68, 50, 57, 53, 65, 2, 18, 30, 9, 49, 38, 16, 5]\n",
      "\t\tAccuracy: 0.6902824858757062\n",
      "\tAnt 2 :\n",
      "\t\tPath: [57, 15, 46, 42, 25, 65, 67, 49, 63, 50, 41, 51, 22, 0, 14]\n",
      "\t\tAccuracy: 0.7234463276836157\n",
      "\tAnt 3 :\n",
      "\t\tPath: [15, 57, 19, 59, 28, 38, 14, 36, 42, 25, 2, 50, 58, 56, 22]\n",
      "\t\tAccuracy: 0.7071751412429379\n",
      "\tAnt 4 :\n",
      "\t\tPath: [33, 15, 21, 8, 9, 30, 59, 43, 57, 19, 53, 50, 14, 38, 44]\n",
      "\t\tAccuracy: 0.723954802259887\n",
      "\tAnt 5 :\n",
      "\t\tPath: [7, 26, 63, 56, 12, 23, 43, 50, 14, 18, 20, 33, 30, 57, 67]\n",
      "\t\tAccuracy: 0.6663841807909604\n",
      "\tAnt 6 :\n",
      "\t\tPath: [18, 36, 44, 67, 48, 47, 29, 8, 37, 14, 43, 49, 41, 58, 69]\n",
      "\t\tAccuracy: 0.6802259887005649\n",
      "\tAnt 7 :\n",
      "\t\tPath: [42, 34, 63, 67, 38, 53, 14, 49, 37, 19, 59, 10, 57, 9, 65]\n",
      "\t\tAccuracy: 0.6868361581920904\n",
      "\tAnt 8 :\n",
      "\t\tPath: [7, 67, 57, 14, 6, 37, 8, 28, 56, 43, 59, 15, 19, 23, 63]\n",
      "\t\tAccuracy: 0.6935593220338983\n",
      "\tAnt 9 :\n",
      "\t\tPath: [16, 43, 44, 5, 12, 54, 42, 30, 41, 14, 38, 49, 27, 58, 37]\n",
      "\t\tAccuracy: 0.6667231638418079\n",
      "\tAnt 10 :\n",
      "\t\tPath: [33, 44, 43, 56, 14, 31, 18, 16, 63, 34, 12, 48, 50, 8, 38]\n",
      "\t\tAccuracy: 0.6967231638418079\n",
      "\tAnt 11 :\n",
      "\t\tPath: [25, 44, 42, 46, 59, 29, 51, 34, 18, 19, 63, 14, 67, 53, 36]\n",
      "\t\tAccuracy: 0.6602824858757061\n",
      "\tAnt 12 :\n",
      "\t\tPath: [11, 48, 34, 33, 38, 22, 59, 28, 42, 41, 5, 19, 56, 51, 44]\n",
      "\t\tAccuracy: 0.673502824858757\n",
      "\tAnt 13 :\n",
      "\t\tPath: [12, 25, 28, 29, 23, 37, 63, 18, 60, 22, 50, 10, 67, 48, 52]\n",
      "\t\tAccuracy: 0.6767796610169492\n",
      "\tAnt 14 :\n",
      "\t\tPath: [42, 14, 57, 50, 68, 52, 44, 33, 11, 43, 64, 65, 58, 67, 9]\n",
      "\t\tAccuracy: 0.7001694915254237\n",
      "\tAnt 15 :\n",
      "\t\tPath: [15, 53, 10, 9, 57, 51, 12, 8, 28, 52, 18, 2, 37, 44, 34]\n",
      "\t\tAccuracy: 0.7074576271186441\n",
      "\tAnt 16 :\n",
      "\t\tPath: [46, 38, 23, 57, 10, 50, 18, 51, 30, 37, 59, 25, 67, 27, 19]\n",
      "\t\tAccuracy: 0.6936158192090396\n",
      "\tAnt 17 :\n",
      "\t\tPath: [31, 30, 50, 14, 16, 46, 68, 67, 44, 59, 10, 49, 8, 37, 9]\n",
      "\t\tAccuracy: 0.7001129943502825\n",
      "\tAnt 18 :\n",
      "\t\tPath: [22, 6, 63, 51, 57, 9, 2, 14, 15, 65, 18, 27, 37, 12, 46]\n",
      "\t\tAccuracy: 0.7102824858757062\n",
      "\tAnt 19 :\n",
      "\t\tPath: [52, 42, 67, 60, 28, 24, 41, 51, 44, 10, 25, 38, 18, 58, 12]\n",
      "\t\tAccuracy: 0.72090395480226\n",
      "Colony 26 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [19, 56, 52, 47, 33, 40, 41, 53, 8, 59, 38, 34, 46, 67, 21]\n",
      "\t\tAccuracy: 0.6902824858757063\n",
      "\tAnt 1 :\n",
      "\t\tPath: [19, 27, 16, 31, 63, 3, 57, 23, 6, 42, 25, 49, 44, 64, 29]\n",
      "\t\tAccuracy: 0.6935593220338984\n",
      "\tAnt 2 :\n",
      "\t\tPath: [68, 30, 44, 42, 25, 51, 37, 38, 6, 18, 67, 63, 53, 10, 48]\n",
      "\t\tAccuracy: 0.7071751412429379\n",
      "\tAnt 3 :\n",
      "\t\tPath: [2, 21, 38, 65, 58, 68, 63, 59, 37, 44, 25, 52, 57, 54, 47]\n",
      "\t\tAccuracy: 0.733954802259887\n",
      "\tAnt 4 :\n",
      "\t\tPath: [9, 19, 48, 14, 57, 49, 38, 42, 37, 59, 68, 50, 67, 52, 29]\n",
      "\t\tAccuracy: 0.7171186440677966\n",
      "\tAnt 5 :\n",
      "\t\tPath: [23, 38, 57, 52, 59, 36, 41, 15, 16, 25, 67, 51, 65, 20, 10]\n",
      "\t\tAccuracy: 0.734406779661017\n",
      "\tAnt 6 :\n",
      "\t\tPath: [63, 42, 38, 59, 44, 58, 19, 46, 51, 9, 57, 18, 20, 8, 53]\n",
      "\t\tAccuracy: 0.7138418079096044\n",
      "\tAnt 7 :\n",
      "\t\tPath: [63, 11, 27, 16, 25, 46, 43, 44, 65, 10, 58, 38, 37, 49, 51]\n",
      "\t\tAccuracy: 0.6973446327683616\n",
      "\tAnt 8 :\n",
      "\t\tPath: [19, 53, 33, 52, 28, 68, 59, 21, 44, 12, 51, 50, 42, 22, 2]\n",
      "\t\tAccuracy: 0.7171186440677966\n",
      "\tAnt 9 :\n",
      "\t\tPath: [63, 29, 65, 19, 49, 10, 31, 43, 14, 52, 2, 9, 48, 44, 67]\n",
      "\t\tAccuracy: 0.6970621468926554\n",
      "\tAnt 10 :\n",
      "\t\tPath: [5, 9, 14, 65, 12, 18, 57, 8, 22, 54, 20, 63, 43, 38, 46]\n",
      "\t\tAccuracy: 0.7206214689265537\n",
      "\tAnt 11 :\n",
      "\t\tPath: [37, 68, 3, 50, 9, 65, 45, 52, 48, 42, 46, 10, 28, 27, 44]\n",
      "\t\tAccuracy: 0.690225988700565\n",
      "\tAnt 12 :\n",
      "\t\tPath: [9, 68, 27, 52, 30, 37, 65, 10, 18, 20, 51, 53, 44, 38, 59]\n",
      "\t\tAccuracy: 0.6802824858757062\n",
      "\tAnt 13 :\n",
      "\t\tPath: [59, 41, 42, 30, 65, 21, 9, 18, 67, 51, 54, 19, 40, 53, 20]\n",
      "\t\tAccuracy: 0.6938418079096046\n",
      "\tAnt 14 :\n",
      "\t\tPath: [6, 42, 63, 14, 58, 59, 60, 10, 54, 67, 11, 33, 50, 56, 44]\n",
      "\t\tAccuracy: 0.683502824858757\n",
      "\tAnt 15 :\n",
      "\t\tPath: [19, 65, 24, 59, 28, 14, 29, 42, 38, 11, 67, 57, 46, 58, 63]\n",
      "\t\tAccuracy: 0.6566101694915254\n",
      "\tAnt 16 :\n",
      "\t\tPath: [63, 19, 36, 38, 6, 22, 2, 52, 47, 14, 67, 9, 59, 57, 15]\n",
      "\t\tAccuracy: 0.7070056497175142\n",
      "\tAnt 17 :\n",
      "\t\tPath: [14, 22, 65, 10, 9, 38, 52, 6, 2, 63, 64, 50, 49, 5, 59]\n",
      "\t\tAccuracy: 0.7340112994350282\n",
      "\tAnt 18 :\n",
      "\t\tPath: [43, 49, 23, 52, 33, 27, 31, 63, 12, 65, 42, 59, 25, 30, 54]\n",
      "\t\tAccuracy: 0.7070056497175141\n",
      "\tAnt 19 :\n",
      "\t\tPath: [14, 2, 22, 6, 68, 11, 15, 16, 10, 38, 57, 18, 12, 19, 49]\n",
      "\t\tAccuracy: 0.70045197740113\n",
      "Colony 27 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [51, 9, 57, 56, 11, 48, 63, 46, 67, 44, 19, 18, 31, 54, 12]\n",
      "\t\tAccuracy: 0.6529378531073446\n",
      "\tAnt 1 :\n",
      "\t\tPath: [67, 27, 44, 29, 11, 63, 60, 43, 34, 52, 38, 65, 21, 23, 9]\n",
      "\t\tAccuracy: 0.6701129943502824\n",
      "\tAnt 2 :\n",
      "\t\tPath: [63, 67, 20, 66, 2, 38, 65, 68, 0, 15, 30, 8, 36, 52, 53]\n",
      "\t\tAccuracy: 0.6935593220338984\n",
      "\tAnt 3 :\n",
      "\t\tPath: [59, 57, 54, 43, 12, 10, 58, 42, 9, 33, 55, 65, 14, 6, 16]\n",
      "\t\tAccuracy: 0.6933333333333334\n",
      "\tAnt 4 :\n",
      "\t\tPath: [31, 54, 5, 63, 43, 24, 37, 49, 8, 18, 41, 11, 38, 57, 16]\n",
      "\t\tAccuracy: 0.717231638418079\n",
      "\tAnt 5 :\n",
      "\t\tPath: [24, 23, 57, 15, 52, 67, 64, 38, 59, 16, 63, 8, 53, 65, 14]\n",
      "\t\tAccuracy: 0.7006214689265537\n",
      "\tAnt 6 :\n",
      "\t\tPath: [44, 67, 16, 29, 50, 14, 38, 37, 30, 60, 55, 18, 8, 43, 54]\n",
      "\t\tAccuracy: 0.7103954802259886\n",
      "\tAnt 7 :\n",
      "\t\tPath: [5, 50, 57, 31, 34, 18, 38, 44, 63, 12, 48, 58, 29, 59, 42]\n",
      "\t\tAccuracy: 0.6902259887005651\n",
      "\tAnt 8 :\n",
      "\t\tPath: [8, 19, 50, 65, 16, 9, 43, 18, 15, 59, 38, 49, 57, 11, 10]\n",
      "\t\tAccuracy: 0.6905084745762712\n",
      "\tAnt 9 :\n",
      "\t\tPath: [59, 29, 18, 15, 26, 16, 19, 20, 31, 38, 9, 10, 54, 14, 47]\n",
      "\t\tAccuracy: 0.6532768361581921\n",
      "\tAnt 10 :\n",
      "\t\tPath: [32, 65, 25, 30, 46, 43, 2, 14, 53, 9, 50, 11, 68, 48, 31]\n",
      "\t\tAccuracy: 0.723954802259887\n",
      "\tAnt 11 :\n",
      "\t\tPath: [67, 36, 13, 40, 56, 20, 38, 63, 65, 25, 22, 10, 28, 48, 12]\n",
      "\t\tAccuracy: 0.6870056497175141\n",
      "\tAnt 12 :\n",
      "\t\tPath: [25, 31, 18, 51, 15, 69, 11, 20, 53, 14, 57, 19, 42, 38, 52]\n",
      "\t\tAccuracy: 0.69045197740113\n",
      "\tAnt 13 :\n",
      "\t\tPath: [59, 50, 28, 19, 22, 16, 60, 7, 56, 18, 40, 42, 38, 6, 57]\n",
      "\t\tAccuracy: 0.6867231638418079\n",
      "\tAnt 14 :\n",
      "\t\tPath: [28, 56, 22, 21, 45, 46, 57, 44, 23, 9, 42, 41, 11, 25, 36]\n",
      "\t\tAccuracy: 0.7105084745762712\n",
      "\tAnt 15 :\n",
      "\t\tPath: [30, 14, 36, 45, 52, 38, 18, 9, 66, 2, 57, 44, 15, 8, 67]\n",
      "\t\tAccuracy: 0.7307909604519773\n",
      "\tAnt 16 :\n",
      "\t\tPath: [53, 57, 16, 37, 65, 36, 42, 49, 59, 38, 12, 46, 44, 29, 40]\n",
      "\t\tAccuracy: 0.6968926553672317\n",
      "\tAnt 17 :\n",
      "\t\tPath: [8, 18, 37, 19, 67, 59, 14, 38, 9, 44, 56, 12, 15, 13, 57]\n",
      "\t\tAccuracy: 0.6864406779661018\n",
      "\tAnt 18 :\n",
      "\t\tPath: [43, 48, 22, 51, 14, 57, 67, 44, 3, 36, 63, 59, 15, 54, 6]\n",
      "\t\tAccuracy: 0.6868361581920904\n",
      "\tAnt 19 :\n",
      "\t\tPath: [29, 52, 33, 11, 38, 25, 16, 57, 42, 50, 9, 63, 20, 58, 18]\n",
      "\t\tAccuracy: 0.7306779661016949\n",
      "Colony 28 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [49, 9, 50, 38, 34, 21, 57, 14, 48, 30, 15, 28, 63, 59, 6]\n",
      "\t\tAccuracy: 0.7137853107344633\n",
      "\tAnt 1 :\n",
      "\t\tPath: [68, 50, 67, 65, 24, 16, 8, 44, 10, 59, 28, 29, 63, 57, 53]\n",
      "\t\tAccuracy: 0.649774011299435\n",
      "\tAnt 2 :\n",
      "\t\tPath: [59, 50, 12, 57, 21, 27, 48, 38, 34, 28, 37, 41, 23, 22, 19]\n",
      "\t\tAccuracy: 0.6967796610169492\n",
      "\tAnt 3 :\n",
      "\t\tPath: [7, 9, 18, 67, 19, 47, 51, 12, 6, 25, 49, 46, 43, 33, 38]\n",
      "\t\tAccuracy: 0.7003954802259887\n",
      "\tAnt 4 :\n",
      "\t\tPath: [44, 57, 27, 40, 2, 50, 59, 8, 43, 10, 12, 56, 58, 64, 9]\n",
      "\t\tAccuracy: 0.6901129943502824\n",
      "\tAnt 5 :\n",
      "\t\tPath: [28, 63, 69, 9, 59, 16, 43, 20, 8, 33, 10, 30, 47, 44, 38]\n",
      "\t\tAccuracy: 0.7036723163841808\n",
      "\tAnt 6 :\n",
      "\t\tPath: [67, 19, 16, 68, 60, 15, 42, 48, 14, 30, 38, 63, 9, 11, 52]\n",
      "\t\tAccuracy: 0.7001129943502825\n",
      "\tAnt 7 :\n",
      "\t\tPath: [53, 59, 63, 43, 57, 11, 0, 10, 38, 67, 6, 28, 48, 3, 56]\n",
      "\t\tAccuracy: 0.6868361581920903\n",
      "\tAnt 8 :\n",
      "\t\tPath: [51, 44, 65, 57, 19, 10, 37, 30, 33, 23, 43, 56, 38, 14, 52]\n",
      "\t\tAccuracy: 0.6801694915254237\n",
      "\tAnt 9 :\n",
      "\t\tPath: [30, 50, 63, 38, 57, 52, 69, 68, 58, 28, 44, 49, 41, 10, 27]\n",
      "\t\tAccuracy: 0.6800564971751413\n",
      "\tAnt 10 :\n",
      "\t\tPath: [19, 57, 54, 58, 52, 63, 68, 27, 43, 12, 16, 67, 14, 59, 53]\n",
      "\t\tAccuracy: 0.683276836158192\n",
      "\tAnt 11 :\n",
      "\t\tPath: [4, 2, 33, 63, 10, 42, 8, 56, 11, 50, 32, 34, 25, 36, 31]\n",
      "\t\tAccuracy: 0.6935593220338983\n",
      "\tAnt 12 :\n",
      "\t\tPath: [22, 42, 50, 56, 14, 10, 16, 68, 9, 12, 19, 57, 44, 28, 0]\n",
      "\t\tAccuracy: 0.6900564971751413\n",
      "\tAnt 13 :\n",
      "\t\tPath: [10, 63, 43, 38, 51, 50, 16, 52, 9, 69, 44, 46, 8, 67, 65]\n",
      "\t\tAccuracy: 0.6603389830508475\n",
      "\tAnt 14 :\n",
      "\t\tPath: [43, 20, 16, 24, 2, 10, 19, 38, 49, 9, 44, 5, 42, 52, 25]\n",
      "\t\tAccuracy: 0.7036723163841808\n",
      "\tAnt 15 :\n",
      "\t\tPath: [60, 40, 11, 50, 65, 0, 44, 38, 29, 14, 46, 49, 34, 6, 27]\n",
      "\t\tAccuracy: 0.6598870056497175\n",
      "\tAnt 16 :\n",
      "\t\tPath: [5, 50, 9, 48, 65, 63, 30, 49, 14, 33, 52, 59, 37, 23, 67]\n",
      "\t\tAccuracy: 0.7138418079096045\n",
      "\tAnt 17 :\n",
      "\t\tPath: [43, 14, 63, 40, 65, 51, 59, 19, 13, 2, 21, 9, 31, 29, 38]\n",
      "\t\tAccuracy: 0.7072316384180791\n",
      "\tAnt 18 :\n",
      "\t\tPath: [31, 63, 59, 12, 14, 44, 67, 58, 46, 53, 18, 10, 29, 20, 65]\n",
      "\t\tAccuracy: 0.6733333333333333\n",
      "\tAnt 19 :\n",
      "\t\tPath: [56, 63, 2, 16, 15, 19, 14, 43, 22, 52, 67, 44, 9, 49, 65]\n",
      "\t\tAccuracy: 0.6970056497175141\n",
      "Colony 29 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [48, 27, 33, 65, 29, 16, 21, 40, 50, 53, 2, 11, 15, 37, 5]\n",
      "\t\tAccuracy: 0.6633333333333333\n",
      "\tAnt 1 :\n",
      "\t\tPath: [8, 18, 57, 59, 3, 53, 9, 20, 65, 2, 42, 12, 29, 63, 50]\n",
      "\t\tAccuracy: 0.7137853107344633\n",
      "\tAnt 2 :\n",
      "\t\tPath: [56, 12, 30, 25, 37, 65, 20, 53, 59, 14, 36, 23, 34, 11, 51]\n",
      "\t\tAccuracy: 0.6732203389830509\n",
      "\tAnt 3 :\n",
      "\t\tPath: [25, 6, 48, 8, 38, 28, 44, 52, 14, 9, 21, 27, 13, 67, 60]\n",
      "\t\tAccuracy: 0.7174011299435029\n",
      "\tAnt 4 :\n",
      "\t\tPath: [57, 43, 47, 9, 25, 27, 60, 3, 7, 34, 19, 63, 5, 44, 30]\n",
      "\t\tAccuracy: 0.7372881355932204\n",
      "\tAnt 5 :\n",
      "\t\tPath: [11, 33, 9, 42, 58, 19, 38, 67, 30, 37, 18, 54, 59, 44, 51]\n",
      "\t\tAccuracy: 0.6970056497175141\n",
      "\tAnt 6 :\n",
      "\t\tPath: [46, 42, 65, 30, 9, 43, 59, 56, 27, 50, 48, 37, 67, 49, 8]\n",
      "\t\tAccuracy: 0.713954802259887\n",
      "\tAnt 7 :\n",
      "\t\tPath: [54, 12, 19, 15, 51, 59, 43, 36, 7, 57, 16, 38, 56, 33, 64]\n",
      "\t\tAccuracy: 0.6971186440677967\n",
      "\tAnt 8 :\n",
      "\t\tPath: [37, 53, 23, 34, 29, 28, 22, 11, 58, 12, 44, 63, 52, 49, 59]\n",
      "\t\tAccuracy: 0.7069491525423728\n",
      "\tAnt 9 :\n",
      "\t\tPath: [10, 30, 12, 63, 43, 38, 23, 67, 29, 25, 42, 54, 14, 44, 15]\n",
      "\t\tAccuracy: 0.6933898305084746\n",
      "\tAnt 10 :\n",
      "\t\tPath: [11, 63, 57, 49, 29, 24, 10, 44, 52, 18, 65, 28, 67, 56, 38]\n",
      "\t\tAccuracy: 0.7038983050847457\n",
      "\tAnt 11 :\n",
      "\t\tPath: [0, 6, 43, 67, 38, 10, 9, 44, 29, 14, 63, 59, 25, 11, 52]\n",
      "\t\tAccuracy: 0.7140112994350283\n",
      "\tAnt 12 :\n",
      "\t\tPath: [24, 10, 33, 8, 68, 59, 43, 49, 52, 57, 2, 50, 20, 31, 56]\n",
      "\t\tAccuracy: 0.7338983050847456\n",
      "\tAnt 13 :\n",
      "\t\tPath: [23, 37, 38, 49, 58, 10, 43, 52, 3, 50, 47, 67, 19, 25, 14]\n",
      "\t\tAccuracy: 0.7375706214689266\n",
      "\tAnt 14 :\n",
      "\t\tPath: [53, 22, 44, 47, 18, 38, 67, 37, 36, 57, 30, 23, 49, 33, 14]\n",
      "\t\tAccuracy: 0.7103954802259886\n",
      "\tAnt 15 :\n",
      "\t\tPath: [54, 56, 63, 57, 19, 11, 44, 34, 12, 41, 60, 38, 37, 6, 8]\n",
      "\t\tAccuracy: 0.7001694915254237\n",
      "\tAnt 16 :\n",
      "\t\tPath: [68, 57, 28, 29, 36, 56, 42, 50, 58, 2, 14, 22, 48, 46, 10]\n",
      "\t\tAccuracy: 0.7002824858757062\n",
      "\tAnt 17 :\n",
      "\t\tPath: [50, 63, 0, 18, 57, 44, 14, 69, 48, 49, 15, 67, 30, 11, 41]\n",
      "\t\tAccuracy: 0.6496045197740112\n",
      "\tAnt 18 :\n",
      "\t\tPath: [59, 63, 60, 50, 40, 11, 68, 21, 53, 22, 43, 33, 49, 9, 6]\n",
      "\t\tAccuracy: 0.690225988700565\n",
      "\tAnt 19 :\n",
      "\t\tPath: [48, 65, 52, 67, 29, 57, 6, 46, 36, 9, 22, 5, 51, 47, 14]\n",
      "\t\tAccuracy: 0.7038983050847457\n"
     ]
    }
   ],
   "source": [
    "fs.acoFS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final subset of features is:  [23, 37, 38, 49, 58, 10, 43, 52, 3, 50, 47, 67, 19, 25, 14]\n",
      "Top 5 ants:\n",
      "Path:  [10, 30, 12, 63, 43, 38, 23, 67, 29, 25, 42, 54, 14, 44, 15] \t | IPC:  0.0273389665580823\n",
      "Path:  [37, 53, 23, 34, 29, 28, 22, 11, 58, 12, 44, 63, 52, 49, 59] \t | IPC:  0.026420807431140256\n",
      "Path:  [54, 56, 63, 57, 19, 11, 44, 34, 12, 41, 60, 38, 37, 6, 8] \t | IPC:  0.026321821062748826\n",
      "Path:  [11, 33, 9, 42, 58, 19, 38, 67, 30, 37, 18, 54, 59, 44, 51] \t | IPC:  0.02545483680157007\n",
      "Path:  [0, 6, 43, 67, 38, 10, 9, 44, 29, 14, 63, 59, 25, 11, 52] \t | IPC:  0.024523910027034104\n",
      "Number of features:  15\n",
      "Subset of features dataset accuracy:\n",
      "\t CV-Training set:  0.7375706214689266\n",
      "\t Testing set    :  0.696969696969697\n",
      "\t Time elapsed reading data        :  0.038373470306396484\n",
      "\t Time elapsed in LUT compute      :  0.5266597270965576\n",
      "\t Time elapsed reseting values     :  10.205790281295776\n",
      "\t Time elapsed in local search     :  14.626372814178467\n",
      "\t Time elapsed updating pheromones :  0.0069506168365478516\n",
      "\n",
      "TOTAL AUC FROM MODEL:  0.8078968903436988\n",
      "AUC Score ([10, 30, 12, 63, 43, 38, 23, 67, 29, 25, 42, 54, 14, 44, 15], 0.0273389665580823): 0.39013911620294595\n",
      "AUC Score ([37, 53, 23, 34, 29, 28, 22, 11, 58, 12, 44, 63, 52, 49, 59], 0.026420807431140256): 0.728518821603928\n",
      "AUC Score ([54, 56, 63, 57, 19, 11, 44, 34, 12, 41, 60, 38, 37, 6, 8], 0.026321821062748826): 0.4474222585924713\n",
      "AUC Score ([11, 33, 9, 42, 58, 19, 38, 67, 30, 37, 18, 54, 59, 44, 51], 0.02545483680157007): 0.5785597381342061\n",
      "AUC Score ([0, 6, 43, 67, 38, 10, 9, 44, 29, 14, 63, 59, 25, 11, 52], 0.024523910027034104): 0.605973813420622\n"
     ]
    }
   ],
   "source": [
    "fs.printTestingResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23, 37, 38, 49, 58, 10, 43, 52, 3, 50, 47, 67, 19, 25, 14], [57, 43, 47, 9, 25, 27, 60, 3, 7, 34, 19, 63, 5, 44, 30], [24, 10, 33, 8, 68, 59, 43, 49, 52, 57, 2, 50, 20, 31, 56], [25, 6, 48, 8, 38, 28, 44, 52, 14, 9, 21, 27, 13, 67, 60], [0, 6, 43, 67, 38, 10, 9, 44, 29, 14, 63, 59, 25, 11, 52], [46, 42, 65, 30, 9, 43, 59, 56, 27, 50, 48, 37, 67, 49, 8], [8, 18, 57, 59, 3, 53, 9, 20, 65, 2, 42, 12, 29, 63, 50], [53, 22, 44, 47, 18, 38, 67, 37, 36, 57, 30, 23, 49, 33, 14], [37, 53, 23, 34, 29, 28, 22, 11, 58, 12, 44, 63, 52, 49, 59], [11, 63, 57, 49, 29, 24, 10, 44, 52, 18, 65, 28, 67, 56, 38], [48, 65, 52, 67, 29, 57, 6, 46, 36, 9, 22, 5, 51, 47, 14], [68, 57, 28, 29, 36, 56, 42, 50, 58, 2, 14, 22, 48, 46, 10], [54, 56, 63, 57, 19, 11, 44, 34, 12, 41, 60, 38, 37, 6, 8], [54, 12, 19, 15, 51, 59, 43, 36, 7, 57, 16, 38, 56, 33, 64], [11, 33, 9, 42, 58, 19, 38, 67, 30, 37, 18, 54, 59, 44, 51], [10, 30, 12, 63, 43, 38, 23, 67, 29, 25, 42, 54, 14, 44, 15], [59, 63, 60, 50, 40, 11, 68, 21, 53, 22, 43, 33, 49, 9, 6], [56, 12, 30, 25, 37, 65, 20, 53, 59, 14, 36, 23, 34, 11, 51], [48, 27, 33, 65, 29, 16, 21, 40, 50, 53, 2, 11, 15, 37, 5], [50, 63, 0, 18, 57, 44, 14, 69, 48, 49, 15, 67, 30, 11, 41]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[23, 37, 38, 49, 58, 10, 43, 52, 3, 50, 47, 67, 19, 25, 14]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features = fs.selectedPaths()\n",
    "selected_features = selected_features[0]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from classifier import Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# classifier = Classifier(selected_features, './rtfDataSet.csv')\n",
    "# classifier.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1   0.64641   0.758769  0.465238  0.650777   \n",
      "\n",
      "       Loss            Loss Curve  \n",
      "0  0.835947  [0.8359465034037774]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1        1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "\n",
      "       Loss                                         Loss Curve  \n",
      "0  0.835947                               [0.8359465034037774]  \n",
      "1  0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1        1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2        1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "\n",
      "       Loss                                         Loss Curve  \n",
      "0  0.835947                               [0.8359465034037774]  \n",
      "1  0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1        1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2        1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3        1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "\n",
      "       Loss                                         Loss Curve  \n",
      "0  0.835947                               [0.8359465034037774]  \n",
      "1  0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1        1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2        1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3        1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4        1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "\n",
      "       Loss                                         Loss Curve  \n",
      "0  0.835947                               [0.8359465034037774]  \n",
      "1  0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1        1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2        1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3        1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4        1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5        1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "\n",
      "       Loss                                         Loss Curve  \n",
      "0  0.835947                               [0.8359465034037774]  \n",
      "1  0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1        1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2        1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3        1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4        1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5        1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6        1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "\n",
      "       Loss                                         Loss Curve  \n",
      "0  0.835947                               [0.8359465034037774]  \n",
      "1  0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1        1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2        1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3        1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4        1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5        1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6        1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7        1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "\n",
      "       Loss                                         Loss Curve  \n",
      "0  0.835947                               [0.8359465034037774]  \n",
      "1  0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1        1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2        1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3        1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4        1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5        1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6        1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7        1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8        1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "\n",
      "       Loss                                         Loss Curve  \n",
      "0  0.835947                               [0.8359465034037774]  \n",
      "1  0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1        1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2        1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3        1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4        1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5        1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6        1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7        1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8        1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9        1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "\n",
      "       Loss                                         Loss Curve  \n",
      "0  0.835947                               [0.8359465034037774]  \n",
      "1  0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "48        2            0.3   201  0.682644   0.683665  0.712556  0.682363   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "48  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "48        2            0.3   201  0.682644   0.683665  0.712556  0.682363   \n",
      "49        2            0.3   251  0.682014   0.683387  0.711286  0.681759   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "48  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "49  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "48        2            0.3   201  0.682644   0.683665  0.712556  0.682363   \n",
      "49        2            0.3   251  0.682014   0.683387  0.711286  0.681759   \n",
      "50        2            0.3   301  0.681409   0.683120  0.710065  0.681178   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "48  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "49  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "50  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "48        2            0.3   201  0.682644   0.683665  0.712556  0.682363   \n",
      "49        2            0.3   251  0.682014   0.683387  0.711286  0.681759   \n",
      "50        2            0.3   301  0.681409   0.683120  0.710065  0.681178   \n",
      "51        2            0.3   351  0.680827   0.682864  0.708892  0.680620   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "48  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "49  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "50  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "51  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "48        2            0.3   201  0.682644   0.683665  0.712556  0.682363   \n",
      "49        2            0.3   251  0.682014   0.683387  0.711286  0.681759   \n",
      "50        2            0.3   301  0.681409   0.683120  0.710065  0.681178   \n",
      "51        2            0.3   351  0.680827   0.682864  0.708892  0.680620   \n",
      "52        2            0.3   401  0.680267   0.682617  0.707763  0.680083   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "48  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "49  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "50  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "51  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "52  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "48        2            0.3   201  0.682644   0.683665  0.712556  0.682363   \n",
      "49        2            0.3   251  0.682014   0.683387  0.711286  0.681759   \n",
      "50        2            0.3   301  0.681409   0.683120  0.710065  0.681178   \n",
      "51        2            0.3   351  0.680827   0.682864  0.708892  0.680620   \n",
      "52        2            0.3   401  0.680267   0.682617  0.707763  0.680083   \n",
      "53        2            0.3   451  0.679728   0.682379  0.706675  0.679566   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "48  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "49  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "50  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "51  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "52  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "53  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "48        2            0.3   201  0.682644   0.683665  0.712556  0.682363   \n",
      "49        2            0.3   251  0.682014   0.683387  0.711286  0.681759   \n",
      "50        2            0.3   301  0.681409   0.683120  0.710065  0.681178   \n",
      "51        2            0.3   351  0.680827   0.682864  0.708892  0.680620   \n",
      "52        2            0.3   401  0.680267   0.682617  0.707763  0.680083   \n",
      "53        2            0.3   451  0.679728   0.682379  0.706675  0.679566   \n",
      "54        2            0.3   501  0.679209   0.682150  0.705628  0.679067   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "48  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "49  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "50  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "51  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "52  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "53  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "54  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "48        2            0.3   201  0.682644   0.683665  0.712556  0.682363   \n",
      "49        2            0.3   251  0.682014   0.683387  0.711286  0.681759   \n",
      "50        2            0.3   301  0.681409   0.683120  0.710065  0.681178   \n",
      "51        2            0.3   351  0.680827   0.682864  0.708892  0.680620   \n",
      "52        2            0.3   401  0.680267   0.682617  0.707763  0.680083   \n",
      "53        2            0.3   451  0.679728   0.682379  0.706675  0.679566   \n",
      "54        2            0.3   501  0.679209   0.682150  0.705628  0.679067   \n",
      "55        2            0.5     1  0.678440   0.681879  0.703376  0.678340   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "48  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "49  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "50  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "51  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "52  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "53  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "54  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "55  1.588967                               [1.5889671422189695]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "48        2            0.3   201  0.682644   0.683665  0.712556  0.682363   \n",
      "49        2            0.3   251  0.682014   0.683387  0.711286  0.681759   \n",
      "50        2            0.3   301  0.681409   0.683120  0.710065  0.681178   \n",
      "51        2            0.3   351  0.680827   0.682864  0.708892  0.680620   \n",
      "52        2            0.3   401  0.680267   0.682617  0.707763  0.680083   \n",
      "53        2            0.3   451  0.679728   0.682379  0.706675  0.679566   \n",
      "54        2            0.3   501  0.679209   0.682150  0.705628  0.679067   \n",
      "55        2            0.5     1  0.678440   0.681879  0.703376  0.678340   \n",
      "56        2            0.5    51  0.677928   0.681403  0.702498  0.677821   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "48  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "49  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "50  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "51  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "52  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "53  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "54  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "55  1.588967                               [1.5889671422189695]  \n",
      "56  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "48        2            0.3   201  0.682644   0.683665  0.712556  0.682363   \n",
      "49        2            0.3   251  0.682014   0.683387  0.711286  0.681759   \n",
      "50        2            0.3   301  0.681409   0.683120  0.710065  0.681178   \n",
      "51        2            0.3   351  0.680827   0.682864  0.708892  0.680620   \n",
      "52        2            0.3   401  0.680267   0.682617  0.707763  0.680083   \n",
      "53        2            0.3   451  0.679728   0.682379  0.706675  0.679566   \n",
      "54        2            0.3   501  0.679209   0.682150  0.705628  0.679067   \n",
      "55        2            0.5     1  0.678440   0.681879  0.703376  0.678340   \n",
      "56        2            0.5    51  0.677928   0.681403  0.702498  0.677821   \n",
      "57        2            0.5   101  0.677772   0.681343  0.702094  0.677671   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "48  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "49  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "50  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "51  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "52  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "53  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "54  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "55  1.588967                               [1.5889671422189695]  \n",
      "56  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "57  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "48        2            0.3   201  0.682644   0.683665  0.712556  0.682363   \n",
      "49        2            0.3   251  0.682014   0.683387  0.711286  0.681759   \n",
      "50        2            0.3   301  0.681409   0.683120  0.710065  0.681178   \n",
      "51        2            0.3   351  0.680827   0.682864  0.708892  0.680620   \n",
      "52        2            0.3   401  0.680267   0.682617  0.707763  0.680083   \n",
      "53        2            0.3   451  0.679728   0.682379  0.706675  0.679566   \n",
      "54        2            0.3   501  0.679209   0.682150  0.705628  0.679067   \n",
      "55        2            0.5     1  0.678440   0.681879  0.703376  0.678340   \n",
      "56        2            0.5    51  0.677928   0.681403  0.702498  0.677821   \n",
      "57        2            0.5   101  0.677772   0.681343  0.702094  0.677671   \n",
      "58        2            0.5   151  0.677621   0.681285  0.701703  0.677526   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "48  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "49  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "50  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "51  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "52  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "53  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "54  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "55  1.588967                               [1.5889671422189695]  \n",
      "56  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "57  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "58  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "5         1            0.1   251  0.699145   0.705328  0.721071  0.698825   \n",
      "6         1            0.1   301  0.700293   0.703362  0.728401  0.699840   \n",
      "7         1            0.1   351  0.701154   0.701887  0.733899  0.700601   \n",
      "8         1            0.1   401  0.701823   0.700741  0.738175  0.701193   \n",
      "9         1            0.1   451  0.702359   0.699823  0.741595  0.701666   \n",
      "10        1            0.1   501  0.702797   0.699073  0.744394  0.702053   \n",
      "11        1            0.3     1  0.685075   0.681260  0.708433  0.684765   \n",
      "12        1            0.3    51  0.687165   0.682915  0.712143  0.686810   \n",
      "13        1            0.3   101  0.687143   0.682870  0.712874  0.686785   \n",
      "14        1            0.3   151  0.686957   0.682760  0.713190  0.686604   \n",
      "15        1            0.3   201  0.686795   0.682663  0.713467  0.686446   \n",
      "16        1            0.3   251  0.686652   0.682578  0.713711  0.686306   \n",
      "17        1            0.3   301  0.686524   0.682502  0.713929  0.686182   \n",
      "18        1            0.3   351  0.686410   0.682435  0.714123  0.686071   \n",
      "19        1            0.3   401  0.686308   0.682374  0.714298  0.685971   \n",
      "20        1            0.3   451  0.686215   0.682318  0.714456  0.685881   \n",
      "21        1            0.3   501  0.686131   0.682268  0.714600  0.685799   \n",
      "22        1            0.5     1  0.677497   0.673721  0.716108  0.676990   \n",
      "23        1            0.5    51  0.679471   0.675860  0.717619  0.678974   \n",
      "24        1            0.5   101  0.681082   0.677619  0.718790  0.680585   \n",
      "25        1            0.5   151  0.682569   0.679243  0.719872  0.682072   \n",
      "26        1            0.5   201  0.683946   0.680746  0.720873  0.683448   \n",
      "27        1            0.5   251  0.685224   0.682142  0.721803  0.684727   \n",
      "28        1            0.5   301  0.686415   0.683442  0.722668  0.685917   \n",
      "29        1            0.5   351  0.687526   0.684656  0.723476  0.687028   \n",
      "30        1            0.5   401  0.688565   0.685790  0.724232  0.688067   \n",
      "31        1            0.5   451  0.689539   0.686855  0.724940  0.689041   \n",
      "32        1            0.5   501  0.690455   0.687854  0.725606  0.689956   \n",
      "33        2            0.1     1  0.680100   0.678251  0.716050  0.679569   \n",
      "34        2            0.1    51  0.680145   0.678448  0.715639  0.679602   \n",
      "35        2            0.1   101  0.680532   0.679093  0.715556  0.680004   \n",
      "36        2            0.1   151  0.681862   0.680471  0.716828  0.681351   \n",
      "37        2            0.1   201  0.682794   0.681471  0.717769  0.682299   \n",
      "38        2            0.1   251  0.683677   0.682420  0.718663  0.683198   \n",
      "39        2            0.1   301  0.684516   0.683322  0.719512  0.684052   \n",
      "40        2            0.1   351  0.685314   0.684180  0.720319  0.684864   \n",
      "41        2            0.1   401  0.686074   0.684997  0.721088  0.685638   \n",
      "42        2            0.1   451  0.686799   0.685775  0.721822  0.686376   \n",
      "43        2            0.1   501  0.687491   0.686519  0.722522  0.687080   \n",
      "44        2            0.3     1  0.685272   0.685005  0.717672  0.684915   \n",
      "45        2            0.3    51  0.684698   0.684570  0.716698  0.684333   \n",
      "46        2            0.3   101  0.683984   0.684256  0.715258  0.683648   \n",
      "47        2            0.3   151  0.683300   0.683954  0.713879  0.682992   \n",
      "48        2            0.3   201  0.682644   0.683665  0.712556  0.682363   \n",
      "49        2            0.3   251  0.682014   0.683387  0.711286  0.681759   \n",
      "50        2            0.3   301  0.681409   0.683120  0.710065  0.681178   \n",
      "51        2            0.3   351  0.680827   0.682864  0.708892  0.680620   \n",
      "52        2            0.3   401  0.680267   0.682617  0.707763  0.680083   \n",
      "53        2            0.3   451  0.679728   0.682379  0.706675  0.679566   \n",
      "54        2            0.3   501  0.679209   0.682150  0.705628  0.679067   \n",
      "55        2            0.5     1  0.678440   0.681879  0.703376  0.678340   \n",
      "56        2            0.5    51  0.677928   0.681403  0.702498  0.677821   \n",
      "57        2            0.5   101  0.677772   0.681343  0.702094  0.677671   \n",
      "58        2            0.5   151  0.677621   0.681285  0.701703  0.677526   \n",
      "59        2            0.5   201  0.677474   0.681229  0.701325  0.677386   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "5   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "6   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "7   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "8   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "9   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "10  0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "11  1.720294                               [1.7202937604757675]  \n",
      "12  0.020175  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "13  0.003148  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "14  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "15  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "16  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "17  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "18  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "19  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "20  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "21  0.003088  [1.7202937604757675, 5.190630557685546, 2.3786...  \n",
      "22  2.795739                               [2.7957390725917772]  \n",
      "23  0.017621  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "24  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "25  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "26  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "27  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "28  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "29  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "30  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "31  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "32  0.011557  [2.7957390725917772, 7.3057659769177885, 4.657...  \n",
      "33  1.482749                               [1.4827491526189345]  \n",
      "34  0.261419  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "35  0.080052  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "36  0.019464  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "37  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "38  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "39  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "40  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "41  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "42  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "43  0.016412  [1.4827491526189345, 3.4777309827838025, 1.340...  \n",
      "44  1.307082                                [1.307082084602422]  \n",
      "45  0.429701  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "46  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "47  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "48  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "49  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "50  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "51  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "52  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "53  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "54  0.470220  [1.307082084602422, 8.725891706862033, 3.00028...  \n",
      "55  1.588967                               [1.5889671422189695]  \n",
      "56  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "57  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "58  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "59  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "56        2            0.5    51  0.677928   0.681403  0.702498  0.677821   \n",
      "57        2            0.5   101  0.677772   0.681343  0.702094  0.677671   \n",
      "58        2            0.5   151  0.677621   0.681285  0.701703  0.677526   \n",
      "59        2            0.5   201  0.677474   0.681229  0.701325  0.677386   \n",
      "60        2            0.5   251  0.677333   0.681175  0.700960  0.677251   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "56  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "57  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "58  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "59  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "60  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "\n",
      "[61 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "57        2            0.5   101  0.677772   0.681343  0.702094  0.677671   \n",
      "58        2            0.5   151  0.677621   0.681285  0.701703  0.677526   \n",
      "59        2            0.5   201  0.677474   0.681229  0.701325  0.677386   \n",
      "60        2            0.5   251  0.677333   0.681175  0.700960  0.677251   \n",
      "61        2            0.5   301  0.677196   0.681122  0.700607  0.677120   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "57  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "58  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "59  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "60  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "61  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "\n",
      "[62 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "58        2            0.5   151  0.677621   0.681285  0.701703  0.677526   \n",
      "59        2            0.5   201  0.677474   0.681229  0.701325  0.677386   \n",
      "60        2            0.5   251  0.677333   0.681175  0.700960  0.677251   \n",
      "61        2            0.5   301  0.677196   0.681122  0.700607  0.677120   \n",
      "62        2            0.5   351  0.677063   0.681071  0.700265  0.676993   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "58  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "59  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "60  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "61  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "62  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "\n",
      "[63 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "59        2            0.5   201  0.677474   0.681229  0.701325  0.677386   \n",
      "60        2            0.5   251  0.677333   0.681175  0.700960  0.677251   \n",
      "61        2            0.5   301  0.677196   0.681122  0.700607  0.677120   \n",
      "62        2            0.5   351  0.677063   0.681071  0.700265  0.676993   \n",
      "63        2            0.5   401  0.676935   0.681022  0.699933  0.676870   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "59  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "60  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "61  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "62  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "63  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "\n",
      "[64 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "60        2            0.5   251  0.677333   0.681175  0.700960  0.677251   \n",
      "61        2            0.5   301  0.677196   0.681122  0.700607  0.677120   \n",
      "62        2            0.5   351  0.677063   0.681071  0.700265  0.676993   \n",
      "63        2            0.5   401  0.676935   0.681022  0.699933  0.676870   \n",
      "64        2            0.5   451  0.676811   0.680974  0.699612  0.676751   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "60  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "61  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "62  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "63  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "64  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "\n",
      "[65 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "61        2            0.5   301  0.677196   0.681122  0.700607  0.677120   \n",
      "62        2            0.5   351  0.677063   0.681071  0.700265  0.676993   \n",
      "63        2            0.5   401  0.676935   0.681022  0.699933  0.676870   \n",
      "64        2            0.5   451  0.676811   0.680974  0.699612  0.676751   \n",
      "65        2            0.5   501  0.676690   0.680928  0.699300  0.676636   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "61  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "62  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "63  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "64  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "65  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "\n",
      "[66 rows x 9 columns]\n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "62        2            0.5   351  0.677063   0.681071  0.700265  0.676993   \n",
      "63        2            0.5   401  0.676935   0.681022  0.699933  0.676870   \n",
      "64        2            0.5   451  0.676811   0.680974  0.699612  0.676751   \n",
      "65        2            0.5   501  0.676690   0.680928  0.699300  0.676636   \n",
      "66        3            0.1     1  0.673902   0.670765  0.688863  0.673999   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "62  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "63  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "64  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "65  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "66  0.836880                               [0.8368798943629764]  \n",
      "\n",
      "[67 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "63        2            0.5   401  0.676935   0.681022  0.699933  0.676870   \n",
      "64        2            0.5   451  0.676811   0.680974  0.699612  0.676751   \n",
      "65        2            0.5   501  0.676690   0.680928  0.699300  0.676636   \n",
      "66        3            0.1     1  0.673902   0.670765  0.688863  0.673999   \n",
      "67        3            0.1    51  0.674053   0.670959  0.689646  0.674127   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "63  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "64  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "65  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "66  0.836880                               [0.8368798943629764]  \n",
      "67  0.514687  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "\n",
      "[68 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "64        2            0.5   451  0.676811   0.680974  0.699612  0.676751   \n",
      "65        2            0.5   501  0.676690   0.680928  0.699300  0.676636   \n",
      "66        3            0.1     1  0.673902   0.670765  0.688863  0.673999   \n",
      "67        3            0.1    51  0.674053   0.670959  0.689646  0.674127   \n",
      "68        3            0.1   101  0.674639   0.671343  0.690914  0.674693   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "64  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "65  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "66  0.836880                               [0.8368798943629764]  \n",
      "67  0.514687  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "68  0.147558  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "\n",
      "[69 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "65        2            0.5   501  0.676690   0.680928  0.699300  0.676636   \n",
      "66        3            0.1     1  0.673902   0.670765  0.688863  0.673999   \n",
      "67        3            0.1    51  0.674053   0.670959  0.689646  0.674127   \n",
      "68        3            0.1   101  0.674639   0.671343  0.690914  0.674693   \n",
      "69        3            0.1   151  0.675208   0.672044  0.691357  0.675246   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "65  0.699744  [1.5889671422189695, 6.012413040105636, 3.4686...  \n",
      "66  0.836880                               [0.8368798943629764]  \n",
      "67  0.514687  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "68  0.147558  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "69  0.018943  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "\n",
      "[70 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "66        3            0.1     1  0.673902   0.670765  0.688863  0.673999   \n",
      "67        3            0.1    51  0.674053   0.670959  0.689646  0.674127   \n",
      "68        3            0.1   101  0.674639   0.671343  0.690914  0.674693   \n",
      "69        3            0.1   151  0.675208   0.672044  0.691357  0.675246   \n",
      "70        3            0.1   201  0.675796   0.672670  0.692002  0.675825   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "66  0.836880                               [0.8368798943629764]  \n",
      "67  0.514687  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "68  0.147558  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "69  0.018943  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "70  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "\n",
      "[71 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "67        3            0.1    51  0.674053   0.670959  0.689646  0.674127   \n",
      "68        3            0.1   101  0.674639   0.671343  0.690914  0.674693   \n",
      "69        3            0.1   151  0.675208   0.672044  0.691357  0.675246   \n",
      "70        3            0.1   201  0.675796   0.672670  0.692002  0.675825   \n",
      "71        3            0.1   251  0.676297   0.673266  0.692421  0.676320   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "67  0.514687  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "68  0.147558  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "69  0.018943  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "70  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "71  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "\n",
      "[72 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "68        3            0.1   101  0.674639   0.671343  0.690914  0.674693   \n",
      "69        3            0.1   151  0.675208   0.672044  0.691357  0.675246   \n",
      "70        3            0.1   201  0.675796   0.672670  0.692002  0.675825   \n",
      "71        3            0.1   251  0.676297   0.673266  0.692421  0.676320   \n",
      "72        3            0.1   301  0.676890   0.673889  0.693033  0.676904   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "68  0.147558  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "69  0.018943  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "70  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "71  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "72  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "\n",
      "[73 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "69        3            0.1   151  0.675208   0.672044  0.691357  0.675246   \n",
      "70        3            0.1   201  0.675796   0.672670  0.692002  0.675825   \n",
      "71        3            0.1   251  0.676297   0.673266  0.692421  0.676320   \n",
      "72        3            0.1   301  0.676890   0.673889  0.693033  0.676904   \n",
      "73        3            0.1   351  0.677466   0.674496  0.693629  0.677473   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "69  0.018943  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "70  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "71  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "72  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "73  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "\n",
      "[74 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "70        3            0.1   201  0.675796   0.672670  0.692002  0.675825   \n",
      "71        3            0.1   251  0.676297   0.673266  0.692421  0.676320   \n",
      "72        3            0.1   301  0.676890   0.673889  0.693033  0.676904   \n",
      "73        3            0.1   351  0.677466   0.674496  0.693629  0.677473   \n",
      "74        3            0.1   401  0.678027   0.675086  0.694210  0.678026   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "70  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "71  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "72  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "73  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "74  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "\n",
      "[75 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "71        3            0.1   251  0.676297   0.673266  0.692421  0.676320   \n",
      "72        3            0.1   301  0.676890   0.673889  0.693033  0.676904   \n",
      "73        3            0.1   351  0.677466   0.674496  0.693629  0.677473   \n",
      "74        3            0.1   401  0.678027   0.675086  0.694210  0.678026   \n",
      "75        3            0.1   451  0.678574   0.675661  0.694774  0.678565   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "71  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "72  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "73  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "74  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "75  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "\n",
      "[76 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "72        3            0.1   301  0.676890   0.673889  0.693033  0.676904   \n",
      "73        3            0.1   351  0.677466   0.674496  0.693629  0.677473   \n",
      "74        3            0.1   401  0.678027   0.675086  0.694210  0.678026   \n",
      "75        3            0.1   451  0.678574   0.675661  0.694774  0.678565   \n",
      "76        3            0.1   501  0.679106   0.676221  0.695325  0.679089   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "72  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "73  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "74  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "75  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "76  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "\n",
      "[77 rows x 9 columns]\n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "73        3            0.1   351  0.677466   0.674496  0.693629  0.677473   \n",
      "74        3            0.1   401  0.678027   0.675086  0.694210  0.678026   \n",
      "75        3            0.1   451  0.678574   0.675661  0.694774  0.678565   \n",
      "76        3            0.1   501  0.679106   0.676221  0.695325  0.679089   \n",
      "77        3            0.3     1  0.676680   0.667551  0.686410  0.676793   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "73  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "74  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "75  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "76  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "77  1.176109                                [1.176109403788889]  \n",
      "\n",
      "[78 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "74        3            0.1   401  0.678027   0.675086  0.694210  0.678026   \n",
      "75        3            0.1   451  0.678574   0.675661  0.694774  0.678565   \n",
      "76        3            0.1   501  0.679106   0.676221  0.695325  0.679089   \n",
      "77        3            0.3     1  0.676680   0.667551  0.686410  0.676793   \n",
      "78        3            0.3    51  0.674572   0.665558  0.690380  0.674555   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "74  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "75  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "76  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "77  1.176109                                [1.176109403788889]  \n",
      "78  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "\n",
      "[79 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "75        3            0.1   451  0.678574   0.675661  0.694774  0.678565   \n",
      "76        3            0.1   501  0.679106   0.676221  0.695325  0.679089   \n",
      "77        3            0.3     1  0.676680   0.667551  0.686410  0.676793   \n",
      "78        3            0.3    51  0.674572   0.665558  0.690380  0.674555   \n",
      "79        3            0.3   101  0.672516   0.663616  0.694250  0.672373   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "75  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "76  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "77  1.176109                                [1.176109403788889]  \n",
      "78  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "79  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "\n",
      "[80 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "76        3            0.1   501  0.679106   0.676221  0.695325  0.679089   \n",
      "77        3            0.3     1  0.676680   0.667551  0.686410  0.676793   \n",
      "78        3            0.3    51  0.674572   0.665558  0.690380  0.674555   \n",
      "79        3            0.3   101  0.672516   0.663616  0.694250  0.672373   \n",
      "80        3            0.3   151  0.670511   0.661721  0.698025  0.670245   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "76  0.014019  [0.8368798943629764, 0.7138191703033366, 0.742...  \n",
      "77  1.176109                                [1.176109403788889]  \n",
      "78  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "79  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "80  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "\n",
      "[81 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "77        3            0.3     1  0.676680   0.667551  0.686410  0.676793   \n",
      "78        3            0.3    51  0.674572   0.665558  0.690380  0.674555   \n",
      "79        3            0.3   101  0.672516   0.663616  0.694250  0.672373   \n",
      "80        3            0.3   151  0.670511   0.661721  0.698025  0.670245   \n",
      "81        3            0.3   201  0.668555   0.659872  0.701707  0.668169   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "77  1.176109                                [1.176109403788889]  \n",
      "78  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "79  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "80  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "81  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "\n",
      "[82 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "78        3            0.3    51  0.674572   0.665558  0.690380  0.674555   \n",
      "79        3            0.3   101  0.672516   0.663616  0.694250  0.672373   \n",
      "80        3            0.3   151  0.670511   0.661721  0.698025  0.670245   \n",
      "81        3            0.3   201  0.668555   0.659872  0.701707  0.668169   \n",
      "82        3            0.3   251  0.666647   0.658068  0.705301  0.666143   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "78  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "79  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "80  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "81  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "82  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "\n",
      "[83 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "79        3            0.3   101  0.672516   0.663616  0.694250  0.672373   \n",
      "80        3            0.3   151  0.670511   0.661721  0.698025  0.670245   \n",
      "81        3            0.3   201  0.668555   0.659872  0.701707  0.668169   \n",
      "82        3            0.3   251  0.666647   0.658068  0.705301  0.666143   \n",
      "83        3            0.3   301  0.664783   0.656307  0.708810  0.664165   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "79  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "80  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "81  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "82  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "83  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "\n",
      "[84 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "80        3            0.3   151  0.670511   0.661721  0.698025  0.670245   \n",
      "81        3            0.3   201  0.668555   0.659872  0.701707  0.668169   \n",
      "82        3            0.3   251  0.666647   0.658068  0.705301  0.666143   \n",
      "83        3            0.3   301  0.664783   0.656307  0.708810  0.664165   \n",
      "84        3            0.3   351  0.662964   0.654587  0.712235  0.662234   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "80  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "81  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "82  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "83  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "84  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "\n",
      "[85 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "81        3            0.3   201  0.668555   0.659872  0.701707  0.668169   \n",
      "82        3            0.3   251  0.666647   0.658068  0.705301  0.666143   \n",
      "83        3            0.3   301  0.664783   0.656307  0.708810  0.664165   \n",
      "84        3            0.3   351  0.662964   0.654587  0.712235  0.662234   \n",
      "85        3            0.3   401  0.661187   0.652907  0.715581  0.660347   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "81  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "82  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "83  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "84  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "85  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "\n",
      "[86 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "82        3            0.3   251  0.666647   0.658068  0.705301  0.666143   \n",
      "83        3            0.3   301  0.664783   0.656307  0.708810  0.664165   \n",
      "84        3            0.3   351  0.662964   0.654587  0.712235  0.662234   \n",
      "85        3            0.3   401  0.661187   0.652907  0.715581  0.660347   \n",
      "86        3            0.3   451  0.659450   0.651266  0.718851  0.658504   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "82  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "83  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "84  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "85  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "86  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "\n",
      "[87 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "83        3            0.3   301  0.664783   0.656307  0.708810  0.664165   \n",
      "84        3            0.3   351  0.662964   0.654587  0.712235  0.662234   \n",
      "85        3            0.3   401  0.661187   0.652907  0.715581  0.660347   \n",
      "86        3            0.3   451  0.659450   0.651266  0.718851  0.658504   \n",
      "87        3            0.3   501  0.657753   0.649662  0.722045  0.656703   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "83  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "84  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "85  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "86  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "87  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "\n",
      "[88 rows x 9 columns]\n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "84        3            0.3   351  0.662964   0.654587  0.712235  0.662234   \n",
      "85        3            0.3   401  0.661187   0.652907  0.715581  0.660347   \n",
      "86        3            0.3   451  0.659450   0.651266  0.718851  0.658504   \n",
      "87        3            0.3   501  0.657753   0.649662  0.722045  0.656703   \n",
      "88        3            0.5     1  0.655867   0.642363  0.713933  0.654942   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "84  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "85  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "86  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "87  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "88  1.488689                               [1.4886891879607331]  \n",
      "\n",
      "[89 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "85        3            0.3   401  0.661187   0.652907  0.715581  0.660347   \n",
      "86        3            0.3   451  0.659450   0.651266  0.718851  0.658504   \n",
      "87        3            0.3   501  0.657753   0.649662  0.722045  0.656703   \n",
      "88        3            0.5     1  0.655867   0.642363  0.713933  0.654942   \n",
      "89        3            0.5    51  0.654162   0.639184  0.713778  0.653221   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "85  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "86  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "87  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "88  1.488689                               [1.4886891879607331]  \n",
      "89  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "\n",
      "[90 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "86        3            0.3   451  0.659450   0.651266  0.718851  0.658504   \n",
      "87        3            0.3   501  0.657753   0.649662  0.722045  0.656703   \n",
      "88        3            0.5     1  0.655867   0.642363  0.713933  0.654942   \n",
      "89        3            0.5    51  0.654162   0.639184  0.713778  0.653221   \n",
      "90        3            0.5   101  0.652495   0.636075  0.713626  0.651537   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "86  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "87  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "88  1.488689                               [1.4886891879607331]  \n",
      "89  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "90  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "\n",
      "[91 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "87        3            0.3   501  0.657753   0.649662  0.722045  0.656703   \n",
      "88        3            0.5     1  0.655867   0.642363  0.713933  0.654942   \n",
      "89        3            0.5    51  0.654162   0.639184  0.713778  0.653221   \n",
      "90        3            0.5   101  0.652495   0.636075  0.713626  0.651537   \n",
      "91        3            0.5   151  0.650864   0.633034  0.713478  0.649890   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "87  0.695465  [1.176109403788889, 0.8931357613019167, 0.7787...  \n",
      "88  1.488689                               [1.4886891879607331]  \n",
      "89  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "90  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "91  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "\n",
      "[92 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "88        3            0.5     1  0.655867   0.642363  0.713933  0.654942   \n",
      "89        3            0.5    51  0.654162   0.639184  0.713778  0.653221   \n",
      "90        3            0.5   101  0.652495   0.636075  0.713626  0.651537   \n",
      "91        3            0.5   151  0.650864   0.633034  0.713478  0.649890   \n",
      "92        3            0.5   201  0.649268   0.630058  0.713333  0.648278   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "88  1.488689                               [1.4886891879607331]  \n",
      "89  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "90  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "91  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "92  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "\n",
      "[93 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "89        3            0.5    51  0.654162   0.639184  0.713778  0.653221   \n",
      "90        3            0.5   101  0.652495   0.636075  0.713626  0.651537   \n",
      "91        3            0.5   151  0.650864   0.633034  0.713478  0.649890   \n",
      "92        3            0.5   201  0.649268   0.630058  0.713333  0.648278   \n",
      "93        3            0.5   251  0.647706   0.627146  0.713191  0.646701   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "89  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "90  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "91  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "92  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "93  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "\n",
      "[94 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "90        3            0.5   101  0.652495   0.636075  0.713626  0.651537   \n",
      "91        3            0.5   151  0.650864   0.633034  0.713478  0.649890   \n",
      "92        3            0.5   201  0.649268   0.630058  0.713333  0.648278   \n",
      "93        3            0.5   251  0.647706   0.627146  0.713191  0.646701   \n",
      "94        3            0.5   301  0.646177   0.624294  0.713053  0.645157   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "90  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "91  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "92  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "93  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "94  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "\n",
      "[95 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "91        3            0.5   151  0.650864   0.633034  0.713478  0.649890   \n",
      "92        3            0.5   201  0.649268   0.630058  0.713333  0.648278   \n",
      "93        3            0.5   251  0.647706   0.627146  0.713191  0.646701   \n",
      "94        3            0.5   301  0.646177   0.624294  0.713053  0.645157   \n",
      "95        3            0.5   351  0.644679   0.621503  0.712917  0.643644   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "91  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "92  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "93  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "94  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "95  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "\n",
      "[96 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "92        3            0.5   201  0.649268   0.630058  0.713333  0.648278   \n",
      "93        3            0.5   251  0.647706   0.627146  0.713191  0.646701   \n",
      "94        3            0.5   301  0.646177   0.624294  0.713053  0.645157   \n",
      "95        3            0.5   351  0.644679   0.621503  0.712917  0.643644   \n",
      "96        3            0.5   401  0.643213   0.618768  0.712784  0.642164   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "92  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "93  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "94  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "95  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "96  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "\n",
      "[97 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "93        3            0.5   251  0.647706   0.627146  0.713191  0.646701   \n",
      "94        3            0.5   301  0.646177   0.624294  0.713053  0.645157   \n",
      "95        3            0.5   351  0.644679   0.621503  0.712917  0.643644   \n",
      "96        3            0.5   401  0.643213   0.618768  0.712784  0.642164   \n",
      "97        3            0.5   451  0.641777   0.616090  0.712653  0.640713   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "93  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "94  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "95  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "96  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "97  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "\n",
      "[98 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "94        3            0.5   301  0.646177   0.624294  0.713053  0.645157   \n",
      "95        3            0.5   351  0.644679   0.621503  0.712917  0.643644   \n",
      "96        3            0.5   401  0.643213   0.618768  0.712784  0.642164   \n",
      "97        3            0.5   451  0.641777   0.616090  0.712653  0.640713   \n",
      "98        3            0.5   501  0.640369   0.613466  0.712525  0.639292   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "94  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "95  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "96  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "97  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "98  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "\n",
      "[99 rows x 9 columns]\n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1         1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2         1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3         1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4         1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "95        3            0.5   351  0.644679   0.621503  0.712917  0.643644   \n",
      "96        3            0.5   401  0.643213   0.618768  0.712784  0.642164   \n",
      "97        3            0.5   451  0.641777   0.616090  0.712653  0.640713   \n",
      "98        3            0.5   501  0.640369   0.613466  0.712525  0.639292   \n",
      "99        4            0.1     1  0.638712   0.608710  0.707600  0.637704   \n",
      "\n",
      "        Loss                                         Loss Curve  \n",
      "0   0.835947                               [0.8359465034037774]  \n",
      "1   0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4   0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..       ...                                                ...  \n",
      "95  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "96  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "97  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "98  0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "99  1.046724                                [1.046723855211773]  \n",
      "\n",
      "[100 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "96         3            0.5   401  0.643213   0.618768  0.712784  0.642164   \n",
      "97         3            0.5   451  0.641777   0.616090  0.712653  0.640713   \n",
      "98         3            0.5   501  0.640369   0.613466  0.712525  0.639292   \n",
      "99         4            0.1     1  0.638712   0.608710  0.707600  0.637704   \n",
      "100        4            0.1    51  0.639462   0.609983  0.707501  0.638470   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "96   0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "97   0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "98   0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "99   1.046724                                [1.046723855211773]  \n",
      "100  0.278973  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "\n",
      "[101 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "97         3            0.5   451  0.641777   0.616090  0.712653  0.640713   \n",
      "98         3            0.5   501  0.640369   0.613466  0.712525  0.639292   \n",
      "99         4            0.1     1  0.638712   0.608710  0.707600  0.637704   \n",
      "100        4            0.1    51  0.639462   0.609983  0.707501  0.638470   \n",
      "101        4            0.1   101  0.640000   0.611026  0.707218  0.639025   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "97   0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "98   0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "99   1.046724                                [1.046723855211773]  \n",
      "100  0.278973  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "101  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "\n",
      "[102 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "98         3            0.5   501  0.640369   0.613466  0.712525  0.639292   \n",
      "99         4            0.1     1  0.638712   0.608710  0.707600  0.637704   \n",
      "100        4            0.1    51  0.639462   0.609983  0.707501  0.638470   \n",
      "101        4            0.1   101  0.640000   0.611026  0.707218  0.639025   \n",
      "102        4            0.1   151  0.640528   0.612068  0.706891  0.639571   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "98   0.699446  [1.4886891879607331, 0.9852996157920351, 1.421...  \n",
      "99   1.046724                                [1.046723855211773]  \n",
      "100  0.278973  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "101  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "102  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "\n",
      "[103 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "99         4            0.1     1  0.638712   0.608710  0.707600  0.637704   \n",
      "100        4            0.1    51  0.639462   0.609983  0.707501  0.638470   \n",
      "101        4            0.1   101  0.640000   0.611026  0.707218  0.639025   \n",
      "102        4            0.1   151  0.640528   0.612068  0.706891  0.639571   \n",
      "103        4            0.1   201  0.641045   0.613089  0.706571  0.640106   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "99   1.046724                                [1.046723855211773]  \n",
      "100  0.278973  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "101  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "102  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "103  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "\n",
      "[104 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "100        4            0.1    51  0.639462   0.609983  0.707501  0.638470   \n",
      "101        4            0.1   101  0.640000   0.611026  0.707218  0.639025   \n",
      "102        4            0.1   151  0.640528   0.612068  0.706891  0.639571   \n",
      "103        4            0.1   201  0.641045   0.613089  0.706571  0.640106   \n",
      "104        4            0.1   251  0.641553   0.614091  0.706256  0.640631   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "100  0.278973  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "101  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "102  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "103  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "104  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "\n",
      "[105 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "101        4            0.1   101  0.640000   0.611026  0.707218  0.639025   \n",
      "102        4            0.1   151  0.640528   0.612068  0.706891  0.639571   \n",
      "103        4            0.1   201  0.641045   0.613089  0.706571  0.640106   \n",
      "104        4            0.1   251  0.641553   0.614091  0.706256  0.640631   \n",
      "105        4            0.1   301  0.642051   0.615074  0.705948  0.641145   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "101  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "102  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "103  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "104  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "105  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "\n",
      "[106 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "102        4            0.1   151  0.640528   0.612068  0.706891  0.639571   \n",
      "103        4            0.1   201  0.641045   0.613089  0.706571  0.640106   \n",
      "104        4            0.1   251  0.641553   0.614091  0.706256  0.640631   \n",
      "105        4            0.1   301  0.642051   0.615074  0.705948  0.641145   \n",
      "106        4            0.1   351  0.642540   0.616038  0.705645  0.641651   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "102  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "103  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "104  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "105  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "106  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "\n",
      "[107 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "103        4            0.1   201  0.641045   0.613089  0.706571  0.640106   \n",
      "104        4            0.1   251  0.641553   0.614091  0.706256  0.640631   \n",
      "105        4            0.1   301  0.642051   0.615074  0.705948  0.641145   \n",
      "106        4            0.1   351  0.642540   0.616038  0.705645  0.641651   \n",
      "107        4            0.1   401  0.643020   0.616985  0.705348  0.642147   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "103  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "104  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "105  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "106  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "107  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "\n",
      "[108 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "104        4            0.1   251  0.641553   0.614091  0.706256  0.640631   \n",
      "105        4            0.1   301  0.642051   0.615074  0.705948  0.641145   \n",
      "106        4            0.1   351  0.642540   0.616038  0.705645  0.641651   \n",
      "107        4            0.1   401  0.643020   0.616985  0.705348  0.642147   \n",
      "108        4            0.1   451  0.643491   0.617914  0.705057  0.642633   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "104  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "105  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "106  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "107  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "108  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "\n",
      "[109 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "105        4            0.1   301  0.642051   0.615074  0.705948  0.641145   \n",
      "106        4            0.1   351  0.642540   0.616038  0.705645  0.641651   \n",
      "107        4            0.1   401  0.643020   0.616985  0.705348  0.642147   \n",
      "108        4            0.1   451  0.643491   0.617914  0.705057  0.642633   \n",
      "109        4            0.1   501  0.643953   0.618827  0.704771  0.643111   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "105  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "106  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "107  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "108  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "109  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "\n",
      "[110 rows x 9 columns]\n",
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "106        4            0.1   351  0.642540   0.616038  0.705645  0.641651   \n",
      "107        4            0.1   401  0.643020   0.616985  0.705348  0.642147   \n",
      "108        4            0.1   451  0.643491   0.617914  0.705057  0.642633   \n",
      "109        4            0.1   501  0.643953   0.618827  0.704771  0.643111   \n",
      "110        4            0.3     1  0.642633   0.614637  0.701124  0.641822   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "106  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "107  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "108  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "109  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "110  5.507589                                [5.507588526989625]  \n",
      "\n",
      "[111 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "107        4            0.1   401  0.643020   0.616985  0.705348  0.642147   \n",
      "108        4            0.1   451  0.643491   0.617914  0.705057  0.642633   \n",
      "109        4            0.1   501  0.643953   0.618827  0.704771  0.643111   \n",
      "110        4            0.3     1  0.642633   0.614637  0.701124  0.641822   \n",
      "111        4            0.3    51  0.641520   0.612966  0.699060  0.640751   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "107  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "108  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "109  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "110  5.507589                                [5.507588526989625]  \n",
      "111  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "\n",
      "[112 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "108        4            0.1   451  0.643491   0.617914  0.705057  0.642633   \n",
      "109        4            0.1   501  0.643953   0.618827  0.704771  0.643111   \n",
      "110        4            0.3     1  0.642633   0.614637  0.701124  0.641822   \n",
      "111        4            0.3    51  0.641520   0.612966  0.699060  0.640751   \n",
      "112        4            0.3   101  0.640427   0.611325  0.697033  0.639699   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "108  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "109  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "110  5.507589                                [5.507588526989625]  \n",
      "111  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "112  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "\n",
      "[113 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "109        4            0.1   501  0.643953   0.618827  0.704771  0.643111   \n",
      "110        4            0.3     1  0.642633   0.614637  0.701124  0.641822   \n",
      "111        4            0.3    51  0.641520   0.612966  0.699060  0.640751   \n",
      "112        4            0.3   101  0.640427   0.611325  0.697033  0.639699   \n",
      "113        4            0.3   151  0.639352   0.609713  0.695042  0.638665   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "109  0.273977  [1.046723855211773, 4.172523520414484, 0.69560...  \n",
      "110  5.507589                                [5.507588526989625]  \n",
      "111  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "112  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "113  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "\n",
      "[114 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "110        4            0.3     1  0.642633   0.614637  0.701124  0.641822   \n",
      "111        4            0.3    51  0.641520   0.612966  0.699060  0.640751   \n",
      "112        4            0.3   101  0.640427   0.611325  0.697033  0.639699   \n",
      "113        4            0.3   151  0.639352   0.609713  0.695042  0.638665   \n",
      "114        4            0.3   201  0.638297   0.608128  0.693085  0.637649   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "110  5.507589                                [5.507588526989625]  \n",
      "111  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "112  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "113  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "114  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "\n",
      "[115 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "111        4            0.3    51  0.641520   0.612966  0.699060  0.640751   \n",
      "112        4            0.3   101  0.640427   0.611325  0.697033  0.639699   \n",
      "113        4            0.3   151  0.639352   0.609713  0.695042  0.638665   \n",
      "114        4            0.3   201  0.638297   0.608128  0.693085  0.637649   \n",
      "115        4            0.3   251  0.637259   0.606571  0.691162  0.636651   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "111  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "112  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "113  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "114  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "115  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "\n",
      "[116 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "112        4            0.3   101  0.640427   0.611325  0.697033  0.639699   \n",
      "113        4            0.3   151  0.639352   0.609713  0.695042  0.638665   \n",
      "114        4            0.3   201  0.638297   0.608128  0.693085  0.637649   \n",
      "115        4            0.3   251  0.637259   0.606571  0.691162  0.636651   \n",
      "116        4            0.3   301  0.636239   0.605040  0.689271  0.635669   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "112  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "113  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "114  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "115  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "116  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "\n",
      "[117 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "113        4            0.3   151  0.639352   0.609713  0.695042  0.638665   \n",
      "114        4            0.3   201  0.638297   0.608128  0.693085  0.637649   \n",
      "115        4            0.3   251  0.637259   0.606571  0.691162  0.636651   \n",
      "116        4            0.3   301  0.636239   0.605040  0.689271  0.635669   \n",
      "117        4            0.3   351  0.635237   0.603536  0.687413  0.634705   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "113  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "114  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "115  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "116  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "117  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "\n",
      "[118 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "114        4            0.3   201  0.638297   0.608128  0.693085  0.637649   \n",
      "115        4            0.3   251  0.637259   0.606571  0.691162  0.636651   \n",
      "116        4            0.3   301  0.636239   0.605040  0.689271  0.635669   \n",
      "117        4            0.3   351  0.635237   0.603536  0.687413  0.634705   \n",
      "118        4            0.3   401  0.634251   0.602057  0.685586  0.633756   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "114  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "115  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "116  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "117  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "118  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "\n",
      "[119 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "115        4            0.3   251  0.637259   0.606571  0.691162  0.636651   \n",
      "116        4            0.3   301  0.636239   0.605040  0.689271  0.635669   \n",
      "117        4            0.3   351  0.635237   0.603536  0.687413  0.634705   \n",
      "118        4            0.3   401  0.634251   0.602057  0.685586  0.633756   \n",
      "119        4            0.3   451  0.633282   0.600602  0.683790  0.632824   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "115  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "116  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "117  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "118  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "119  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "\n",
      "[120 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "116        4            0.3   301  0.636239   0.605040  0.689271  0.635669   \n",
      "117        4            0.3   351  0.635237   0.603536  0.687413  0.634705   \n",
      "118        4            0.3   401  0.634251   0.602057  0.685586  0.633756   \n",
      "119        4            0.3   451  0.633282   0.600602  0.683790  0.632824   \n",
      "120        4            0.3   501  0.632329   0.599171  0.682023  0.631906   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "116  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "117  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "118  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "119  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "120  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "\n",
      "[121 rows x 9 columns]\n",
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "117        4            0.3   351  0.635237   0.603536  0.687413  0.634705   \n",
      "118        4            0.3   401  0.634251   0.602057  0.685586  0.633756   \n",
      "119        4            0.3   451  0.633282   0.600602  0.683790  0.632824   \n",
      "120        4            0.3   501  0.632329   0.599171  0.682023  0.631906   \n",
      "121        4            0.5     1  0.631223   0.595930  0.679711  0.630825   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "117  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "118  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "119  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "120  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "121  7.352554                               [7.3525537850888565]  \n",
      "\n",
      "[122 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "118        4            0.3   401  0.634251   0.602057  0.685586  0.633756   \n",
      "119        4            0.3   451  0.633282   0.600602  0.683790  0.632824   \n",
      "120        4            0.3   501  0.632329   0.599171  0.682023  0.631906   \n",
      "121        4            0.5     1  0.631223   0.595930  0.679711  0.630825   \n",
      "122        4            0.5    51  0.630176   0.593586  0.679063  0.629740   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "118  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "119  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "120  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "121  7.352554                               [7.3525537850888565]  \n",
      "122  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "\n",
      "[123 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "119        4            0.3   451  0.633282   0.600602  0.683790  0.632824   \n",
      "120        4            0.3   501  0.632329   0.599171  0.682023  0.631906   \n",
      "121        4            0.5     1  0.631223   0.595930  0.679711  0.630825   \n",
      "122        4            0.5    51  0.630176   0.593586  0.679063  0.629740   \n",
      "123        4            0.5   101  0.629146   0.591279  0.678425  0.628673   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "119  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "120  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "121  7.352554                               [7.3525537850888565]  \n",
      "122  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "123  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "\n",
      "[124 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "120        4            0.3   501  0.632329   0.599171  0.682023  0.631906   \n",
      "121        4            0.5     1  0.631223   0.595930  0.679711  0.630825   \n",
      "122        4            0.5    51  0.630176   0.593586  0.679063  0.629740   \n",
      "123        4            0.5   101  0.629146   0.591279  0.678425  0.628673   \n",
      "124        4            0.5   151  0.628132   0.589010  0.677798  0.627622   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "120  0.693727  [5.507588526989625, 17.92189384962561, 6.23817...  \n",
      "121  7.352554                               [7.3525537850888565]  \n",
      "122  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "123  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "124  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "\n",
      "[125 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "121        4            0.5     1  0.631223   0.595930  0.679711  0.630825   \n",
      "122        4            0.5    51  0.630176   0.593586  0.679063  0.629740   \n",
      "123        4            0.5   101  0.629146   0.591279  0.678425  0.628673   \n",
      "124        4            0.5   151  0.628132   0.589010  0.677798  0.627622   \n",
      "125        4            0.5   201  0.627135   0.586776  0.677181  0.626588   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "121  7.352554                               [7.3525537850888565]  \n",
      "122  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "123  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "124  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "125  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "\n",
      "[126 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "122        4            0.5    51  0.630176   0.593586  0.679063  0.629740   \n",
      "123        4            0.5   101  0.629146   0.591279  0.678425  0.628673   \n",
      "124        4            0.5   151  0.628132   0.589010  0.677798  0.627622   \n",
      "125        4            0.5   201  0.627135   0.586776  0.677181  0.626588   \n",
      "126        4            0.5   251  0.626153   0.584577  0.676573  0.625571   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "122  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "123  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "124  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "125  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "126  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "\n",
      "[127 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "123        4            0.5   101  0.629146   0.591279  0.678425  0.628673   \n",
      "124        4            0.5   151  0.628132   0.589010  0.677798  0.627622   \n",
      "125        4            0.5   201  0.627135   0.586776  0.677181  0.626588   \n",
      "126        4            0.5   251  0.626153   0.584577  0.676573  0.625571   \n",
      "127        4            0.5   301  0.625186   0.582413  0.675975  0.624569   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "123  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "124  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "125  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "126  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "127  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "\n",
      "[128 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "124        4            0.5   151  0.628132   0.589010  0.677798  0.627622   \n",
      "125        4            0.5   201  0.627135   0.586776  0.677181  0.626588   \n",
      "126        4            0.5   251  0.626153   0.584577  0.676573  0.625571   \n",
      "127        4            0.5   301  0.625186   0.582413  0.675975  0.624569   \n",
      "128        4            0.5   351  0.624235   0.580283  0.675386  0.623583   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "124  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "125  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "126  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "127  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "128  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "\n",
      "[129 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "125        4            0.5   201  0.627135   0.586776  0.677181  0.626588   \n",
      "126        4            0.5   251  0.626153   0.584577  0.676573  0.625571   \n",
      "127        4            0.5   301  0.625186   0.582413  0.675975  0.624569   \n",
      "128        4            0.5   351  0.624235   0.580283  0.675386  0.623583   \n",
      "129        4            0.5   401  0.623298   0.578185  0.674806  0.622612   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "125  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "126  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "127  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "128  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "129  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "\n",
      "[130 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "126        4            0.5   251  0.626153   0.584577  0.676573  0.625571   \n",
      "127        4            0.5   301  0.625186   0.582413  0.675975  0.624569   \n",
      "128        4            0.5   351  0.624235   0.580283  0.675386  0.623583   \n",
      "129        4            0.5   401  0.623298   0.578185  0.674806  0.622612   \n",
      "130        4            0.5   451  0.622375   0.576119  0.674235  0.621656   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "126  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "127  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "128  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "129  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "130  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "\n",
      "[131 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.646410   0.758769  0.465238  0.650777   \n",
      "1          1            0.1    51  0.683077   0.732851  0.618452  0.684621   \n",
      "2          1            0.1   101  0.691111   0.719089  0.669762  0.691723   \n",
      "3          1            0.1   151  0.695128   0.712209  0.695417  0.695274   \n",
      "4          1            0.1   201  0.697538   0.708080  0.710810  0.697405   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "127        4            0.5   301  0.625186   0.582413  0.675975  0.624569   \n",
      "128        4            0.5   351  0.624235   0.580283  0.675386  0.623583   \n",
      "129        4            0.5   401  0.623298   0.578185  0.674806  0.622612   \n",
      "130        4            0.5   451  0.622375   0.576119  0.674235  0.621656   \n",
      "131        4            0.5   501  0.621467   0.574084  0.673672  0.620715   \n",
      "\n",
      "         Loss                                         Loss Curve  \n",
      "0    0.835947                               [0.8359465034037774]  \n",
      "1    0.020346  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "2    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "3    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "4    0.002553  [0.8359465034037774, 0.6607344320829996, 0.720...  \n",
      "..        ...                                                ...  \n",
      "127  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "128  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "129  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "130  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "131  0.702471  [7.3525537850888565, 18.376529329935877, 12.76...  \n",
      "\n",
      "[132 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_13728\\893242329.py:145: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, matthews_corrcoef, roc_curve, auc, precision_recall_curve\n",
    "from statistics import mean, stdev\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv('./rtfDataSet.csv')\n",
    "\n",
    "x = df.iloc[:, selected_features]\n",
    "\n",
    "# Input_ y_Target_Variable.\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Normalize\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x_scaled)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accu_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "auc_list = []\n",
    "\n",
    "\n",
    "all_y_test = []\n",
    "all_y_pred = []\n",
    "\n",
    "\n",
    "learning_rates = [0.1, 0.3, 0.5]\n",
    "epoch_ranges = range(1, 502, 50)\n",
    "input_dim = len(selected_features) \n",
    "output_neurons = 1\n",
    "output_activation = 'logistic' \n",
    "\n",
    "data_frame = pd.DataFrame(columns=['Topology','Learning Rate', 'Epoch', 'Accuracy', 'Precision', 'Recall', 'AUC','Loss','Loss Curve'])\n",
    "\n",
    "\n",
    "activation_functions = ['identity', 'logistic', 'tanh', 'relu']\n",
    "topologies = [\n",
    "    {\n",
    "        'name': \"1\",\n",
    "        'input_neurons': input_dim,\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_layers': [{'neurons': 64, 'activation': 'relu'}],\n",
    "        'output_neurons': output_neurons,\n",
    "        'output_activation': output_activation,\n",
    "        'activation_function': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'name': \"2\",\n",
    "        'input_neurons': input_dim,\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_layers': [\n",
    "            {'neurons': 128, 'activation': 'tanh'},\n",
    "            {'neurons': 64, 'activation': 'tanh'}\n",
    "        ],\n",
    "        'output_neurons': output_neurons,\n",
    "        'output_activation': output_activation,\n",
    "        'activation_function': 'tanh'\n",
    "    },\n",
    "    {\n",
    "        'name': \"3\",\n",
    "        'input_neurons': input_dim,\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_layers': [\n",
    "            {'neurons': 32, 'activation': 'logistic'},\n",
    "            {'neurons': 32, 'activation': 'logistic'},\n",
    "            {'neurons': 16, 'activation': 'logistic'}\n",
    "        ],\n",
    "        'output_neurons': output_neurons,\n",
    "        'output_activation': output_activation,\n",
    "        'activation_function': 'logistic'\n",
    "    },\n",
    "    {\n",
    "        'name': \"4\",\n",
    "        'input_neurons': input_dim,\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_layers': [\n",
    "            {'neurons': 100, 'activation': 'relu'},\n",
    "            {'neurons': 50, 'activation': 'relu'},\n",
    "            {'neurons': 25, 'activation': 'relu'},\n",
    "            {'neurons': 10, 'activation': 'relu'}\n",
    "        ],\n",
    "        'output_neurons': output_neurons,\n",
    "        'output_activation': output_activation,\n",
    "        'activation_function': 'relu'\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for topology in topologies:\n",
    "    for learning_rate in learning_rates:\n",
    "        hidden_layer_sizes = tuple(layer['neurons'] for layer in topology['hidden_layers'])\n",
    "        activation_function = topology['activation_function']\n",
    "        for epoch in epoch_ranges:\n",
    "\n",
    "            model = MLPClassifier(learning_rate_init = learning_rate,\n",
    "                                hidden_layer_sizes = hidden_layer_sizes,\n",
    "                                max_iter = epoch,\n",
    "                                random_state = 42,\n",
    "                                activation = activation_function,\n",
    "                                # out_activation_ = output_activation,\n",
    "                                # n_outputs_ = output_neurons\n",
    "                                #n_features_in_ = input_dim\n",
    "                                )\n",
    "            \n",
    "            for train_index, test_index in skf.split(x_scaled, y):\n",
    "                x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "                y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "                # Fit the classifier to the training data\n",
    "                model.fit(x_train_fold, y_train_fold)\n",
    "\n",
    "                # Make predictions on the test data\n",
    "                y_pred = model.predict(x_test_fold)\n",
    "                #all_y_pred.extend(model.predict(x_train_fold))\n",
    "\n",
    "                # Evaluate the classifier's performance \n",
    "                accu_list.append(accuracy_score(y_test_fold, y_pred))\n",
    "                precision_list.append(precision_score(y_test_fold, y_pred))\n",
    "                recall_list.append(recall_score(y_test_fold, y_pred))\n",
    "                auc_list.append(roc_auc_score(y_test_fold, y_pred))\n",
    "\n",
    "            add_data = {\n",
    "                'Topology': topology['name'],\n",
    "                'Learning Rate': learning_rate,\n",
    "                'Epoch': epoch,\n",
    "                'Accuracy': mean(accu_list),\n",
    "                'Precision': mean(precision_list),\n",
    "                'Recall': mean(recall_list),\n",
    "                'AUC': mean(auc_list),\n",
    "                'Loss': model.loss_,\n",
    "                'Loss Curve': model.loss_curve_,\n",
    "            }\n",
    "\n",
    "            data_frame = data_frame.append(add_data, ignore_index=True)\n",
    "            print(data_frame)\n",
    "            \n",
    "            # print('Metrics Results:')\n",
    "            # print('-'*30)\n",
    "            # print(f\"Accuracy: {mean(accu_list)*100:.2f}% ± {stdev(accu_list):.2f}\")\n",
    "            # print(f\"Precision: {mean(precision_list)*100:.2f}% ± {stdev(precision_list):.2f}\")\n",
    "            # print(f\"Recall: {mean(recall_list)*100:.2f}% ± {stdev(recall_list):.2f}\")\n",
    "            # print(f\"AUC: {mean(auc_list)*100:.2f}% ± {stdev(auc_list):.2f}\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Calcular sensibilidad y especificidad\n",
    "# tn, fp, fn, tp = confusion_matrix(y_test_fold, y_pred).ravel()\n",
    "        \n",
    "\n",
    "# # Plot Confusion Matrix \n",
    "# matrix = confusion_matrix(y_test_fold, y_pred)\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.set(font_scale=1.4)  # for label size\n",
    "# sns.heatmap(matrix, annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
    "# plt.xlabel('True')\n",
    "# plt.ylabel('Predicted')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot ROC Curve\n",
    "# fpr, tpr, _ = roc_curve(y_test_fold, y_pred)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# plt.figure()\n",
    "# lw = 2\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title(f'ROC - Criterion')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "\n",
    "# # Plot Precision vs Recall\n",
    "# precision, recall, thresholds = precision_recall_curve(y_test_fold, y_pred)\n",
    "# # precision, recall, thresholds = precision_recall_curve(all_y_test, all_y_pred)\n",
    "# plt.figure()\n",
    "# plt.plot(recall, precision, color='blue', lw=lw)\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title(f'Precision vs Recall - Criterion')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7020534290271133"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['AUC'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topology                                                         1\n",
       "Learning Rate                                                  0.1\n",
       "Epoch                                                          501\n",
       "Accuracy                                                  0.702797\n",
       "Precision                                                 0.699073\n",
       "Recall                                                    0.744394\n",
       "AUC                                                       0.702053\n",
       "Loss                                                      0.002553\n",
       "Loss Curve       [0.8359465034037774, 0.6607344320829996, 0.720...\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.loc[data_frame['AUC'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAIjCAYAAADhisjVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABfdklEQVR4nO3deXxU9b3/8fdkT5AQIEASdlD2VSyIqKCswQUUN8TigtJa8NpLtZb+WgG1xbq3leIKaBVZvIpWEQkoIgIisiMgUnaSQNAkhEAyJOf3x3EmCdlOkpk5s7yej8c8/M6ZM+d85sMJvjn5zjkOwzAMAQAAAEEqzO4CAAAAAG8i8AIAACCoEXgBAAAQ1Ai8AAAACGoEXgAAAAQ1Ai8AAACCGoEXAAAAQY3ACwAAgKBG4AUAAEBQI/ACAPzevHnz5HA4tHHjRrtLARCACLwAQgahqXKu3lT2WL9+vd0lAkCtRdhdAADAfzz22GNq27ZtueUXXnihDdUAgGcQeAEAbqmpqbrkkkvsLgMAPIopDQBwns2bNys1NVXx8fG64IILNHjw4HK/0nc6nZoxY4YuuugixcTEqHHjxrr88suVlpbmXicjI0N33323WrRooejoaCUnJ2vUqFE6cOBApft+5pln5HA4dPDgwXKvTZ06VVFRUfrpp58kSXv37tWYMWOUlJSkmJgYtWjRQrfddptycnI804gKHDhwQA6HQ88884yef/55tW7dWrGxsRo4cKB27NhRbv3PPvtMV1xxherVq6eEhASNGjVKu3btKrfe0aNHNWHCBKWkpCg6Olpt27bV/fffr8LCwjLrFRQUaMqUKWrSpInq1aunG264QSdOnCizzsaNGzV8+HAlJiYqNjZWbdu21T333OPZRgAIKJzhBYBSdu7cqSuuuELx8fH6/e9/r8jISL388ssaNGiQvvjiC/Xr10+SNH36dM2cOVP33nuv+vbtq9zcXG3cuFGbNm3S0KFDJUljxozRzp079cADD6hNmzY6fvy40tLSdOjQIbVp06bC/d9yyy36/e9/r0WLFunhhx8u89qiRYs0bNgwNWzYUIWFhRo+fLgKCgr0wAMPKCkpSUePHtVHH32k7OxsNWjQoFafPycnR1lZWWWWORwONW7cuMyyN998U6dOndKkSZN09uxZ/f3vf9fVV1+t7du3q1mzZpKkFStWKDU1Ve3atdP06dN15swZ/fOf/9SAAQO0adMmdw+OHTumvn37Kjs7WxMnTlSnTp109OhRvfvuu8rPz1dUVJR7vw888IAaNmyoadOm6cCBA3rhhRc0efJkLVy4UJJ0/PhxDRs2TE2aNNEf/vAHJSQk6MCBA3rvvfdq1Q8AQcIAgBAxd+5cQ5LxzTffVLrO6NGjjaioKGPfvn3uZceOHTPq169vXHnlle5lPXv2NK655ppKt/PTTz8Zkoynn366xnX279/f6NOnT5llGzZsMCQZb775pmEYhrF582ZDkrF48eIab78irt5U9IiOjnavt3//fkOSERsbaxw5csS9/OuvvzYkGf/7v//rXtarVy+jadOmxsmTJ93Ltm7daoSFhRnjx493Lxs/frwRFhZW4Z9LcXFxmfqGDBniXmYYhvG///u/Rnh4uJGdnW0YhmG8//771f4ZAwg9TGkAgJ8VFRVp+fLlGj16tNq1a+denpycrNtvv11r1qxRbm6uJCkhIUE7d+7U3r17K9xWbGysoqKitGrVKvcUBKtuvfVWffvtt9q3b5972cKFCxUdHa1Ro0ZJkvsM7qeffqr8/Pwabb8qs2bNUlpaWpnHJ598Um690aNHq3nz5u7nffv2Vb9+/bR06VJJUnp6urZs2aK77rpLjRo1cq/Xo0cPDR061L1ecXGxlixZouuuu67CucMOh6PM84kTJ5ZZdsUVV6ioqMg9BSQhIUGS9NFHH8npdNayCwCCDYEXAH524sQJ5efnq2PHjuVe69y5s4qLi3X48GFJ5tUMsrOz1aFDB3Xv3l0PP/ywtm3b5l4/Ojpaf/vb3/TJJ5+oWbNmuvLKK/XUU08pIyOj2jpuvvlmhYWFuX9NbxiGFi9e7J5XLElt27bVlClT9NprrykxMVHDhw/XrFmz6jx/t2/fvhoyZEiZx1VXXVVuvYsuuqjcsg4dOrjnJ7sCaGW9zMrK0unTp3XixAnl5uaqW7dulupr1apVmecNGzaUJPc/KgYOHKgxY8ZoxowZSkxM1KhRozR37lwVFBRY2j6A4ETgBYBauPLKK7Vv3z7NmTNH3bp102uvvaaLL75Yr732mnud3/72t/r+++81c+ZMxcTE6M9//rM6d+6szZs3V7ntlJQUXXHFFVq0aJEkaf369Tp06JBuvfXWMus9++yz2rZtm/74xz/qzJkz+p//+R917dpVR44c8fwH9hPh4eEVLjcMQ5J5Rvjdd9/VunXrNHnyZB09elT33HOP+vTpo7y8PF+WCsCPEHgB4GdNmjRRXFyc9uzZU+613bt3KywsTC1btnQva9Soke6++2698847Onz4sHr06KHp06eXeV/79u31u9/9TsuXL9eOHTtUWFioZ599ttpabr31Vm3dulV79uzRwoULFRcXp+uuu67cet27d9ef/vQnrV69Wl9++aWOHj2ql156qeYfvoYqmsrx/fffu7+I1rp1a0mqtJeJiYmqV6+emjRpovj4+Aqv8FAXl156qf7yl79o48aNevvtt7Vz504tWLDAo/sAEDgIvADws/DwcA0bNkwffPBBmUuHZWZmav78+br88svdUwpOnjxZ5r0XXHCBLrzwQvevzvPz83X27Nky67Rv317169e39Ov1MWPGKDw8XO+8844WL16sa6+9VvXq1XO/npubq3PnzpV5T/fu3RUWFlZm+4cOHdLu3butNaAGlixZoqNHj7qfb9iwQV9//bVSU1MlmfOee/XqpTfeeEPZ2dnu9Xbs2KHly5dr5MiRkqSwsDCNHj1a//nPfyq8A57rzK1VP/30U7n39OrVS5KY1gCEMC5LBiDkzJkzR8uWLSu3/MEHH9QTTzyhtLQ0XX755frNb36jiIgIvfzyyyooKNBTTz3lXrdLly4aNGiQ+vTpo0aNGmnjxo169913NXnyZEnm2c7BgwfrlltuUZcuXRQREaH3339fmZmZuu2226qtsWnTprrqqqv03HPP6dSpU+WmM3z22WeaPHmybr75ZnXo0EHnzp3Tv//9b4WHh2vMmDHu9caPH68vvvjCcnD85JNPKgzIl112WZkv8l144YW6/PLLdf/996ugoEAvvPCCGjdurN///vfudZ5++mmlpqaqf//+mjBhgvuyZA0aNChzJvyvf/2rli9froEDB2rixInq3Lmz0tPTtXjxYq1Zs8b9RTQr3njjDf3rX//SDTfcoPbt2+vUqVN69dVXFR8f7w7ZAEKQrdeIAAAfqurSW5KMw4cPG4ZhGJs2bTKGDx9uXHDBBUZcXJxx1VVXGWvXri2zrSeeeMLo27evkZCQYMTGxhqdOnUy/vKXvxiFhYWGYRhGVlaWMWnSJKNTp05GvXr1jAYNGhj9+vUzFi1aZLneV1991ZBk1K9f3zhz5kyZ1/773/8a99xzj9G+fXsjJibGaNSokXHVVVcZK1asKLPewIEDDSt/1VfXm7lz5xqGUXJZsqefftp49tlnjZYtWxrR0dHGFVdcYWzdurXcdlesWGEMGDDAiI2NNeLj443rrrvO+O6778qtd/DgQWP8+PFGkyZNjOjoaKNdu3bGpEmTjIKCgjL1nX+5sc8//9yQZHz++eeGYZh/dmPHjjVatWplREdHG02bNjWuvfZaY+PGjdX2AEDwchhGDX9fBAAIWQcOHFDbtm319NNP66GHHrK7HACwhDm8AAAACGoEXgAAAAQ1Ai8AAACCGnN4AQAAENQ4wwsAAICgRuAFAABAUOPGExUoLi7WsWPHVL9+fTkcDrvLAQAAwHkMw9CpU6eUkpKisLCqz+ESeCtw7NgxtWzZ0u4yAAAAUI3Dhw+rRYsWVa5D4K1A/fr1JZkNjI+P9/r+nE6nli9frmHDhikyMtLr+wtU9Mk6emUNfbKGPllDn6yhT9bQp+rl5uaqZcuW7txWFQJvBVzTGOLj430WeOPi4hQfH89BXQX6ZB29soY+WUOfrKFP1tAna+iTdVamn/KlNQAAAAQ1Ai8AAACCGoEXAAAAQY05vAAAAF5mGIbOnTunoqIiS+s7nU5FRETo7Nmzlt8TbMLDwxUREeGRS8QSeAEAALyosLBQ6enpys/Pt/wewzCUlJSkw4cPh/Q9AeLi4pScnKyoqKg6bYfACwAA4CXFxcXav3+/wsPDlZKSoqioKEsBtri4WHl5ebrggguqvalCMDIMQ4WFhTpx4oT279+viy66qE59IPACAAB4SWFhoYqLi9WyZUvFxcVZfl9xcbEKCwsVExMTkoFXkmJjYxUZGamDBw+6e1FbodlBAAAAHwrV0FpXnuob3QcAAEBQI/ACAAAgqBF4AQAAENQIvAAAACjnrrvu0ujRo+0uwyMIvAAAAAhqBF4AAADUyBdffKG+ffsqOjpaycnJ+sMf/qBz5865X3/33XfVvXt3xcbGqnHjxhoyZIhOnz4tSVq1apX69u2revXqKSEhQQMGDNDBgwe9Wi/X4QUAAPChSy6RMjKqW8shw4j36F3WkpKkjRvrvp2jR49q5MiRuuuuu/Tmm29q9+7duu+++xQTE6Pp06crPT1dY8eO1VNPPaUbbrhBp06d0pdffum+vfLo0aN133336Z133lFhYaE2bNjg9bvJEXgBAAB8KCNDOnq0urUcPz/8z7/+9S+1bNlSL774ohwOhzp16qRjx47pkUce0aOPPqr09HSdO3dON954o1q3bi1J6t69uyTpxx9/VE5Ojq699lq1b99ektS5c2ev10zgDVF5edJf/yq1bi396ld2VwMAQOhISrKyliHDMH4+8+mZ4Gttv9XbtWuX+vfvX+as7IABA5SXl6cjR46oZ8+eGjx4sLp3767hw4dr2LBhuummm9SwYUM1atRId911l4YPH66hQ4dqyJAhuuWWW5ScnOyZ4ipB4A1Rr70mzZxpjnv3lvr2tbceAABChZVpBcXFhnJzcxUfH6+wMP8801uZ8PBwpaWlae3atVq+fLn++c9/6v/9v/+nr7/+Wm3bttXcuXP1P//zP1q2bJkWLlyoP/3pT0pLS9Oll17qtZr40lqI2r69ZPzZZ/bVAQAAAkvnzp21bt06GYbhXvbVV1+pfv36atGihSTJ4XBowIABmjFjhjZv3qyoqCi9//777vV79+6tqVOnau3aterWrZvmz5/v1Zo5wxuiDh8uGa9da18dAADAf+Xk5GjLli1llk2cOFEvvPCCHnjgAU2ePFl79uzRtGnTNGXKFIWFhenrr7/WypUrNWzYMDVt2lRff/21Tpw4oc6dO2v//v165ZVXdP311yslJUV79uzR3r17NX78eK9+DgJviCodeNetkwxD8vIXJAEAQIBZtWqVevfuXWbZhAkTtHTpUj388MPq2bOnGjVqpAkTJuhPf/qTJCk+Pl6rV6/WCy+8oNzcXLVu3VrPPvusUlNTlZmZqd27d+uNN97QyZMnlZycrEmTJulXXv5CEYE3BBlG2cCblSXt2yddeKF9NQEAAP8yb948zZs3r9LXN2zYUOHyzp07a9myZRW+1qxZszJTG3yFObwhKDtb+vnaz27r1tlSCgAAgNcReENQ6bO7LgReAAAQrAi8IYjACwAAQgmBNwRVFHi3bTNvRgEAABBsCLwhqHTgbdPG/G9xsfTNN7aUAwBA0Ct9zVpY56m+EXhDUOnAe8stJWOmNQAA4FmRkZGSpPz8fJsrCUyuvrn6WFtcliwElQ68N98sPfWUOSbwAgDgWeHh4UpISNDx48clSXFxcXJYuPB9cXGxCgsLdfbsWYWFhd75ScMwlJ+fr+PHjyshIUHh4eF12h6BNwS5Am+DBlKfPlKjRtKPP0rr13MDCgAAPC0pKUmS3KHXCsMwdObMGcXGxloKyMEqISHB3b+6IPCGGMOQjhwxxy1bmuH20kulpUvNG1D88IN00UX21ggAQDBxOBxKTk5W06ZN5XQ6Lb3H6XRq9erVuvLKK+v86/xAFRkZWeczuy4E3hBz4oRUUGCOW7Y0/9u/vxl4JXNaA4EXAADPCw8PtxzgwsPDde7cOcXExIRs4PWk0JsUEuJKz98tHXhdmMcLAACCDYE3xFQUePv2lVzz4Qm8AAAg2BB4Q0xFgbd+fal7d3O8fbt06pTv6wIAAPAWWwPv6tWrdd111yklJUUOh0NLliwp87rD4ajw8fTTT1e6zenTp5dbv1OnTl7+JIGjosArlUxr4AYUAAAg2NgaeE+fPq2ePXtq1qxZFb6enp5e5jFnzhw5HA6NGTOmyu127dq1zPvWrFnjjfIDUnWBV2JaAwAACC62XqUhNTVVqamplb5+/nXXPvjgA1111VVq165dlduNiIjwyDXbglHpwNuiRcmYwAsAAIJVwFyWLDMzUx9//LHeeOONatfdu3evUlJSFBMTo/79+2vmzJlq1apVpesXFBSowHWtLkm5ubmSzGvgWb1eXl249uGLfR0+HCHJocREQxER5+TaZevWUmJihLKyHFq3zlBh4Tm/uwGFL/sU6OiVNfTJGvpkDX2yhj5ZQ5+qV5PeOAzDMLxYi2UOh0Pvv/++Ro8eXeHrTz31lJ588kkdO3ZMMTExlW7nk08+UV5enjp27Kj09HTNmDFDR48e1Y4dO1S/fv0K3zN9+nTNmDGj3PL58+crLi6uVp/HHxUVSTfffJ2Ki8PUrl22nnvuizKvP/FEP23caJ4ZnzVrpZo3z7OjTAAAgGrl5+fr9ttvV05OjuLj46tcN2DO8M6ZM0fjxo2rMuxKKjNFokePHurXr59at26tRYsWacKECRW+Z+rUqZoyZYr7eW5urlq2bKlhw4ZV20BPcDqdSktL09ChQ716cekjR6TiYnPadpcu8Ro5cmSZ17dtC9PGjeY4OnqgRo70i38LufmqT8GAXllDn6yhT9bQJ2vokzX0qXqu38hbERCB98svv9SePXu0cOHCGr83ISFBHTp00A8//FDpOtHR0YqOji63PDIy0qcHmbf3l5FRMm7dOkyRkWW/s3j55SXjDRsiVMm/D2zn6z+XQEavrKFP1tAna+iTNfTJGvpUuZr0JSCuw/v666+rT58+6tmzZ43fm5eXp3379ik5OdkLlQWWyq7Q4PKLX3ADCgAAEHxsDbx5eXnasmWLtmzZIknav3+/tmzZokOHDrnXyc3N1eLFi3XvvfdWuI3BgwfrxRdfdD9/6KGH9MUXX+jAgQNau3atbrjhBoWHh2vs2LFe/SyBoHTgreg7fBdcIPXoYY537JBq8JsCAAAAv2Vr4N24caN69+6t3r17S5KmTJmi3r1769FHH3Wvs2DBAhmGUWlg3bdvn7KystzPjxw5orFjx6pjx4665ZZb1LhxY61fv15NmjTx7ocJANWd4ZVKLk9mGNKGDd6vCQAAwNtsncM7aNAgVXeRiIkTJ2rixImVvn7gwIEyzxcsWOCJ0oKS1cA7e7Y5XrdOGjLE+3UBAAB4U0DM4YVnuAKvwyGlpFS8DjegAAAAwYbAG0JcgTc5Warsi43t20uJieZ4/XqpuNg3tQEAAHgLgTdEFBZKmZnmuLLpDJJ59td1lvenn6Tvv/d+bQAAAN5E4A0RR4+aX0STqg68EtMaAABAcCHwhggrX1hzueyykjGBFwAABDoCb4ioSeC95BIpPNwcE3gBAECgI/CGiJoE3nr1JNdN7XbulHJyvFcXAACAtxF4Q0RNAq/EDSgAAEDwIPCGiNoGXolpDQAAILAReEOEK/BGREjNmlW/PoEXAAAECwJviHAF3ubNS76QVpW2baWmTc3xunXcgAIAAAQuAm8IyM+XTp40x1amM0hlb0CRkyPt3u2d2gAAALyNwBsCjhwpGVsNvBLTGgAAQHAg8IaAmn5hzYXACwAAggGBNwTUNvBecon5JTeJwAsAAAIXgTcE1DbwxsWV3IDiu++k7GyPlgUAAOATBN4QUNvAK5Wd1vD1156pBwAAwJcIvCHAU4GXaQ0AACAQEXhDgCvwxsRIiYk1ey+BFwAABDoCbwhwBd4WLczr69ZEmzYld2b7+mtuQAEAAAIPgTfI5eaaD6nm0xmk8jeg2LXLc7UBAAD4AoE3yNVl/q4L0xoAAEAgI/AGOU8E3ssuKxkTeAEAQKAh8AY5TwTePn24AQUAAAhcBN4g54nAGxsr9e5tjnftkn76qe51AQAA+AqBN8h5IvBK3IACAAAELgJvkPNG4GVaAwAACCQE3iDnCrwXXCA1aFD77RB4AQBAoCLwBjHDKAm8LVvW/KYTpbVqJSUnm+Ovv5aKiupeHwAAgC8QeIPYjz9KZ86Y47pMZ5DK3oAiN1f67ru6bQ8AAMBXCLxBzFPzd12Y1gAAAAIRgTeIEXgBAAAIvEHN04G3Tx8pMtIcE3gBAECgIPAGMU8H3piYkhtQ7NljzhEGAADwdwTeIObpwCuVndawfr1ntgkAAOBNBN4g5u3Ay7QGAAAQCAi8QcwVeBs2lOrV88w2CbwAACDQEHiDVHGxdOSIOfbU2V3XtlJSzDE3oAAAAIGAwBukjh+XnE5z7MnAW/oGFHl50s6dnts2AACANxB4g5Q35u+6XHZZyZhpDQAAwN8ReIOUNwMv83gBAEAgIfAGKW8G3osvlqKizDGBFwAA+DsCb5A6dKhk7OnAGx1thl5J+v576eRJz24fAADAkwi8QcqbZ3glbkABAAACB4E3SJUOvC1aeH77zOMFAACBgsAbpFyBt2lTcwqCpxF4AQBAoCDwBqFz56T0dHPsjekMknnW2HXmeMMGbkABAAD8F4E3CB07Zt5pTfJe4JXK3oBixw7v7QcAAKAubA28q1ev1nXXXaeUlBQ5HA4tWbKkzOt33XWXHA5HmceIESOq3e6sWbPUpk0bxcTEqF+/ftqwYYOXPoF/8vYX1lxKT2tYu9Z7+wEAAKgLWwPv6dOn1bNnT82aNavSdUaMGKH09HT345133qlymwsXLtSUKVM0bdo0bdq0ST179tTw4cN1/PhxT5fvt+wIvMzjBQAA/irCzp2npqYqNTW1ynWio6OVlJRkeZvPPfec7rvvPt19992SpJdeekkff/yx5syZoz/84Q91qjdQ+Crw9u5t3oCisJDACwAA/JetgdeKVatWqWnTpmrYsKGuvvpqPfHEE2rcuHGF6xYWFurbb7/V1KlT3cvCwsI0ZMgQrasikRUUFKigoMD9PDc3V5LkdDrldDo99Ekq59qHp/Z18GCYpHBJUnLyOTmdhke2e76wMOnii8O1fn2YfvhBOnbMqSZNvLIrSZ7vUzCjV9bQJ2vokzX0yRr6ZA19ql5NeuPXgXfEiBG68cYb1bZtW+3bt09//OMflZqaqnXr1ik8PLzc+llZWSoqKlKzZs3KLG/WrJl2795d6X5mzpypGTNmlFu+fPlyxcXF1f2DWJSWluaR7Wzc2FdSsiRp797PlJ19xiPbrUjTpl0lXShJmjXrW/Xtm+m1fbl4qk+hgF5ZQ5+soU/W0Cdr6JM19Kly+fn5ltf168B72223ucfdu3dXjx491L59e61atUqDBw/22H6mTp2qKVOmuJ/n5uaqZcuWGjZsmOLj4z22n8o4nU6lpaVp6NChioyMrPP2HnvM/MdAWJihceOuUoQX/5TPnHHoww/N8blzv9DIkcVe25en+xTM6JU19Mka+mQNfbKGPllDn6rn+o28FX4deM/Xrl07JSYm6ocffqgw8CYmJio8PFyZmWXPMmZmZlY5Dzg6OlrRFdydITIy0qcHmaf2d+SI+d/kZIdiY71b/xVXlIw3bAhXZGT5M++e5us/l0BGr6yhT9bQJ2vokzX0yRr6VLma9CWgrsN75MgRnTx5UsnJyRW+HhUVpT59+mjlypXuZcXFxVq5cqX6l76kQBArKJBcF6Tw5hfWXJo3L9nPhg3mTS8AAAD8ia2BNy8vT1u2bNGWLVskSfv379eWLVt06NAh5eXl6eGHH9b69et14MABrVy5UqNGjdKFF16o4cOHu7cxePBgvfjii+7nU6ZM0auvvqo33nhDu3bt0v3336/Tp0+7r9oQ7FxndyXfBF5Juuwy87/5+dL27b7ZJwAAgFW2TmnYuHGjrrrqKvdz1zzaO++8U7Nnz9a2bdv0xhtvKDs7WykpKRo2bJgef/zxMtMP9u3bp6ysLPfzW2+9VSdOnNCjjz6qjIwM9erVS8uWLSv3RbZg5atLkpXWv7+0cKE5XrfOvFwZAACAv7A18A4aNEiGUfklsz799NNqt3HgwIFyyyZPnqzJkyfXpbSAVTrwtmrlm32efwOK3/zGN/sFAACwIqDm8KJ6dpzh7dVLiokxx9yAAgAA+BsCb5CxI/BGRUl9+pjjfftKvjQHAADgDwi8QcaOwCuVndawfr3v9gsAAFAdAm+QcQXeyEipaVPf7ff8ebwAAAD+gsAbZFyBt0ULKcyHf7oEXgAA4K8IvEHk9Gnpp5/MsS+nM0hScrLUurU5/uYbbkABAAD8B4E3iNg1f9fFdZY3P1/ats33+wcAAKgIgTeI+EvglaS1a32/fwAAgIoQeIOIPwVe5vECAAB/QeANInYH3p49uQEFAADwPwTeIGJ34I2Kki65xBzv3y9lZvq+BgAAgPMReIOI3YFXYloDAADwPwTeIOIKvLGxUqNG9tRA4AUAAP6GwBskDKMk8LZsKTkc9tRB4AUAAP6GwBskcnKkvDxzbNd0BklKSpLatjXHGzdKTqd9tQAAAEgE3qDhD/N3XVxnec+ckbZutbcWAAAAAm+Q8MfAKzGtAQAA2I/AGyQIvAAAABUj8AYJfwq8PXqYV4qQCLwAAMB+BN4g4U+BNzJS+sUvzPGBA1JGhq3lAACAEEfgDRL+FHglpjUAAAD/QeANEq7AGx9vPuxG4AUAAP6CwBsEDEM6csQc+8PZXYnACwAA/AeBNwhkZUlnz5pjfwm8TZtK7dqZ440bpcJCe+sBAAChi8AbBPxt/q6L6yzv2bPcgAIAANiHwBsE/D3wSkxrAAAA9iHwBoFACLxr19pXBwAACG0E3iDgr4G3Rw8pLs4cc4YXAADYhcAbBPw18EZElNyA4tAh6dgxe+sBAAChicAbBEoH3hYt7KujIszjBQAAdiPwBgFX4G3cuGQKgb8g8AIAALsReANcUZH/3XSiNAIvAACwG4E3wGVkmKFX8s/A26SJdOGF5vjbb7kBBQAA8D0Cb4Dz1y+sleY6y1tQIG3ebG8tAAAg9BB4A1wgBV6JaQ0AAMD3CLwBjsALAABQNQJvgAuEwNutm1Svnjkm8AIAAF8j8Aa4QAi8ERFS377m+PBh6ehRe+sBAAChhcAb4FyB1+GQmje3t5aqMK0BAADYhcAb4FyBt1kzKSrK3lqqQuAFAAB2IfAGsMJC8zq8kv9OZ3C59NKSMYEXAAD4EoE3gB07JhmGOfb3wJuYKF10kTn+9lvzmrwAAAC+QOANYIHwhbXSXNMaCgu5AQUAAPAdAm8AC9TAKzGtAQAA+A6BN4AReAEAAKpH4A1ggRZ4u3WTLrjAHH/5pVRUZG89AAAgNBB4A1igBd7wcOmqq8xxRoa0erW99QAAgNBA4A1grsAbHi4lJ9tbi1XjxpWM337bvjoAAEDosDXwrl69Wtddd51SUlLkcDi0ZMkS92tOp1OPPPKIunfvrnr16iklJUXjx4/XsWPHqtzm9OnT5XA4yjw6derk5U9iD1fgTUkxQ28guO66kmkN774rnT1rbz0AACD42Rp4T58+rZ49e2rWrFnlXsvPz9emTZv05z//WZs2bdJ7772nPXv26Prrr692u127dlV6err7sWbNGm+Ub6szZ6SsLHMcCNMZXOLipBtvNMc5OdLHH9tbDwAACH4Rdu48NTVVqampFb7WoEEDpaWllVn24osvqm/fvjp06JBatWpV6XYjIiKUlJTk0Vr9zZEjJeNACrySOa3hzTfN8dtvS2PG2FsPAAAIbrYG3prKycmRw+FQQkJClevt3btXKSkpiomJUf/+/TVz5swqA3JBQYEKSt36Kzc3V5I5rcLpdHqk9qq49lGTfe3f75Drj6958yI5ncXeKM0rrrhCSkqKUEaGQx9/bOj48XNq2LD699WmT6GKXllDn6yhT9bQJ2vokzX0qXo16Y3DMFw3p7WXw+HQ+++/r9GjR1f4+tmzZzVgwAB16tRJb1fxbadPPvlEeXl56tixo9LT0zVjxgwdPXpUO3bsUP369St8z/Tp0zVjxoxyy+fPn6+4uLhafR5v++yzlvrHPy6WJN1773Zde+1/ba6oZl5/vZv+85/2kqRJkzZr6NBDNlcEAAACSX5+vm6//Xbl5OQoPj6+ynUDIvA6nU6NGTNGR44c0apVq6r9UKVlZ2erdevWeu655zRhwoQK16noDG/Lli2VlZVVo33VltPpVFpamoYOHarIyEhL7/nrX8M0fbr5TbVFi85p9Gi/+GO07NtvHerf3zxDfeWVxVqxovqL8tamT6GKXllDn6yhT9bQJ2vokzX0qXq5ublKTEy0FHj9fkqD0+nULbfcooMHD+qzzz6rcQBNSEhQhw4d9MMPP1S6TnR0tKKjo8stj4yM9OlBVpP9lb5YRdu2EQq0n4V+/aQOHaTvv5dWrw5TRkaY5bnIvv5zCWT0yhr6ZA19soY+WUOfrKFPlatJX/z6OryusLt3716tWLFCjRs3rvE28vLytG/fPiUHyoVqLQq0m06cz+GQ7rij5Pk779hXCwAACG62Bt68vDxt2bJFW7ZskSTt379fW7Zs0aFDh+R0OnXTTTdp48aNevvtt1VUVKSMjAxlZGSosLDQvY3BgwfrxRdfdD9/6KGH9MUXX+jAgQNau3atbrjhBoWHh2vs2LG+/nhe5Qq8UVFSkyb21lJbt99eMuYmFAAAwFtsndKwceNGXeW616ykKVOmSJLuvPNOTZ8+XR9++KEkqVevXmXe9/nnn2vQoEGSpH379inLdUFaSUeOHNHYsWN18uRJNWnSRJdffrnWr1+vJoGaCivhCrwtWkhhfn2evnLt20uXXiqtXy9t2yZt3y517253VQAAINjYGngHDRqkqr4zZ+X7dAcOHCjzfMGCBXUty++dOmXetEEKzOkMpY0bZwZeyTzL++ST9tYDAACCT4CeGwxtgT5/t7Rbby25LfL8+VJx4FxOGAAABAgCbwAqHXiruJ9GQGjSRBo+3BwfPiwF4V2gAQCAzQi8ASiYzvBK5rQGl7fesq8OAAAQnAi8ASjYAu+oUVK9euZ48WKp1D1AAAAA6ozAG4CCLfDWqyfdcIM5zs6WPvnE1nIAAECQIfAGoGALvFLZaQ1ckxcAAHgSgTcAuQJvvXpSQoKtpXjMkCFS06bm+D//KbnsGgAAQF0ReAOMYZQE3pYtzVv0BoOICPMSZZI5h/f//s/eegAAQPAg8AaYn36S8vPNcbBMZ3C5446SMdMaAACApxB4A0wwzt91+cUvpAsvNMeffy4dPWpvPQAAIDgQeANMMAdeh6Pky2uGIb3zjr31AACA4EDgDTDBHHglrtYAAAA8j8AbYII98F50kdS3rzneskX67jtbywEAAEGAwBtggj3wSpzlBQAAnkXgDTChEHhvvVUKDzfHb78tFRfbWw8AAAhsBN4A4wq8CQnSBRfYWorXNGtm3ohCkg4elNautbceAAAQ2Ai8AaS4WDpyxBwH69ldF67JCwAAPIXAG0BOnJAKC81xsAfe0aOluDhzvGhRyecGAACoKQJvAAmF+bsuF1wgjRpljn/8UVq2zN56AABA4CLwBpBQCrwSV2sAAACeQeANIKEWeIcNkxITzfGHH0q5ufbWAwAAAhOBN4AcOlQyDoXAGxlpXqJMks6elZYscdhbEAAACEgE3gASamd4pbLTGt55h8MVAADUHAkigJQOvC1a2FeHL116qdSunTn+/HOHfvwx2t6CAABAwCHwBhBX4G3SRIqJsbcWX3E4Ss7yFhc7tGZNc3sLAgAAAYfAGyDOnZOOHTPHoTKdwaX0tIYvvgixDw8AAOqMwBsg0tPNO61JoRd4O3aU+vQxx/v2JWj3bnvrAQAAgYXAGyBC8QtrpfHlNQAAUFskhwAR6oH3ttuksDBDkrRwYZgMw+aCAABAwCDwBohQD7zJydLVV5sp97//dWj9epsLAgAAAYPAGyBCPfBK0m23FbvHb71lYyEAACCgEHgDBIFXGj3aUFRUkSRp0SLJ6bS5IAAAEBAIvAHCFXgdDiklxd5a7BIfL/Xtmy5JysqSli+3uSAAABAQCLwBwhV4k5OlyEh7a7HTlVcecY/fftvGQgAAQMAg8AaAggIpM9Mch+p0BpfevY+rUSPzy2tLlkinTtlbDwAA8H8E3gBw9GjJONQDb2SkoZtuMr+8duaMGXoBAACqQuANAHxhrazbby+5CC/TGgAAQHUIvAGAwFtW//6G2rQxx2lpJdM9AAAAKkLgDQAE3rIcDun2281xcbG0YIG99QAAAP9G4A0ABN7yxo0rGTOtAQAAVIXAGwAIvOV16SL17m2Ov/lG2rvX3noAAID/IvAGAFfgjYiQmjWztxZ/wlleAABgBYE3ALgCb/PmUni4vbX4k9tuM+fzStJbb0mGUfX6AAAgNBF4/Vx+vvTjj+aY6QxlNW8uXX21Od63T9qwwd56AACAfyLw+jnm71aNaQ0AAKA6BF4/R+Ct2o03StHR5njBAsnptLceAADgfwi8fo7AW7UGDaTrrjPHJ05IK1bYWw8AAPA/BF4/R+Ct3h13lIyZ1gAAAM5H4PVzBN7qpaZKDRua4yVLpNOnbS0HAAD4GVsD7+rVq3XdddcpJSVFDodDS5YsKfO6YRh69NFHlZycrNjYWA0ZMkR7LdxhYNasWWrTpo1iYmLUr18/bQjgr+8TeKsXFSXdfLM5Pn1a+uADe+sBAAD+xdbAe/r0afXs2VOzZs2q8PWnnnpK//jHP/TSSy/p66+/Vr169TR8+HCdPXu20m0uXLhQU6ZM0bRp07Rp0yb17NlTw4cP1/Hjx731MbzKFXhjYqTERHtr8Welr9bw1lv21QEAAPyPrYE3NTVVTzzxhG644YZyrxmGoRdeeEF/+tOfNGrUKPXo0UNvvvmmjh07Vu5McGnPPfec7rvvPt19993q0qWLXnrpJcXFxWnOnDle/CTe4wq8LVqU3GQB5V1+udSqlTlevlwK0H/fAAAAL4iozZsOHz4sh8OhFi1aSJI2bNig+fPnq0uXLpo4caJHCtu/f78yMjI0ZMgQ97IGDRqoX79+WrdunW677bZy7yksLNS3336rqVOnupeFhYVpyJAhWrduXaX7KigoUEFBgft5bm6uJMnpdMrpg+tcufZx/r5ycqRTpyIlSS1aFMvpLPJ6Lf6ssj653HprmJ5+OlxFRdI77xTpN78p9mV5fqW6XsFEn6yhT9bQJ2vokzX0qXo16U2tAu/tt9+uiRMn6pe//KUyMjI0dOhQde3aVW+//bYyMjL06KOP1mazZWRkZEiSmjVrVmZ5s2bN3K+dLysrS0VFRRW+Z/fu3ZXua+bMmZoxY0a55cuXL1dcXFxNS6+1tLS0Ms8PHqwvybyVmMNxREuXbvZZLf7s/D65tGhR0q/Zs3PUps2XPqzKP1XWK5RFn6yhT9bQJ2vokzX0qXL5+fmW161V4N2xY4f69u0rSVq0aJG6deumr776SsuXL9evf/1rjwReX5o6daqmTJnifp6bm6uWLVtq2LBhio+P9/r+nU6n0tLSNHToUEVGRrqXL1tWMoehX7/mGjky2eu1+LPK+lTaa68Z2r7doT17GqlDh5G68EIfF+knrPQK9Mkq+mQNfbKGPllDn6rn+o28FbUKvE6nU9E/395qxYoVuv766yVJnTp1Unp6em02WU5SUpIkKTMzU8nJJUEvMzNTvXr1qvA9iYmJCg8PV2ZmZpnlmZmZ7u1VJDo62v15SouMjPTpQXb+/kq3sk2bcEVGhvusFn9W1Z/LL38p/f735njx4kgF2L+9PM7Xx3Cgok/W0Cdr6JM19Mka+lS5mvSlVl9a69q1q1566SV9+eWXSktL04gRIyRJx44dU+PGjWuzyXLatm2rpKQkrVy50r0sNzdXX3/9tfr371/he6KiotSnT58y7ykuLtbKlSsrfY8/45JkNTd2bMmX+95+WzIMe+sBAAD2q1Xg/dvf/qaXX35ZgwYN0tixY9WzZ09J0ocffuie6mBFXl6etmzZoi1btkgyv6i2ZcsWHTp0SA6HQ7/97W/1xBNP6MMPP9T27ds1fvx4paSkaPTo0e5tDB48WC+++KL7+ZQpU/Tqq6/qjTfe0K5du3T//ffr9OnTuvvuu2vzUW1F4K25Fi2kgQPN8fffS99+a289AADAfrWa0jBo0CBlZWUpNzdXDV23uJI0ceLEGn3Ja+PGjbrqqqvcz13zaO+8807NmzdPv//973X69GlNnDhR2dnZuvzyy7Vs2TLFxMS437Nv3z5lZWW5n9966606ceKEHn30UWVkZKhXr15atmxZuS+yBQICb+2MGyetWmWO33pLuuQSW8sBAAA2q1XgPXPmjAzDcIfdgwcP6v3331fnzp01fPhwy9sZNGiQjCp+5+xwOPTYY4/pscceq3SdAwcOlFs2efJkTZ482XId/soVeC+4QGrQwN5aAslNN0mTJkmFhdKCBdIzz0gRtTrSAQBAMKjVlIZRo0bpzTfflCRlZ2erX79+evbZZzV69GjNnj3bowWGKsMoCbwtW3LTiZpISJCuvdYcZ2ZKn31mazkAAMBmtQq8mzZt0hVXXCFJevfdd9WsWTMdPHhQb775pv7xj394tMBQdfKk5LqDsusOYrCu9K2G337bvjoAAID9ahV48/PzVb9+fUnmzRluvPFGhYWF6dJLL9XBgwc9WmCoYv5u3YwcWTIN5L33pBpcmxoAAASZWgXeCy+8UEuWLNHhw4f16aefatiwYZKk48eP++RGDaGAwFs3MTHSzTeb47w86cMP7a0HAADYp1aB99FHH9VDDz2kNm3aqG/fvu5r3C5fvly9e/f2aIGhisBbd0xrAAAAUi2v0nDTTTfp8ssvV3p6uvsavJJ5TdwbbrjBY8WFMgJv3V15pXld3iNHpGXLpKwsKTHR7qoAAICv1eoMr2Te+rd37946duyYjhw5Iknq27evOnXq5LHiQhmBt+7Cwsw7r0nSuXPSokX21gMAAOxRq8BbXFysxx57TA0aNFDr1q3VunVrJSQk6PHHH1dxcbGnawxJBF7PuOOOkjHTGgAACE21mtLw//7f/9Prr7+uJ598UgMGDJAkrVmzRtOnT9fZs2f1l7/8xaNFhiJX4G3USKrBzetwnh49pG7dpB07pLVrpf37pbZt7a4KAAD4Uq3O8L7xxht67bXXdP/996tHjx7q0aOHfvOb3+jVV1/VvHnzPFxi6Ckqko4eNcec3a270l9eW7DAvjoAAIA9ahV4f/zxxwrn6nbq1Ek//vhjnYsKdZmZktNpjgm8dXfTTSXjtWvtqwMAANijVoG3Z8+eevHFF8stf/HFF9WjR486FxXqmL/rWe3bS67LQ2/dam8tAADA92o1h/epp57SNddcoxUrVrivwbtu3TodPnxYS5cu9WiBoYjA61kOhzmXd80as7c//mjOjQYAAKGhVmd4Bw4cqO+//1433HCDsrOzlZ2drRtvvFE7d+7Uv//9b0/XGHIIvJ5X6nLR2rbNvjoAAIDv1eoMrySlpKSUuxrD1q1b9frrr+uVV16pc2GhjMDreaUD79at0qBBtpUCAAB8rNY3noD3EHg97/zACwAAQgeB1w+VDrzNm9tXRzDp1s2885pE4AUAINQQeP2QK/A2ayZFR9tbS7CIi5M6dDDHO3eatxoGAAChoUZzeG+88cYqX8/Ozq5LLZB5/d30dHPMdAbP6tlT2r1bKiiQ9uyRuna1uyIAAOALNQq8DRo0qPb18ePH16mgUHfsmGQY5pjA61k9e0oLF5rjLVsIvAAAhIoaBd65c+d6qw78jC+sec/5X1wrfcthAAAQvJjD62cIvN7DlRoAAAhNBF4/Q+D1npQUqXFjc0zgBQAgdBB4/QyB13scjpKzvJmZ5gMAAAQ/Aq+fIfB6F9MaAAAIPQReP+MKvGFhUnKyvbUEIwIvAAChh8DrZ1yBNyVFiqjRNTRgBYEXAIDQQ+D1I2fPSidOmGOmM3hH584l/5Ag8AIAEBoIvH7kyJGSMYHXO6KjzdArldx1DQAABDcCrx85csThHhN4vcc1reHcOem77+ytBQAAeB+B149whQbfYB4vAAChhcDrRzjD6xsEXgAAQguB148wh9c3CLwAAIQWAq8f4QyvbzRtKiUlmeOtWyXDsLceAADgXQReP3L4sBl4IyPNUAbvcZ3l/fFH6ehRe2sBAADeReD1I64pDS1amHdag/cwrQEAgNBBrPITZ86EKzvbPMPLdAbv69WrZEzgBQAguBF4/URWVqx7TOD1Ps7wAgAQOgi8foLA61sdOph3XZOkLVtsLQUAAHgZgddPEHh9KyJC6tbNHO/dK50+bW89AADAewi8foLA63uuaQ2GIe3YYW8tAADAewi8foLA63vM4wUAIDQQeP0Egdf3CLwAAIQGAq+fOHnSDLyxsVKjRjYXEyJ69CgZE3gBAAheBF4/YBglZ3hbtpQcjmreAI9o2FBq1cocb9smFRfbWw8AAPAOAq8fyM6Wzp6NkMR0Bl9zTWs4dUo6cMDWUgAAgJcQeP3A4cMlYwKvbzGPFwCA4Efg9QNHjpTMYSDw+haBFwCA4Of3gbdNmzZyOBzlHpMmTapw/Xnz5pVbNyYmxsdV1wyB1z4EXgAAgl+E3QVU55tvvlFRUZH7+Y4dOzR06FDdfPPNlb4nPj5ee/bscT93+Pm3wJjSYJ/27aV69cw7rRF4AQAITn4feJs0aVLm+ZNPPqn27dtr4MCBlb7H4XAoKSnJ26V5DGd47RMWJnXvLq1fL+3fL+XmSvHxdlcFAAA8ye8Db2mFhYV66623NGXKlCrP2ubl5al169YqLi7WxRdfrL/+9a/q2rVrpesXFBSooKDA/Tw3N1eS5HQ65XQ6PfcBKnH4cMnMkqQkp3ywy4Dk+rPw9J9J9+5hWr8+XJK0adM5DRhgeHT7dvBWr4INfbKGPllDn6yhT9bQp+rVpDcOwzAC5v/uixYt0u23365Dhw4pJSWlwnXWrVunvXv3qkePHsrJydEzzzyj1atXa+fOnWrRokWF75k+fbpmzJhRbvn8+fMVFxfn0c9QkV//erAyMi5QXJxT8+cv9fr+UNYnn7TRyy+bk3knTtymkSP321wRAACoTn5+vm6//Xbl5OQovppfzwZU4B0+fLiioqL0n//8x/J7nE6nOnfurLFjx+rxxx+vcJ2KzvC2bNlSWVlZ1TawrgxDio+PUEGBQ507F2vr1qLq3xSinE6n0tLSNHToUEVGRnpsu+vWOTRwoPnLjgkTijV7duD/GXirV8GGPllDn6yhT9bQJ2voU/Vyc3OVmJhoKfAGzJSGgwcPasWKFXrvvfdq9L7IyEj17t1bP/zwQ6XrREdHKzo6usL3evsgO35ccmXtVq3EQW2Bp/9cevcuGW/fHqbISL+/eIllvjiGgwF9soY+WUOfrKFP1tCnytWkLwHzf/a5c+eqadOmuuaaa2r0vqKiIm3fvl3JycleqqxuSl+hoZIZF/Cy+vXNqzVI0vbtUlHgn+AFAAClBETgLS4u1ty5c3XnnXcqIqLsSenx48dr6tSp7uePPfaYli9frv/+97/atGmT7rjjDh08eFD33nuvr8u27Mori9Ws2Wm1axcws0uCjut6vGfOSFX8MgAAAASggJjSsGLFCh06dEj33HNPudcOHTqksLCS3P7TTz/pvvvuU0ZGhho2bKg+ffpo7dq16tKliy9LtqxPH2nFiiItXbpCI0eOlBRud0khqVcvyTVbZssWqWNHO6sBAACeFBCBd9iwYarsu3WrVq0q8/z555/X888/74OqEEzOv+ParbfaVwsAAPCsgJjSAHgbtxgGACB4EXgBmVfISEgwxwReAACCC4EXkORwSD16mOOjR6WTJ+2tBwAAeA6BF/gZ0xoAAAhOBF7gZwReAACCE4EX+BmBFwCA4ETgBX7WtavkuqQzgRcAgOBB4AV+FhtbcsOJ776TnE576wEAAJ5B4AVKcU1rKCyUdu+2txYAAOAZBF6gFObxAgAQfAi8QCkEXgAAgg+BFyiFwAsAQPAh8AKlJCdLiYnmmMALAEBwIPACpTgcJWd5jx+XMjLsrQcAANQdgRc4D9MaAAAILgRe4DwEXgAAgguBFzgPgRcAgOBC4AXO07mzFBlpjgm8AAAEPgIvcJ6oKKlLF3O8e7d09qy99QAAgLoh8AIVcE1rKCqSdu60txYAAFA3BF6gAszjBQAgeBB4gQoQeAEACB4EXqACBF4AAIIHgReoQGKilJJijrdulQzD3noAAEDtEXiBSrjO8mZnS4cP21oKAACoAwIvUAmmNQAAEBwIvEAlCLwAAAQHAi9QCQIvAADBgcALVOKii6SYGHNM4AUAIHAReIFKRERI3bqZ4x9+kE6ftrceAABQOwReoAquaQ2GIW3fbm8tAACgdgi8QBWYxwsAQOAj8AJVIPACABD4CLxAFXr0KBkTeAEACEwEXqAKCQlS69bmeNs2qbjY1nIAAEAtEHiBarimNeTlSfv321sLAACoOQIvUA3m8QIAENgIvEA1SgfeLVtsKwMAANQSgReoBmd4AQAIbAReoBrt2kkXXGCOCbwAAAQeAi9QjbCwksuTHTwoZWfbWg4AAKghAi9gQelpDdu22VcHAACoOQIvYAHzeAEACFwEXsACAi8AAIGLwAtY0L275HCYYwIvAACBhcALWFCvnnThheZ4xw7p3Dl76wEAANYReAGLXNMazp6V9u61txYAAGAdgRewiHm8AAAEJr8OvNOnT5fD4Sjz6NSpU5XvWbx4sTp16qSYmBh1795dS5cu9VG1CHYEXgAAApNfB15J6tq1q9LT092PNWvWVLru2rVrNXbsWE2YMEGbN2/W6NGjNXr0aO3YscOHFSNYEXgBAAhMfh94IyIilJSU5H4kJiZWuu7f//53jRgxQg8//LA6d+6sxx9/XBdffLFefPFFH1aMYNWypZSQYI4JvAAABI4Iuwuozt69e5WSkqKYmBj1799fM2fOVKtWrSpcd926dZoyZUqZZcOHD9eSJUuq3EdBQYEKCgrcz3NzcyVJTqdTTqezbh/AAtc+fLGvQOYPferRI1yrV4fp2DEpPd2pKv79ZSt/6FUgoE/W0Cdr6JM19Mka+lS9mvTGYRiG4cVa6uSTTz5RXl6eOnbsqPT0dM2YMUNHjx7Vjh07VL9+/XLrR0VF6Y033tDYsWPdy/71r39pxowZyszMrHQ/06dP14wZM8otnz9/vuLi4jzzYRAUXnutmz76qL0kacaMr9SzZ5bNFQEAEJry8/N1++23KycnR/Hx8VWu69dneFNTU93jHj16qF+/fmrdurUWLVqkCRMmeGw/U6dOLXNmODc3Vy1bttSwYcOqbaAnOJ1OpaWlaejQoYqMjPT6/gKVP/Tp+HGHPvrIHMfGXqqRI4ttqaM6/tCrQECfrKFP1tAna+iTNfSpeq7fyFvh14H3fAkJCerQoYN++OGHCl9PSkoqdyY3MzNTSUlJVW43Ojpa0dHR5ZZHRkb69CDz9f4ClZ19uvjikvGOHeGKjAy3pQ6rOKasoU/W0Cdr6JM19Mka+lS5mvTF77+0VlpeXp727dun5OTkCl/v37+/Vq5cWWZZWlqa+vfv74vyEAK6dpXCf864fHENAIDA4NeB96GHHtIXX3yhAwcOaO3atbrhhhsUHh7unqM7fvx4TZ061b3+gw8+qGXLlunZZ5/V7t27NX36dG3cuFGTJ0+26yMgyMTESB07muPvvpMKC+2tBwAAVM+vA++RI0c0duxYdezYUbfccosaN26s9evXq0mTJpKkQ4cOKT093b3+ZZddpvnz5+uVV15Rz5499e6772rJkiXq1q2bXR8BQch1PV6nU9q1y95aAABA9fx6Du+CBQuqfH3VqlXllt188826+eabvVQRYAbed94xx1u3lr0hBQAA8D9+fYYX8Ee9epWMmccLAID/I/ACNcQthgEACCwEXqCGkpKkpk3N8datkv/eugUAAEgEXqBWXGd5s7KkUt+bBAAAfojAC9QC0xoAAAgcBF6gFgi8AAAEDgIvUAsEXgAAAgeBF6iFTp2kqChzTOAFAMC/EXiBWoiMlLp0Mcd79khnzthbDwAAqByBF6gl17SG4mJp5057awEAAJUj8AK1xDxeAAACA4EXqCUCLwAAgYHAC9QSgRcAgMBA4AVqqXFjqXlzc8wthgEA8F8EXqAOXGd5c3KkQ4fsrQUAAFSMwAvUAdMaAADwfwReoA5KB94tW2wrAwAAVIHAC9QBZ3gBAPB/BF6gDi66SIqNNccEXgAA/BOBF6iD8HCpWzdzvG+fdOqUvfUAAIDyCLxAHZWe1rB9u311AACAihF4gTrq1atkzLQGAAD8D4EXqCO+uAYAgH8j8AJ11KNHyZjACwCA/yHwAnUUHy+1bWuOt2+XiovtrQcAAJRF4AU8wDWt4fRp82oNAADAfxB4AQ9gHi8AAP6LwAt4AIEXAAD/ReAFPIDACwCA/yLwAh7Qpo1Uv745JvACAOBfCLyAB4SFlVye7NAh6aef7K0HAACUIPACHlJ6WsO2bfbVAQAAyiLwAh7CPF4AAPwTgRfwEAIvAAD+icALeEi3bpLDYY4JvAAA+A8CL+Ah9epJF11kjnfskM6ds7ceAABgIvACHuSa1lBQIO3ZY28tAADAROAFPIh5vAAA+B8CL+BBBF4AAPwPgRfwIAIvAAD+h8ALeFCLFlLDhuaYwAsAgH8g8AIe5HBIvXqZ44wM6fhxW8sBAAAi8AIex7QGAAD8C4EX8DACLwAA/oXAC3gYgRcAAP9C4AU8rEsXKSLCHBN4AQCwH4EX8LDoaKlTJ3O8a5d51zUAAGAfAi/gBa5pDefOmaEXAADYh8ALeAHzeAEA8B9+HXhnzpypX/ziF6pfv76aNm2q0aNHa8+ePVW+Z968eXI4HGUeMTExPqoYMBF4AQDwH34deL/44gtNmjRJ69evV1pampxOp4YNG6bTp09X+b74+Hilp6e7HwcPHvRRxYCJwAsAgP+IsLuAqixbtqzM83nz5qlp06b69ttvdeWVV1b6PofDoaSkJG+XB1SqWTPzkZlpBl7DMO/CBgAAfM+vA+/5cnJyJEmNGjWqcr28vDy1bt1axcXFuvjii/XXv/5VXbt2rXT9goICFZT6Kn1ubq4kyel0yul0eqDyqrn24Yt9BbJA61OPHuFKSwvTyZPSnj1OtW/vu30HWq/sQp+soU/W0Cdr6JM19Kl6NemNwzAMw4u1eExxcbGuv/56ZWdna82aNZWut27dOu3du1c9evRQTk6OnnnmGa1evVo7d+5UixYtKnzP9OnTNWPGjHLL58+fr7i4OI99BoSWf/+7s/7v/zpIkpKT8zR9+jo1a5Zvc1UAAASH/Px83X777crJyVF8fHyV6wZM4L3//vv1ySefaM2aNZUG14o4nU517txZY8eO1eOPP17hOhWd4W3ZsqWysrKqbaAnOJ1OpaWlaejQoYqMjPT6/gJVoPXp2DHp8ssjdOSIOZchKcnQf/5zrsz8Xm8JtF7ZhT5ZQ5+soU/W0Cdr6FP1cnNzlZiYaCnwBsSUhsmTJ+ujjz7S6tWraxR2JSkyMlK9e/fWDz/8UOk60dHRio6OrvC9vjzIfL2/QBUofWrdWlq7Vho+3LwWb0aGQ4MHR+rDD6WBA31TQ6D0ym70yRr6ZA19soY+WUOfKleTvvj1VRoMw9DkyZP1/vvv67PPPlPbtm1rvI2ioiJt375dycnJXqgQqFrLltKXX0qXXmo+z801A/B779lbFwAAocSvA++kSZP01ltvaf78+apfv74yMjKUkZGhM2fOuNcZP368pk6d6n7+2GOPafny5frvf/+rTZs26Y477tDBgwd177332vERADVuLK1YIaWmms8LCqSbb5ZeecXeugAACBV+HXhnz56tnJwcDRo0SMnJye7HwoUL3escOnRI6enp7uc//fST7rvvPnXu3FkjR45Ubm6u1q5dqy5dutjxEQBJUr160gcfSL/8pfm8uFj61a+kxx83L1kGAAC8x6/n8Fr5Pt2qVavKPH/++ef1/PPPe6kioPYiI6V588zr8z7zjLns0UfNa/X+/e9SeLit5QEAELT8+gwvEGzCwqSnnzYfLrNmSWPHmlMdAACA5xF4ARs89JD05ptSxM+/Y1m8WBo50vxSGwAA8CwCL2CTX/5S+vBDyXVvk88+kwYNMqc4AAAAzyHwAjZKTZVWrpRcd8vevFkaMEDat8/eugAACCYEXsBml14qrVljXrNXMsPugAFm+AUAAHVH4AX8QOfO5l3ZXFfPy8w078b2+ef21gUAQDAg8AJ+okUL865sl11mPj91ShoxQnr3XXvrAgAg0BF4AT/SqJGUliZde635vLBQuuUW6aWX7K0LAIBARuAF/ExcnPT++9Ldd5vPDUO6/35p+nTuygYAQG0QeAE/FBEhvf669MgjJctmzJB+8xupqMi+ugAACEQEXsBPORzSk09Kzz1Xsuyll8wpDmfP2lcXAACBhsAL+Ln//V/prbdK7sr23nvm9XtzcuytCwCAQEHgBQLAuHHSRx9J9eqZz1etMu/KlpFhZ1UAAAQGAi8QIIYPN28/3Lix+XzLFvMGFT/8YGtZAAD4PQIvEED69pW++kpq1cp8/t//mqF30yZ76wIAwJ8ReIEA07GjeVe2bt3M58ePm3dlW7nS3roAAPBXBF4gADVvLq1eLV1+ufk8L08aOVJatMjeugAA8EcEXiBANWwoLV8uXX+9+bywULrtNmnWLHvrAgDA3xB4gQAWGyv93/9JEyaYzw1DmjxZmj49jLuyAQDwswi7CwBQNxER0quvSklJ0l/+Yi7761/D1b37ZTp92qExY6SYGHtrBADATpzhBYKAwyE98YT0j3+ULNu+vYnuuCNCKSnmWd9Nm8RZXwBASCLwAkHkgQfMKQ5t25Yk259+Muf19ukj9e4t/f3vUlaWjUUCAOBjBF4gyNx4o7Rr1zk9/vhXGjeuWLGxJa9t3Sr99rdSSop0883S0qXSuXO2lQoAgE8QeIEgFBYmde+epblzi5SeLr38snTppSWvO53Su+9K11wjtW4t/fGP0t699tULAIA3EXiBINeggTRxorRunbRzp/TQQ1LTpiWvHzsmzZwpdeggXXGFNHeueV1fAACCBYEXCCFdukhPPy0dOSJ98IE0apQUHl7y+po10j33mFd8uOce6csv+aIbACDwEXiBEBQZad6wYskS6ehR6ZlnzDDscvq0eab3yivNWxnPnGmuBwBAICLwAiGuWTPpd7+TduyQ1q+XfvUrKT6+5PW9e805vq1ambcvfvddqaDAvnoBAKgpAi8ASea1fPv1k156SUpPl/79b+nqq0teLy6WPvnEvLpDSor04IPmVR8AAPB3BF4A5cTFSXfcIa1cKf33v9Kjj5pneF1+/NG8yUWvXtLFF0svvmguAwDAH3FrYQBVattWmjFDmjZN+uwzac4c6b33SqY1bN5s3vDid7+Thg41v/AWH28+6tcvGVf2PDLS3s8HAAh+BF4AloSFSUOGmI+ffpIWLDDD78aN5uuFhdLHH9d8uzExNQvIFT1v2NA8Kw0AQEUIvABqrGFD6f77zcf27eYVHf7979rdsvjsWfNx/HjdaoqLk5o0KftITCy/zLW8QYO67Q8AEDgIvADqpHt36bnnpKeeMq/ve+qUlJtb8ij9vLJx6ee1vdVxfr508KD5sCIyUkpMjFB09CD94x/hFQbj0qG5ceOy1ywOZYZR/cPqelWt6xIWZn6p0vWoy3OUVbrPvho7ndLZs+E6fbr6KU1WrgNem2uFu46F0sdFTZZV9Lo/s/Kz5snlDRtKUVG+/YzVIfAC8IiICKlNm7ptwzDMs701Cck5OeYX5k6cMB8nT5pXlKiO0ymlpzskNdCBA9Wv73BIjRqVBOHGjc0gZRjm/kr/pe+r53X5b83eE6Hi4lF1+8P1I9UF5IrCS0WhqvyyCBUVXauwsLAq1qn9spo+r2yZ/SIlXWt3EV5TUTCunQgZxvW1fvf5/3D0pTVrpAED7Nl3ZQi8APyGwyHFxpqP0rc/romiInOO8YkT5hQLVxCu6JGVZSgzs1jnzlV/6tYwzDB98qS0e3ftagtcfn76qoas/IOodhyS+DVAqCt9prNuAvfnzh//oUXgBRBUwsPNaQiJidWv63Se08cfL9UVV4xUdnbkeWG48qB8+rT3P0d1v6J3LavLf62uKxUrNzdHDRs2kMMRVu5saGWP8z9Hbdb19Vlz16OiM3PVLTMMQ6dO5So+Pl6OUi/UZluVLavp85q+xxdjwyhWVtZJNWnSWA5H9VdHtXKWtCZnUs//lXxNl9XmPbU502sYxcrJyVFCQgNLfapITX7WPLm8UaNaletVBF4AIc3hMK/00Lix1L69tfecOWNOozCMsmG0ql+V13TeqT9xOou0dOlqjRw5UpGRXL69Mk7nOS1duurnPnG9vcqYx9Najqdq8HPnWQReAKih2FipeXO7qwAAWMU/GQAAABDUCLwAAAAIagReAAAABDUCLwAAAIIagRcAAABBjcALAACAoEbgBQAAQFAj8AIAACCoEXgBAAAQ1AIi8M6aNUtt2rRRTEyM+vXrpw0bNlS5/uLFi9WpUyfFxMSoe/fuWrp0qY8qBQAAgL/x+8C7cOFCTZkyRdOmTdOmTZvUs2dPDR8+XMePH69w/bVr12rs2LGaMGGCNm/erNGjR2v06NHasWOHjysHAACAP/D7wPvcc8/pvvvu0913360uXbropZdeUlxcnObMmVPh+n//+981YsQIPfzww+rcubMef/xxXXzxxXrxxRd9XDkAAAD8QYTdBVSlsLBQ3377raZOnepeFhYWpiFDhmjdunUVvmfdunWaMmVKmWXDhw/XkiVLKt1PQUGBCgoK3M9zc3MlSU6nU06nsw6fwBrXPnyxr0BGn6yjV9bQJ2vokzX0yRr6ZA19ql5NeuPXgTcrK0tFRUVq1qxZmeXNmjXT7t27K3xPRkZGhetnZGRUup+ZM2dqxowZ5ZYvX75ccXFxtai8dtLS0ny2r0BGn6yjV9bQJ2vokzX0yRr6ZA19qlx+fr7ldf068PrK1KlTy5wVzs3NVcuWLTVs2DDFx8d7ff9Op1NpaWkaOnSoIiMjvb6/QEWfrKNX1tAna+iTNfTJGvpkDX2qnus38lb4deBNTExUeHi4MjMzyyzPzMxUUlJShe9JSkqq0fqSFB0drejo6HLLIyMjfXqQ+Xp/gYo+WUevrKFP1tAna+iTNfTJGvpUuZr0xa+/tBYVFaU+ffpo5cqV7mXFxcVauXKl+vfvX+F7+vfvX2Z9yfx1QGXrAwAAILj59RleSZoyZYruvPNOXXLJJerbt69eeOEFnT59Wnfffbckafz48WrevLlmzpwpSXrwwQc1cOBAPfvss7rmmmu0YMECbdy4Ua+88orlfRqGIalmp8rrwul0Kj8/X7m5ufwrrgr0yTp6ZQ19soY+WUOfrKFP1tCn6rlymiu3VckIAP/85z+NVq1aGVFRUUbfvn2N9evXu18bOHCgceedd5ZZf9GiRUaHDh2MqKgoo2vXrsbHH39co/0dPnzYkMSDBw8ePHjw4MHDzx+HDx+uNts5DMNKLA4txcXFOnbsmOrXry+Hw+H1/bm+JHf48GGffEkuUNEn6+iVNfTJGvpkDX2yhj5ZQ5+qZxiGTp06pZSUFIWFVT1L1++nNNghLCxMLVq08Pl+4+PjOagtoE/W0Str6JM19Mka+mQNfbKGPlWtQYMGltbz6y+tAQAAAHVF4AUAAEBQI/D6gejoaE2bNq3CawGjBH2yjl5ZQ5+soU/W0Cdr6JM19Mmz+NIaAAAAghpneAEAABDUCLwAAAAIagReAAAABDUCLwAAAIIagddHZs2apTZt2igmJkb9+vXThg0bqlx/8eLF6tSpk2JiYtS9e3ctXbrUR5XaY+bMmfrFL36h+vXrq2nTpho9erT27NlT5XvmzZsnh8NR5hETE+Ojiu0zffr0cp+7U6dOVb4n1I4nSWrTpk25PjkcDk2aNKnC9UPleFq9erWuu+46paSkyOFwaMmSJWVeNwxDjz76qJKTkxUbG6shQ4Zo79691W63pn/H+buq+uR0OvXII4+oe/fuqlevnlJSUjR+/HgdO3asym3W5mfX31V3PN11113lPvOIESOq3W6wHU9S9b2q6O8rh8Ohp59+utJtBuMx5S0EXh9YuHChpkyZomnTpmnTpk3q2bOnhg8fruPHj1e4/tq1azV27FhNmDBBmzdv1ujRozV69Gjt2LHDx5X7zhdffKFJkyZp/fr1SktLk9Pp1LBhw3T69Okq3xcfH6/09HT34+DBgz6q2F5du3Yt87nXrFlT6bqheDxJ0jfffFOmR2lpaZKkm2++udL3hMLxdPr0afXs2VOzZs2q8PWnnnpK//jHP/TSSy/p66+/Vr169TR8+HCdPXu20m3W9O+4QFBVn/Lz87Vp0yb9+c9/1qZNm/Tee+9pz549uv7666vdbk1+dgNBdceTJI0YMaLMZ37nnXeq3GYwHk9S9b0q3aP09HTNmTNHDodDY8aMqXK7wXZMeY0Br+vbt68xadIk9/OioiIjJSXFmDlzZoXr33LLLcY111xTZlm/fv2MX/3qV16t058cP37ckGR88cUXla4zd+5co0GDBr4ryk9MmzbN6Nmzp+X1OZ5MDz74oNG+fXujuLi4wtdD8XiSZLz//vvu58XFxUZSUpLx9NNPu5dlZ2cb0dHRxjvvvFPpdmr6d1ygOb9PFdmwYYMhyTh48GCl69T0ZzfQVNSnO++80xg1alSNthPsx5NhWDumRo0aZVx99dVVrhPsx5QncYbXywoLC/Xtt99qyJAh7mVhYWEaMmSI1q1bV+F71q1bV2Z9SRo+fHil6wejnJwcSVKjRo2qXC8vL0+tW7dWy5YtNWrUKO3cudMX5dlu7969SklJUbt27TRu3DgdOnSo0nU5nsyfw7feekv33HOPHA5HpeuF6vHksn//fmVkZJQ5Xho0aKB+/fpVerzU5u+4YJSTkyOHw6GEhIQq16vJz26wWLVqlZo2baqOHTvq/vvv18mTJytdl+PJlJmZqY8//lgTJkyodt1QPKZqg8DrZVlZWSoqKlKzZs3KLG/WrJkyMjIqfE9GRkaN1g82xcXF+u1vf6sBAwaoW7dula7XsWNHzZkzRx988IHeeustFRcX67LLLtORI0d8WK3v9evXT/PmzdOyZcs0e/Zs7d+/X1dccYVOnTpV4fqhfjxJ0pIlS5Sdna277rqr0nVC9XgqzXVM1OR4qc3fccHm7NmzeuSRRzR27FjFx8dXul5Nf3aDwYgRI/Tmm29q5cqV+tvf/qYvvvhCqampKioqqnB9jifTG2+8ofr16+vGG2+scr1QPKZqK8LuAoDzTZo0STt27Kh2HlL//v3Vv39/9/PLLrtMnTt31ssvv6zHH3/c22XaJjU11T3u0aOH+vXrp9atW2vRokWWzgaEotdff12pqalKSUmpdJ1QPZ5QN06nU7fccosMw9Ds2bOrXDcUf3Zvu+0297h79+7q0aOH2rdvr1WrVmnw4ME2Vubf5syZo3HjxlX7xdlQPKZqizO8XpaYmKjw8HBlZmaWWZ6ZmamkpKQK35OUlFSj9YPJ5MmT9dFHH+nzzz9XixYtavTeyMhI9e7dWz/88IOXqvNPCQkJ6tChQ6WfO5SPJ0k6ePCgVqxYoXvvvbdG7wvF48l1TNTkeKnN33HBwhV2Dx48qLS0tCrP7lakup/dYNSuXTslJiZW+plD+Xhy+fLLL7Vnz54a/50lheYxZRWB18uioqLUp08frVy50r2suLhYK1euLHM2qbT+/fuXWV+S0tLSKl0/GBiGocmTJ+v999/XZ599prZt29Z4G0VFRdq+fbuSk5O9UKH/ysvL0759+yr93KF4PJU2d+5cNW3aVNdcc02N3heKx1Pbtm2VlJRU5njJzc3V119/XenxUpu/44KBK+zu3btXK1asUOPGjWu8jep+doPRkSNHdPLkyUo/c6geT6W9/vrr6tOnj3r27Fnj94biMWWZ3d+aCwULFiwwoqOjjXnz5hnfffedMXHiRCMhIcHIyMgwDMMwfvnLXxp/+MMf3Ot/9dVXRkREhPHMM88Yu3btMqZNm2ZERkYa27dvt+sjeN39999vNGjQwFi1apWRnp7ufuTn57vXOb9PM2bMMD799FNj3759xrfffmvcdtttRkxMjLFz5047PoLP/O53vzNWrVpl7N+/3/jqq6+MIUOGGImJicbx48cNw+B4Kq2oqMho1aqV8cgjj5R7LVSPp1OnThmbN282Nm/ebEgynnvuOWPz5s3uqws8+eSTRkJCgvHBBx8Y27ZtM0aNGmW0bdvWOHPmjHsbV199tfHPf/7T/by6v+MCUVV9KiwsNK6//nqjRYsWxpYtW8r8nVVQUODexvl9qu5nNxBV1adTp04ZDz30kLFu3Tpj//79xooVK4yLL77YuOiii4yzZ8+6txEKx5NhVP+zZxiGkZOTY8TFxRmzZ8+ucBuhcEx5C4HXR/75z38arVq1MqKiooy+ffsa69evd782cOBA48477yyz/qJFi4wOHToYUVFRRteuXY2PP/7YxxX7lqQKH3PnznWvc36ffvvb37p72qxZM2PkyJHGpk2bfF+8j916661GcnKyERUVZTRv3ty49dZbjR9++MH9OsdTiU8//dSQZOzZs6fca6F6PH3++ecV/qy5elFcXGz8+c9/Npo1a2ZER0cbgwcPLte/1q1bG9OmTSuzrKq/4wJRVX3av39/pX9nff755+5tnN+n6n52A1FVfcrPzzeGDRtmNGnSxIiMjDRat25t3HfffeWCaygcT4ZR/c+eYRjGyy+/bMTGxhrZ2dkVbiMUjilvcRiGYXj1FDIAAABgI+bwAgAAIKgReAEAABDUCLwAAAAIagReAAAABDUCLwAAAIIagRcAAABBjcALAACAoEbgBQAAQFAj8AIAynA4HFqyZIndZQCAxxB4AcCP3HXXXXI4HOUeI0aMsLs0AAhYEXYXAAAoa8SIEZo7d26ZZdHR0TZVAwCBjzO8AOBnoqOjlZSUVObRsGFDSeZ0g9mzZys1NVWxsbFq166d3n333TLv3759u66++mrFxsaqcePGmjhxovLy8sqsM2fOHHXt2lXR0dFKTk7W5MmTy7yelZWlG264QXFxcbrooov04Ycful/76aefNG7cODVp0kSxsbG66KKLygV0APAnBF4ACDB//vOfNWbMGG3dulXjxo3Tbbfdpl27dkmSTp8+reHDh6thw4b65ptvtHjxYq1YsaJMoJ09e7YmTZqkiRMnavv27frwww914YUXltnHjBkzdMstt2jbtm0aOXKkxo0bpx9//NG9/++++06ffPKJdu3apdmzZysxMdF3DQCAGnIYhmHYXQQAwHTXXXfprbfeUkxMTJnlf/zjH/XHP/5RDodDv/71rzV79mz3a5deeqkuvvhi/etf/9Krr76qRx55RIcPH1a9evUkSUuXLtV1112nY8eOqVmzZmrevLnuvvtuPfHEExXW4HA49Kc//UmPP/64JDNEX3DBBfrkk080YsQIXX/99UpMTNScOXO81AUA8Czm8AKAn7nqqqvKBFpJatSokXvcv3//Mq/1799fW7ZskSTt2rVLPXv2dIddSRowYICKi4u1Z88eORwOHTt2TIMHD66yhh49erjH9erVU3x8vI4fPy5Juv/++zVmzBht2rRJw4YN0+jRo3XZZZfV6rMCgC8QeAHAz9SrV6/cFANPiY2NtbReZGRkmecOh0PFxcWSpNTUVB08eFBLly5VWlqaBg8erEmTJumZZ57xeL0A4AnM4QWAALN+/fpyzzt37ixJ6ty5s7Zu3arTp0+7X//qq68UFhamjh07qn79+mrTpo1WrlxZpxqaNGmiO++8U2+99ZZeeOEFvfLKK3XaHgB4E2d4AcDPFBQUKCMjo8yyiIgI9xfDFi9erEsuuUSXX3653n77bW3YsEGvv/66JGncuHGaNm2a7rzzTk2fPl0nTpzQAw88oF/+8pdq1qyZJGn69On69a9/raZNmyo1NVWnTp3SV199pQceeMBSfY8++qj69Omjrl27qqCgQB999JE7cAOAPyLwAoCfWbZsmZKTk8ss69ixo3bv3i3JvILCggUL9Jvf/EbJycl655131KVLF0lSXFycPv30Uz344IP6xS9+obi4OI0ZM0bPPfece1t33nmnzp49q+eff14PPfSQEhMTddNNN1muLyoqSlOnTtWBAwcUGxurK664QgsWLPDAJwcA7+AqDQAQQBwOh95//32NHj3a7lIAIGAwhxcAAABBjcALAACAoMYcXgAIIMxCA4Ca4wwvAAAAghqBFwAAAEGNwAsAAICgRuAFAABAUCPwAgAAIKgReAEAABDUCLwAAAAIagReAAAABLX/D4WoUAKvYbJ2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_values = model.loss_curve_\n",
    "\n",
    "# Crear la gráfica de Loss vs. Epochs\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(loss_values, color='blue', linewidth=2, label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
