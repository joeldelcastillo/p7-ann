{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abacofs import ABACOFeatureSelector\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.294244  0.705043  0.183047  0.610883  0.287570  0.806475  0.179009   \n",
      "1  0.315761  0.143673  0.263651  0.559548  0.330241  0.630611  0.174344   \n",
      "2  0.586875  0.397716  0.413937  0.667351  0.285714  0.785136  0.078134   \n",
      "3  0.481442  0.255947  0.506500  0.377823  0.270872  0.506990  0.232653   \n",
      "4  0.603012  0.574691  0.433177  0.394251  0.319109  0.431199  0.274052   \n",
      "\n",
      "          7         8         9  ...        61        62        63        64  \\\n",
      "0  0.499009  0.890763  0.184259  ...  0.378264  0.459459  0.300824  0.339323   \n",
      "1  0.522299  0.433735  0.462037  ...  0.606916  0.529530  0.333791  0.389081   \n",
      "2  0.570367  0.506827  0.612037  ...  0.448836  0.623624  0.515110  0.374568   \n",
      "3  0.484143  0.623293  0.378704  ...  0.576570  0.377377  0.220467  0.291638   \n",
      "4  0.471754  0.575100  0.686111  ...  0.486944  0.438438  0.695055  0.379406   \n",
      "\n",
      "         65        66        67        68        69  label  \n",
      "0  0.180758  0.348815  0.324151  0.526825  0.128294    0.0  \n",
      "1  0.332945  0.448341  0.433056  0.543741  0.221914    0.0  \n",
      "2  0.360933  0.509005  0.315823  0.435476  0.147712    0.0  \n",
      "3  0.143440  0.403791  0.410634  0.517641  0.126907    0.0  \n",
      "4  0.456560  0.371564  0.553491  0.523925  0.415395    1.0  \n",
      "\n",
      "[5 rows x 71 columns]\n"
     ]
    }
   ],
   "source": [
    "# ACO CALL\n",
    "fs = ABACOFeatureSelector(dtype='csv', data_training_name='./rtfDataSet.csv',\n",
    "                          numberAnts=20, iterations=30, n_features=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colony 0 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [50, 28, 63, 10, 57, 49, 1, 27, 44, 37, 43, 65, 53, 60, 46]\n",
      "\t\tAccuracy: 0.751186440677966\n",
      "\tAnt 1 :\n",
      "\t\tPath: [52, 18, 63, 58, 38, 56, 21, 10, 14, 46, 43, 69, 59, 53, 12]\n",
      "\t\tAccuracy: 0.6936158192090395\n",
      "\tAnt 2 :\n",
      "\t\tPath: [30, 52, 8, 65, 42, 48, 43, 15, 12, 14, 50, 51, 34, 49, 60]\n",
      "\t\tAccuracy: 0.7072316384180791\n",
      "\tAnt 3 :\n",
      "\t\tPath: [19, 54, 65, 12, 15, 34, 25, 59, 38, 57, 18, 42, 20, 14, 44]\n",
      "\t\tAccuracy: 0.7000564971751413\n",
      "\tAnt 4 :\n",
      "\t\tPath: [42, 48, 54, 44, 53, 32, 49, 16, 57, 0, 27, 11, 6, 63, 59]\n",
      "\t\tAccuracy: 0.7137853107344632\n",
      "\tAnt 5 :\n",
      "\t\tPath: [59, 15, 19, 63, 25, 8, 37, 10, 14, 46, 28, 11, 65, 67, 33]\n",
      "\t\tAccuracy: 0.7040112994350282\n",
      "\tAnt 6 :\n",
      "\t\tPath: [57, 54, 59, 52, 8, 19, 14, 37, 11, 49, 38, 65, 10, 63, 12]\n",
      "\t\tAccuracy: 0.7072316384180791\n",
      "\tAnt 7 :\n",
      "\t\tPath: [18, 40, 11, 57, 38, 58, 0, 9, 28, 37, 12, 41, 25, 50, 29]\n",
      "\t\tAccuracy: 0.6969491525423729\n",
      "\tAnt 8 :\n",
      "\t\tPath: [11, 63, 9, 25, 38, 31, 67, 57, 43, 27, 12, 14, 34, 15, 8]\n",
      "\t\tAccuracy: 0.649774011299435\n",
      "\tAnt 9 :\n",
      "\t\tPath: [37, 63, 56, 59, 15, 25, 58, 22, 5, 10, 50, 41, 53, 43, 52]\n",
      "\t\tAccuracy: 0.7205649717514124\n",
      "\tAnt 10 :\n",
      "\t\tPath: [19, 59, 8, 38, 42, 6, 50, 54, 52, 57, 64, 27, 49, 28, 63]\n",
      "\t\tAccuracy: 0.7005084745762712\n",
      "\tAnt 11 :\n",
      "\t\tPath: [48, 47, 25, 30, 57, 65, 20, 14, 43, 44, 32, 56, 60, 63, 50]\n",
      "\t\tAccuracy: 0.6901129943502824\n",
      "\tAnt 12 :\n",
      "\t\tPath: [57, 67, 10, 60, 59, 19, 2, 54, 68, 43, 12, 25, 18, 47, 58]\n",
      "\t\tAccuracy: 0.727457627118644\n",
      "\tAnt 13 :\n",
      "\t\tPath: [43, 56, 9, 11, 47, 52, 30, 57, 59, 63, 44, 51, 24, 2, 34]\n",
      "\t\tAccuracy: 0.703502824858757\n",
      "\tAnt 14 :\n",
      "\t\tPath: [10, 63, 30, 49, 15, 5, 8, 19, 51, 38, 59, 9, 56, 67, 14]\n",
      "\t\tAccuracy: 0.7068926553672317\n",
      "\tAnt 15 :\n",
      "\t\tPath: [14, 67, 54, 47, 10, 65, 15, 21, 63, 57, 48, 32, 38, 36, 45]\n",
      "\t\tAccuracy: 0.6634463276836158\n",
      "\tAnt 16 :\n",
      "\t\tPath: [18, 9, 63, 11, 51, 43, 30, 44, 57, 67, 53, 40, 29, 14, 38]\n",
      "\t\tAccuracy: 0.6664971751412431\n",
      "\tAnt 17 :\n",
      "\t\tPath: [46, 63, 18, 57, 68, 44, 11, 59, 67, 29, 14, 25, 8, 40, 20]\n",
      "\t\tAccuracy: 0.7036158192090395\n",
      "\tAnt 18 :\n",
      "\t\tPath: [59, 2, 44, 14, 57, 9, 34, 10, 65, 18, 60, 7, 68, 22, 30]\n",
      "\t\tAccuracy: 0.7444632768361582\n",
      "\tAnt 19 :\n",
      "\t\tPath: [18, 14, 2, 10, 36, 31, 24, 51, 42, 37, 11, 15, 9, 12, 53]\n",
      "\t\tAccuracy: 0.7173446327683616\n",
      "Colony 1 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [58, 38, 67, 11, 9, 59, 2, 24, 63, 43, 14, 29, 16, 30, 65]\n",
      "\t\tAccuracy: 0.733954802259887\n",
      "\tAnt 1 :\n",
      "\t\tPath: [52, 63, 42, 28, 57, 27, 8, 53, 50, 68, 14, 43, 20, 51, 46]\n",
      "\t\tAccuracy: 0.7203954802259886\n",
      "\tAnt 2 :\n",
      "\t\tPath: [25, 9, 12, 43, 22, 42, 10, 57, 27, 53, 59, 19, 46, 38, 2]\n",
      "\t\tAccuracy: 0.7101694915254237\n",
      "\tAnt 3 :\n",
      "\t\tPath: [22, 19, 53, 41, 57, 43, 49, 30, 50, 65, 2, 20, 14, 44, 15]\n",
      "\t\tAccuracy: 0.687005649717514\n",
      "\tAnt 4 :\n",
      "\t\tPath: [57, 41, 34, 5, 10, 42, 44, 18, 38, 53, 9, 29, 22, 0, 64]\n",
      "\t\tAccuracy: 0.7172881355932204\n",
      "\tAnt 5 :\n",
      "\t\tPath: [65, 27, 59, 22, 9, 38, 29, 57, 36, 33, 19, 44, 12, 7, 43]\n",
      "\t\tAccuracy: 0.7071751412429379\n",
      "\tAnt 6 :\n",
      "\t\tPath: [40, 49, 47, 25, 57, 42, 38, 46, 33, 43, 12, 14, 15, 37, 51]\n",
      "\t\tAccuracy: 0.7306779661016949\n",
      "\tAnt 7 :\n",
      "\t\tPath: [67, 65, 8, 57, 6, 10, 50, 51, 11, 63, 12, 42, 59, 28, 18]\n",
      "\t\tAccuracy: 0.6632203389830509\n",
      "\tAnt 8 :\n",
      "\t\tPath: [33, 11, 56, 7, 2, 57, 60, 59, 63, 15, 14, 42, 67, 37, 44]\n",
      "\t\tAccuracy: 0.7204519774011299\n",
      "\tAnt 9 :\n",
      "\t\tPath: [43, 57, 59, 49, 42, 12, 67, 65, 29, 9, 4, 60, 38, 10, 14]\n",
      "\t\tAccuracy: 0.7306214689265538\n",
      "\tAnt 10 :\n",
      "\t\tPath: [63, 6, 56, 57, 8, 2, 14, 42, 67, 48, 28, 12, 38, 33, 23]\n",
      "\t\tAccuracy: 0.6699435028248588\n",
      "\tAnt 11 :\n",
      "\t\tPath: [50, 9, 52, 37, 10, 68, 14, 41, 12, 7, 45, 57, 42, 16, 22]\n",
      "\t\tAccuracy: 0.6933333333333334\n",
      "\tAnt 12 :\n",
      "\t\tPath: [38, 59, 44, 27, 6, 52, 65, 43, 49, 60, 30, 9, 18, 10, 37]\n",
      "\t\tAccuracy: 0.7244632768361582\n",
      "\tAnt 13 :\n",
      "\t\tPath: [50, 19, 43, 10, 14, 38, 27, 29, 57, 23, 12, 5, 48, 59, 33]\n",
      "\t\tAccuracy: 0.713502824858757\n",
      "\tAnt 14 :\n",
      "\t\tPath: [5, 67, 54, 40, 37, 48, 52, 38, 46, 25, 14, 16, 50, 41, 10]\n",
      "\t\tAccuracy: 0.7240677966101695\n",
      "\tAnt 15 :\n",
      "\t\tPath: [63, 67, 12, 6, 53, 5, 18, 14, 49, 64, 19, 37, 44, 43, 40]\n",
      "\t\tAccuracy: 0.6803389830508475\n",
      "\tAnt 16 :\n",
      "\t\tPath: [57, 63, 65, 38, 28, 16, 8, 47, 43, 30, 29, 45, 36, 50, 53]\n",
      "\t\tAccuracy: 0.6868361581920903\n",
      "\tAnt 17 :\n",
      "\t\tPath: [19, 14, 6, 44, 63, 58, 33, 8, 38, 9, 59, 57, 16, 23, 52]\n",
      "\t\tAccuracy: 0.7036723163841807\n",
      "\tAnt 18 :\n",
      "\t\tPath: [58, 14, 57, 12, 9, 19, 65, 11, 63, 0, 10, 6, 44, 15, 67]\n",
      "\t\tAccuracy: 0.6833898305084746\n",
      "\tAnt 19 :\n",
      "\t\tPath: [67, 15, 63, 40, 57, 38, 49, 10, 64, 18, 34, 13, 23, 44, 59]\n",
      "\t\tAccuracy: 0.6803954802259886\n",
      "Colony 2 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [50, 48, 59, 65, 9, 29, 22, 3, 64, 38, 54, 57, 63, 14, 6]\n",
      "\t\tAccuracy: 0.6767231638418079\n",
      "\tAnt 1 :\n",
      "\t\tPath: [31, 53, 57, 10, 67, 37, 49, 12, 63, 5, 50, 56, 44, 28, 14]\n",
      "\t\tAccuracy: 0.7305084745762712\n",
      "\tAnt 2 :\n",
      "\t\tPath: [22, 57, 2, 9, 50, 30, 48, 51, 63, 59, 43, 14, 38, 65, 31]\n",
      "\t\tAccuracy: 0.7005084745762712\n",
      "\tAnt 3 :\n",
      "\t\tPath: [9, 60, 44, 30, 38, 52, 27, 21, 2, 11, 57, 24, 19, 58, 49]\n",
      "\t\tAccuracy: 0.7240677966101695\n",
      "\tAnt 4 :\n",
      "\t\tPath: [42, 0, 13, 57, 7, 46, 60, 37, 9, 54, 2, 63, 44, 15, 23]\n",
      "\t\tAccuracy: 0.7343502824858759\n",
      "\tAnt 5 :\n",
      "\t\tPath: [43, 42, 30, 10, 14, 52, 29, 50, 15, 60, 24, 53, 19, 38, 49]\n",
      "\t\tAccuracy: 0.7070621468926553\n",
      "\tAnt 6 :\n",
      "\t\tPath: [31, 12, 29, 43, 21, 57, 2, 56, 33, 18, 65, 38, 11, 34, 42]\n",
      "\t\tAccuracy: 0.71045197740113\n",
      "\tAnt 7 :\n",
      "\t\tPath: [9, 23, 67, 25, 40, 63, 22, 12, 30, 59, 16, 57, 24, 52, 33]\n",
      "\t\tAccuracy: 0.70045197740113\n",
      "\tAnt 8 :\n",
      "\t\tPath: [36, 59, 12, 49, 25, 38, 19, 42, 52, 10, 33, 51, 57, 9, 63]\n",
      "\t\tAccuracy: 0.7105649717514124\n",
      "\tAnt 9 :\n",
      "\t\tPath: [66, 9, 52, 38, 63, 50, 21, 47, 41, 27, 19, 5, 67, 46, 20]\n",
      "\t\tAccuracy: 0.6698305084745763\n",
      "\tAnt 10 :\n",
      "\t\tPath: [67, 63, 19, 64, 57, 60, 33, 10, 9, 12, 18, 51, 31, 50, 52]\n",
      "\t\tAccuracy: 0.6971751412429378\n",
      "\tAnt 11 :\n",
      "\t\tPath: [14, 4, 44, 68, 59, 18, 57, 31, 53, 5, 9, 13, 38, 49, 51]\n",
      "\t\tAccuracy: 0.6868361581920904\n",
      "\tAnt 12 :\n",
      "\t\tPath: [46, 38, 54, 50, 42, 51, 44, 10, 40, 57, 63, 12, 34, 24, 30]\n",
      "\t\tAccuracy: 0.7136158192090395\n",
      "\tAnt 13 :\n",
      "\t\tPath: [51, 65, 46, 33, 30, 53, 28, 63, 41, 49, 42, 48, 11, 13, 24]\n",
      "\t\tAccuracy: 0.6366666666666667\n",
      "\tAnt 14 :\n",
      "\t\tPath: [51, 49, 44, 38, 63, 46, 22, 54, 28, 56, 37, 50, 18, 19, 52]\n",
      "\t\tAccuracy: 0.7138418079096045\n",
      "\tAnt 15 :\n",
      "\t\tPath: [20, 12, 28, 44, 25, 37, 43, 16, 6, 9, 57, 23, 14, 56, 54]\n",
      "\t\tAccuracy: 0.71045197740113\n",
      "\tAnt 16 :\n",
      "\t\tPath: [51, 12, 19, 59, 9, 65, 27, 22, 57, 28, 38, 60, 52, 48, 14]\n",
      "\t\tAccuracy: 0.7070621468926552\n",
      "\tAnt 17 :\n",
      "\t\tPath: [50, 15, 38, 58, 48, 30, 59, 31, 14, 44, 10, 40, 18, 51, 54]\n",
      "\t\tAccuracy: 0.7170621468926553\n",
      "\tAnt 18 :\n",
      "\t\tPath: [65, 50, 6, 11, 13, 63, 36, 2, 59, 58, 38, 10, 67, 43, 18]\n",
      "\t\tAccuracy: 0.703728813559322\n",
      "\tAnt 19 :\n",
      "\t\tPath: [56, 51, 11, 34, 68, 53, 18, 9, 15, 57, 54, 52, 58, 38, 13]\n",
      "\t\tAccuracy: 0.6702259887005649\n",
      "Colony 3 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [40, 58, 59, 67, 9, 6, 36, 22, 37, 44, 38, 48, 12, 52, 2]\n",
      "\t\tAccuracy: 0.7106214689265536\n",
      "\tAnt 1 :\n",
      "\t\tPath: [49, 67, 10, 33, 8, 43, 9, 59, 14, 2, 63, 65, 6, 57, 38]\n",
      "\t\tAccuracy: 0.7306214689265536\n",
      "\tAnt 2 :\n",
      "\t\tPath: [44, 54, 11, 12, 58, 9, 38, 21, 43, 22, 59, 25, 50, 2, 18]\n",
      "\t\tAccuracy: 0.7474576271186442\n",
      "\tAnt 3 :\n",
      "\t\tPath: [30, 18, 16, 65, 27, 38, 11, 15, 23, 52, 37, 9, 10, 49, 53]\n",
      "\t\tAccuracy: 0.7108474576271187\n",
      "\tAnt 4 :\n",
      "\t\tPath: [28, 5, 50, 52, 8, 18, 59, 57, 14, 49, 67, 58, 53, 25, 38]\n",
      "\t\tAccuracy: 0.7040112994350283\n",
      "\tAnt 5 :\n",
      "\t\tPath: [37, 60, 10, 51, 36, 12, 63, 40, 59, 9, 11, 25, 49, 6, 15]\n",
      "\t\tAccuracy: 0.7138983050847457\n",
      "\tAnt 6 :\n",
      "\t\tPath: [45, 23, 28, 40, 20, 44, 49, 12, 13, 8, 59, 9, 54, 14, 52]\n",
      "\t\tAccuracy: 0.6835593220338984\n",
      "\tAnt 7 :\n",
      "\t\tPath: [12, 63, 44, 65, 10, 52, 46, 15, 9, 43, 38, 27, 16, 53, 22]\n",
      "\t\tAccuracy: 0.6768361581920905\n",
      "\tAnt 8 :\n",
      "\t\tPath: [30, 14, 38, 63, 8, 33, 59, 44, 27, 58, 10, 51, 25, 43, 21]\n",
      "\t\tAccuracy: 0.6903389830508474\n",
      "\tAnt 9 :\n",
      "\t\tPath: [22, 38, 52, 9, 59, 2, 40, 16, 33, 25, 23, 65, 67, 10, 53]\n",
      "\t\tAccuracy: 0.6532768361581921\n",
      "\tAnt 10 :\n",
      "\t\tPath: [42, 12, 28, 22, 51, 65, 49, 30, 50, 27, 19, 8, 52, 18, 56]\n",
      "\t\tAccuracy: 0.6628813559322034\n",
      "\tAnt 11 :\n",
      "\t\tPath: [10, 28, 49, 51, 43, 30, 8, 57, 21, 11, 9, 50, 46, 52, 47]\n",
      "\t\tAccuracy: 0.7340677966101694\n",
      "\tAnt 12 :\n",
      "\t\tPath: [50, 49, 38, 59, 60, 37, 52, 9, 13, 51, 19, 10, 30, 6, 11]\n",
      "\t\tAccuracy: 0.7105649717514124\n",
      "\tAnt 13 :\n",
      "\t\tPath: [14, 65, 9, 15, 43, 18, 59, 57, 6, 11, 33, 44, 37, 25, 34]\n",
      "\t\tAccuracy: 0.7002259887005648\n",
      "\tAnt 14 :\n",
      "\t\tPath: [33, 65, 43, 28, 12, 2, 36, 0, 63, 9, 15, 67, 52, 22, 46]\n",
      "\t\tAccuracy: 0.7203954802259886\n",
      "\tAnt 15 :\n",
      "\t\tPath: [43, 9, 38, 37, 6, 14, 57, 10, 63, 19, 33, 21, 8, 44, 42]\n",
      "\t\tAccuracy: 0.7138983050847457\n",
      "\tAnt 16 :\n",
      "\t\tPath: [8, 12, 19, 42, 6, 38, 27, 47, 65, 51, 50, 21, 23, 56, 54]\n",
      "\t\tAccuracy: 0.6698305084745763\n",
      "\tAnt 17 :\n",
      "\t\tPath: [30, 52, 63, 12, 28, 11, 25, 58, 2, 9, 15, 59, 19, 18, 14]\n",
      "\t\tAccuracy: 0.6868926553672317\n",
      "\tAnt 18 :\n",
      "\t\tPath: [65, 40, 31, 14, 53, 68, 36, 9, 52, 50, 5, 67, 8, 57, 30]\n",
      "\t\tAccuracy: 0.6396610169491526\n",
      "\tAnt 19 :\n",
      "\t\tPath: [30, 46, 11, 51, 25, 52, 65, 18, 48, 37, 8, 43, 67, 59, 49]\n",
      "\t\tAccuracy: 0.7274011299435028\n",
      "Colony 4 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [50, 41, 31, 65, 10, 14, 44, 19, 33, 9, 11, 38, 37, 58, 53]\n",
      "\t\tAccuracy: 0.6766666666666665\n",
      "\tAnt 1 :\n",
      "\t\tPath: [56, 57, 50, 19, 30, 65, 54, 49, 8, 10, 18, 48, 68, 53, 40]\n",
      "\t\tAccuracy: 0.6999435028248587\n",
      "\tAnt 2 :\n",
      "\t\tPath: [2, 13, 57, 45, 10, 23, 15, 9, 11, 52, 59, 63, 14, 33, 54]\n",
      "\t\tAccuracy: 0.680225988700565\n",
      "\tAnt 3 :\n",
      "\t\tPath: [34, 52, 68, 60, 30, 25, 65, 48, 15, 63, 10, 67, 38, 40, 11]\n",
      "\t\tAccuracy: 0.6971186440677967\n",
      "\tAnt 4 :\n",
      "\t\tPath: [57, 63, 67, 27, 14, 50, 49, 23, 19, 58, 51, 6, 29, 34, 38]\n",
      "\t\tAccuracy: 0.703502824858757\n",
      "\tAnt 5 :\n",
      "\t\tPath: [21, 54, 49, 67, 38, 63, 30, 6, 12, 57, 47, 65, 11, 19, 33]\n",
      "\t\tAccuracy: 0.7069491525423729\n",
      "\tAnt 6 :\n",
      "\t\tPath: [9, 23, 14, 63, 25, 38, 36, 2, 48, 30, 18, 67, 54, 43, 15]\n",
      "\t\tAccuracy: 0.6935028248587571\n",
      "\tAnt 7 :\n",
      "\t\tPath: [19, 34, 53, 44, 65, 38, 5, 58, 50, 47, 49, 67, 24, 42, 51]\n",
      "\t\tAccuracy: 0.6970621468926553\n",
      "\tAnt 8 :\n",
      "\t\tPath: [9, 5, 16, 43, 7, 27, 11, 12, 59, 53, 63, 36, 46, 19, 57]\n",
      "\t\tAccuracy: 0.69\n",
      "\tAnt 9 :\n",
      "\t\tPath: [53, 19, 65, 43, 38, 57, 50, 14, 30, 63, 59, 44, 37, 23, 9]\n",
      "\t\tAccuracy: 0.7272881355932203\n",
      "\tAnt 10 :\n",
      "\t\tPath: [54, 11, 19, 59, 58, 67, 50, 65, 28, 33, 63, 51, 48, 57, 68]\n",
      "\t\tAccuracy: 0.6429943502824857\n",
      "\tAnt 11 :\n",
      "\t\tPath: [47, 8, 11, 36, 53, 60, 59, 6, 34, 29, 56, 50, 18, 67, 2]\n",
      "\t\tAccuracy: 0.7038983050847458\n",
      "\tAnt 12 :\n",
      "\t\tPath: [28, 21, 43, 14, 10, 51, 44, 53, 59, 63, 38, 25, 8, 18, 42]\n",
      "\t\tAccuracy: 0.686949152542373\n",
      "\tAnt 13 :\n",
      "\t\tPath: [63, 36, 53, 14, 9, 30, 18, 57, 23, 29, 48, 51, 12, 44, 50]\n",
      "\t\tAccuracy: 0.6800564971751413\n",
      "\tAnt 14 :\n",
      "\t\tPath: [30, 51, 52, 40, 36, 7, 29, 67, 14, 43, 9, 65, 16, 38, 59]\n",
      "\t\tAccuracy: 0.6799999999999999\n",
      "\tAnt 15 :\n",
      "\t\tPath: [44, 63, 1, 41, 56, 43, 34, 22, 42, 49, 15, 25, 48, 8, 18]\n",
      "\t\tAccuracy: 0.6903389830508475\n",
      "\tAnt 16 :\n",
      "\t\tPath: [44, 47, 27, 16, 30, 59, 63, 9, 53, 38, 24, 57, 15, 32, 43]\n",
      "\t\tAccuracy: 0.717231638418079\n",
      "\tAnt 17 :\n",
      "\t\tPath: [13, 30, 59, 44, 19, 43, 46, 14, 18, 50, 49, 15, 10, 48, 57]\n",
      "\t\tAccuracy: 0.7205084745762711\n",
      "\tAnt 18 :\n",
      "\t\tPath: [12, 67, 3, 63, 57, 2, 8, 65, 49, 14, 27, 43, 51, 50, 38]\n",
      "\t\tAccuracy: 0.693502824858757\n",
      "\tAnt 19 :\n",
      "\t\tPath: [15, 40, 54, 53, 57, 50, 49, 10, 44, 21, 48, 65, 11, 28, 29]\n",
      "\t\tAccuracy: 0.6701129943502824\n",
      "Colony 5 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [64, 11, 18, 25, 65, 14, 8, 30, 2, 44, 21, 42, 59, 5, 38]\n",
      "\t\tAccuracy: 0.6971186440677967\n",
      "\tAnt 1 :\n",
      "\t\tPath: [24, 18, 37, 60, 12, 57, 27, 6, 9, 48, 5, 34, 38, 0, 30]\n",
      "\t\tAccuracy: 0.7372881355932204\n",
      "\tAnt 2 :\n",
      "\t\tPath: [15, 27, 57, 43, 29, 9, 48, 37, 25, 20, 54, 41, 67, 16, 18]\n",
      "\t\tAccuracy: 0.703954802259887\n",
      "\tAnt 3 :\n",
      "\t\tPath: [59, 37, 36, 14, 32, 44, 38, 18, 50, 42, 63, 29, 65, 41, 27]\n",
      "\t\tAccuracy: 0.7103389830508475\n",
      "\tAnt 4 :\n",
      "\t\tPath: [27, 49, 10, 51, 52, 42, 22, 2, 38, 12, 28, 67, 50, 44, 65]\n",
      "\t\tAccuracy: 0.7005084745762712\n",
      "\tAnt 5 :\n",
      "\t\tPath: [15, 21, 46, 58, 20, 38, 56, 52, 30, 10, 67, 60, 43, 44, 18]\n",
      "\t\tAccuracy: 0.6766666666666666\n",
      "\tAnt 6 :\n",
      "\t\tPath: [2, 34, 38, 9, 11, 42, 50, 49, 59, 65, 14, 33, 63, 54, 16]\n",
      "\t\tAccuracy: 0.6833898305084746\n",
      "\tAnt 7 :\n",
      "\t\tPath: [14, 34, 43, 21, 63, 9, 38, 51, 44, 48, 18, 15, 67, 12, 11]\n",
      "\t\tAccuracy: 0.6599999999999999\n",
      "\tAnt 8 :\n",
      "\t\tPath: [67, 11, 16, 45, 49, 48, 50, 52, 19, 59, 9, 12, 28, 30, 38]\n",
      "\t\tAccuracy: 0.717231638418079\n",
      "\tAnt 9 :\n",
      "\t\tPath: [51, 25, 19, 44, 65, 60, 10, 48, 58, 18, 52, 59, 11, 14, 29]\n",
      "\t\tAccuracy: 0.7171751412429378\n",
      "\tAnt 10 :\n",
      "\t\tPath: [58, 63, 10, 34, 11, 57, 52, 28, 48, 6, 67, 8, 51, 1, 43]\n",
      "\t\tAccuracy: 0.6936723163841808\n",
      "\tAnt 11 :\n",
      "\t\tPath: [60, 12, 14, 67, 59, 21, 52, 49, 34, 36, 30, 38, 48, 46, 9]\n",
      "\t\tAccuracy: 0.7307909604519774\n",
      "\tAnt 12 :\n",
      "\t\tPath: [6, 38, 14, 23, 31, 30, 50, 15, 58, 57, 49, 19, 18, 25, 29]\n",
      "\t\tAccuracy: 0.7135593220338984\n",
      "\tAnt 13 :\n",
      "\t\tPath: [38, 15, 44, 2, 11, 14, 9, 49, 59, 10, 52, 37, 56, 12, 40]\n",
      "\t\tAccuracy: 0.7003954802259886\n",
      "\tAnt 14 :\n",
      "\t\tPath: [51, 44, 21, 38, 30, 43, 14, 15, 2, 33, 50, 9, 27, 59, 42]\n",
      "\t\tAccuracy: 0.6871186440677965\n",
      "\tAnt 15 :\n",
      "\t\tPath: [41, 19, 63, 36, 14, 12, 15, 37, 52, 65, 2, 46, 0, 47, 59]\n",
      "\t\tAccuracy: 0.6934463276836158\n",
      "\tAnt 16 :\n",
      "\t\tPath: [60, 59, 57, 14, 2, 38, 44, 50, 6, 43, 22, 9, 49, 5, 10]\n",
      "\t\tAccuracy: 0.7712429378531073\n",
      "\tAnt 17 :\n",
      "\t\tPath: [9, 57, 22, 18, 59, 28, 65, 63, 44, 20, 43, 36, 19, 12, 41]\n",
      "\t\tAccuracy: 0.6867231638418079\n",
      "\tAnt 18 :\n",
      "\t\tPath: [38, 63, 36, 44, 12, 43, 65, 56, 45, 20, 29, 14, 9, 40, 46]\n",
      "\t\tAccuracy: 0.6631638418079097\n",
      "\tAnt 19 :\n",
      "\t\tPath: [14, 38, 40, 9, 67, 33, 28, 57, 8, 30, 43, 63, 53, 68, 22]\n",
      "\t\tAccuracy: 0.6833333333333333\n",
      "Colony 6 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [49, 11, 14, 50, 0, 57, 12, 51, 43, 63, 30, 37, 19, 29, 18]\n",
      "\t\tAccuracy: 0.6936158192090395\n",
      "\tAnt 1 :\n",
      "\t\tPath: [32, 57, 59, 31, 14, 38, 58, 49, 47, 53, 8, 9, 50, 12, 67]\n",
      "\t\tAccuracy: 0.6967796610169491\n",
      "\tAnt 2 :\n",
      "\t\tPath: [10, 44, 14, 30, 11, 63, 9, 60, 41, 57, 65, 34, 25, 37, 51]\n",
      "\t\tAccuracy: 0.6768926553672316\n",
      "\tAnt 3 :\n",
      "\t\tPath: [52, 37, 63, 6, 36, 12, 53, 21, 33, 65, 25, 57, 16, 48, 41]\n",
      "\t\tAccuracy: 0.7069491525423729\n",
      "\tAnt 4 :\n",
      "\t\tPath: [59, 30, 51, 57, 38, 18, 44, 22, 25, 63, 9, 10, 52, 27, 12]\n",
      "\t\tAccuracy: 0.7170621468926553\n",
      "\tAnt 5 :\n",
      "\t\tPath: [65, 60, 8, 51, 44, 58, 57, 43, 2, 10, 9, 14, 67, 20, 12]\n",
      "\t\tAccuracy: 0.7037853107344633\n",
      "\tAnt 6 :\n",
      "\t\tPath: [59, 9, 21, 38, 12, 14, 30, 65, 19, 53, 67, 18, 57, 25, 22]\n",
      "\t\tAccuracy: 0.7035593220338983\n",
      "\tAnt 7 :\n",
      "\t\tPath: [11, 60, 12, 9, 29, 6, 63, 22, 43, 18, 10, 57, 8, 38, 68]\n",
      "\t\tAccuracy: 0.7171751412429378\n",
      "\tAnt 8 :\n",
      "\t\tPath: [45, 44, 46, 37, 18, 58, 63, 64, 33, 2, 48, 10, 42, 59, 22]\n",
      "\t\tAccuracy: 0.6968361581920904\n",
      "\tAnt 9 :\n",
      "\t\tPath: [65, 18, 22, 48, 10, 19, 68, 43, 40, 13, 44, 37, 38, 50, 30]\n",
      "\t\tAccuracy: 0.7140112994350283\n",
      "\tAnt 10 :\n",
      "\t\tPath: [47, 14, 44, 12, 31, 57, 67, 9, 50, 19, 28, 45, 10, 30, 51]\n",
      "\t\tAccuracy: 0.6701129943502824\n",
      "\tAnt 11 :\n",
      "\t\tPath: [50, 63, 67, 10, 18, 33, 41, 8, 13, 9, 42, 38, 14, 52, 37]\n",
      "\t\tAccuracy: 0.6935593220338984\n",
      "\tAnt 12 :\n",
      "\t\tPath: [49, 33, 68, 59, 56, 18, 10, 19, 44, 58, 36, 51, 69, 47, 14]\n",
      "\t\tAccuracy: 0.6865536723163842\n",
      "\tAnt 13 :\n",
      "\t\tPath: [15, 44, 8, 65, 2, 57, 63, 6, 52, 11, 20, 59, 38, 67, 21]\n",
      "\t\tAccuracy: 0.7073446327683616\n",
      "\tAnt 14 :\n",
      "\t\tPath: [11, 0, 28, 37, 38, 7, 24, 49, 57, 14, 58, 59, 2, 6, 67]\n",
      "\t\tAccuracy: 0.7476836158192091\n",
      "\tAnt 15 :\n",
      "\t\tPath: [49, 9, 38, 37, 19, 67, 16, 31, 53, 63, 14, 59, 50, 46, 64]\n",
      "\t\tAccuracy: 0.71045197740113\n",
      "\tAnt 16 :\n",
      "\t\tPath: [37, 43, 42, 21, 20, 57, 27, 51, 31, 41, 36, 50, 23, 33, 10]\n",
      "\t\tAccuracy: 0.6868361581920903\n",
      "\tAnt 17 :\n",
      "\t\tPath: [2, 48, 68, 63, 7, 31, 59, 36, 37, 25, 52, 44, 28, 18, 67]\n",
      "\t\tAccuracy: 0.6971186440677966\n",
      "\tAnt 18 :\n",
      "\t\tPath: [14, 15, 43, 27, 52, 50, 67, 33, 12, 49, 30, 57, 28, 11, 63]\n",
      "\t\tAccuracy: 0.6905084745762711\n",
      "\tAnt 19 :\n",
      "\t\tPath: [48, 38, 46, 53, 12, 44, 52, 63, 31, 18, 51, 67, 65, 19, 25]\n",
      "\t\tAccuracy: 0.6768361581920904\n",
      "Colony 7 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [40, 37, 50, 63, 52, 33, 9, 38, 48, 67, 44, 14, 22, 19, 49]\n",
      "\t\tAccuracy: 0.6835028248587571\n",
      "\tAnt 1 :\n",
      "\t\tPath: [59, 10, 22, 14, 38, 50, 68, 1, 57, 46, 63, 9, 16, 44, 52]\n",
      "\t\tAccuracy: 0.7170621468926555\n",
      "\tAnt 2 :\n",
      "\t\tPath: [59, 9, 12, 49, 68, 46, 58, 57, 20, 38, 14, 5, 60, 0, 53]\n",
      "\t\tAccuracy: 0.7338418079096044\n",
      "\tAnt 3 :\n",
      "\t\tPath: [14, 67, 53, 37, 34, 29, 49, 44, 68, 18, 57, 50, 9, 27, 28]\n",
      "\t\tAccuracy: 0.7205649717514124\n",
      "\tAnt 4 :\n",
      "\t\tPath: [43, 6, 38, 51, 21, 57, 52, 34, 19, 49, 42, 24, 59, 63, 65]\n",
      "\t\tAccuracy: 0.7001694915254237\n",
      "\tAnt 5 :\n",
      "\t\tPath: [21, 65, 43, 50, 28, 67, 14, 49, 22, 38, 30, 13, 25, 56, 42]\n",
      "\t\tAccuracy: 0.7205649717514124\n",
      "\tAnt 6 :\n",
      "\t\tPath: [5, 2, 68, 42, 10, 50, 30, 57, 51, 12, 18, 58, 11, 38, 67]\n",
      "\t\tAccuracy: 0.7002824858757062\n",
      "\tAnt 7 :\n",
      "\t\tPath: [49, 67, 63, 10, 15, 58, 27, 38, 8, 60, 11, 9, 59, 25, 19]\n",
      "\t\tAccuracy: 0.6903954802259887\n",
      "\tAnt 8 :\n",
      "\t\tPath: [50, 31, 16, 15, 38, 43, 67, 2, 69, 57, 14, 11, 65, 59, 42]\n",
      "\t\tAccuracy: 0.7138983050847457\n",
      "\tAnt 9 :\n",
      "\t\tPath: [47, 31, 38, 65, 59, 12, 58, 14, 63, 57, 43, 20, 7, 64, 52]\n",
      "\t\tAccuracy: 0.7203389830508474\n",
      "\tAnt 10 :\n",
      "\t\tPath: [8, 52, 48, 12, 25, 38, 22, 53, 37, 15, 13, 65, 28, 30, 14]\n",
      "\t\tAccuracy: 0.6633333333333333\n",
      "\tAnt 11 :\n",
      "\t\tPath: [23, 29, 22, 12, 60, 38, 56, 31, 8, 18, 15, 14, 53, 25, 9]\n",
      "\t\tAccuracy: 0.6833898305084746\n",
      "\tAnt 12 :\n",
      "\t\tPath: [37, 14, 38, 16, 29, 6, 30, 9, 50, 51, 44, 67, 12, 53, 46]\n",
      "\t\tAccuracy: 0.6699435028248588\n",
      "\tAnt 13 :\n",
      "\t\tPath: [42, 25, 44, 48, 19, 45, 63, 18, 38, 59, 5, 33, 57, 49, 29]\n",
      "\t\tAccuracy: 0.740960451977401\n",
      "\tAnt 14 :\n",
      "\t\tPath: [58, 33, 9, 44, 19, 21, 16, 54, 12, 14, 11, 38, 65, 63, 49]\n",
      "\t\tAccuracy: 0.6460451977401129\n",
      "\tAnt 15 :\n",
      "\t\tPath: [59, 38, 21, 48, 14, 15, 2, 18, 68, 49, 16, 30, 43, 42, 44]\n",
      "\t\tAccuracy: 0.7068361581920903\n",
      "\tAnt 16 :\n",
      "\t\tPath: [16, 14, 69, 53, 12, 11, 6, 50, 18, 40, 46, 57, 67, 31, 63]\n",
      "\t\tAccuracy: 0.6664406779661017\n",
      "\tAnt 17 :\n",
      "\t\tPath: [9, 43, 37, 8, 33, 3, 18, 19, 68, 46, 63, 44, 22, 52, 30]\n",
      "\t\tAccuracy: 0.7002824858757062\n",
      "\tAnt 18 :\n",
      "\t\tPath: [58, 14, 57, 59, 67, 22, 10, 13, 27, 43, 65, 12, 38, 0, 18]\n",
      "\t\tAccuracy: 0.6868361581920903\n",
      "\tAnt 19 :\n",
      "\t\tPath: [11, 9, 53, 38, 59, 14, 18, 12, 40, 57, 28, 19, 15, 10, 43]\n",
      "\t\tAccuracy: 0.6703389830508474\n",
      "Colony 8 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [52, 56, 43, 49, 41, 54, 28, 67, 14, 10, 50, 63, 11, 16, 25]\n",
      "\t\tAccuracy: 0.6565536723163842\n",
      "\tAnt 1 :\n",
      "\t\tPath: [46, 63, 14, 38, 12, 54, 22, 36, 67, 25, 23, 65, 44, 27, 29]\n",
      "\t\tAccuracy: 0.6599435028248587\n",
      "\tAnt 2 :\n",
      "\t\tPath: [55, 44, 42, 10, 21, 14, 30, 11, 41, 59, 37, 1, 12, 25, 33]\n",
      "\t\tAccuracy: 0.7203389830508474\n",
      "\tAnt 3 :\n",
      "\t\tPath: [67, 27, 18, 9, 22, 59, 29, 63, 43, 54, 50, 10, 38, 52, 25]\n",
      "\t\tAccuracy: 0.7070056497175141\n",
      "\tAnt 4 :\n",
      "\t\tPath: [51, 29, 50, 18, 10, 43, 12, 19, 63, 15, 23, 33, 24, 65, 16]\n",
      "\t\tAccuracy: 0.6466101694915254\n",
      "\tAnt 5 :\n",
      "\t\tPath: [22, 19, 38, 14, 37, 63, 9, 51, 64, 33, 30, 59, 11, 24, 25]\n",
      "\t\tAccuracy: 0.7071751412429379\n",
      "\tAnt 6 :\n",
      "\t\tPath: [65, 32, 27, 67, 36, 2, 10, 9, 28, 56, 19, 15, 23, 50, 37]\n",
      "\t\tAccuracy: 0.6802259887005649\n",
      "\tAnt 7 :\n",
      "\t\tPath: [21, 51, 14, 36, 67, 50, 42, 13, 8, 18, 38, 63, 12, 52, 64]\n",
      "\t\tAccuracy: 0.683728813559322\n",
      "\tAnt 8 :\n",
      "\t\tPath: [49, 12, 13, 37, 18, 51, 20, 27, 10, 41, 30, 67, 59, 19, 9]\n",
      "\t\tAccuracy: 0.7240677966101694\n",
      "\tAnt 9 :\n",
      "\t\tPath: [51, 9, 63, 37, 22, 5, 50, 67, 43, 3, 11, 57, 0, 65, 40]\n",
      "\t\tAccuracy: 0.7205649717514124\n",
      "\tAnt 10 :\n",
      "\t\tPath: [51, 38, 65, 18, 59, 33, 9, 19, 67, 53, 25, 43, 50, 57, 49]\n",
      "\t\tAccuracy: 0.693728813559322\n",
      "\tAnt 11 :\n",
      "\t\tPath: [49, 2, 38, 65, 68, 23, 14, 64, 43, 44, 18, 20, 48, 12, 63]\n",
      "\t\tAccuracy: 0.7001129943502825\n",
      "\tAnt 12 :\n",
      "\t\tPath: [50, 30, 33, 38, 20, 14, 49, 42, 8, 37, 43, 57, 9, 6, 11]\n",
      "\t\tAccuracy: 0.7374576271186442\n",
      "\tAnt 13 :\n",
      "\t\tPath: [22, 16, 30, 27, 60, 29, 49, 67, 18, 11, 59, 10, 45, 38, 42]\n",
      "\t\tAccuracy: 0.7071186440677966\n",
      "\tAnt 14 :\n",
      "\t\tPath: [23, 43, 28, 38, 10, 44, 2, 18, 46, 63, 65, 67, 15, 59, 21]\n",
      "\t\tAccuracy: 0.7341242937853109\n",
      "\tAnt 15 :\n",
      "\t\tPath: [67, 37, 14, 13, 43, 50, 65, 16, 59, 36, 21, 42, 27, 10, 19]\n",
      "\t\tAccuracy: 0.683502824858757\n",
      "\tAnt 16 :\n",
      "\t\tPath: [16, 59, 10, 23, 65, 52, 44, 11, 2, 63, 57, 42, 29, 50, 51]\n",
      "\t\tAccuracy: 0.6529943502824859\n",
      "\tAnt 17 :\n",
      "\t\tPath: [11, 60, 20, 42, 65, 10, 57, 16, 22, 38, 14, 37, 27, 67, 8]\n",
      "\t\tAccuracy: 0.7205084745762712\n",
      "\tAnt 18 :\n",
      "\t\tPath: [69, 48, 37, 63, 38, 58, 36, 67, 52, 49, 42, 18, 12, 57, 50]\n",
      "\t\tAccuracy: 0.7171186440677967\n",
      "\tAnt 19 :\n",
      "\t\tPath: [42, 14, 46, 59, 12, 25, 8, 9, 11, 10, 63, 38, 19, 27, 48]\n",
      "\t\tAccuracy: 0.7103389830508474\n",
      "Colony 9 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [2, 11, 63, 65, 30, 19, 33, 12, 25, 40, 59, 53, 23, 68, 13]\n",
      "\t\tAccuracy: 0.7105084745762712\n",
      "\tAnt 1 :\n",
      "\t\tPath: [50, 9, 31, 58, 22, 67, 52, 57, 33, 21, 18, 37, 12, 51, 19]\n",
      "\t\tAccuracy: 0.7170056497175141\n",
      "\tAnt 2 :\n",
      "\t\tPath: [15, 67, 68, 37, 29, 42, 16, 65, 53, 33, 9, 63, 14, 44, 57]\n",
      "\t\tAccuracy: 0.7168926553672316\n",
      "\tAnt 3 :\n",
      "\t\tPath: [65, 12, 18, 38, 44, 16, 28, 63, 33, 60, 30, 42, 53, 59, 49]\n",
      "\t\tAccuracy: 0.7002824858757062\n",
      "\tAnt 4 :\n",
      "\t\tPath: [37, 59, 15, 42, 65, 27, 40, 12, 8, 44, 58, 30, 52, 14, 50]\n",
      "\t\tAccuracy: 0.6698870056497175\n",
      "\tAnt 5 :\n",
      "\t\tPath: [67, 64, 31, 43, 18, 15, 30, 13, 57, 38, 16, 59, 12, 21, 60]\n",
      "\t\tAccuracy: 0.7106779661016949\n",
      "\tAnt 6 :\n",
      "\t\tPath: [68, 10, 44, 35, 50, 65, 19, 45, 51, 34, 25, 29, 16, 14, 56]\n",
      "\t\tAccuracy: 0.6936158192090396\n",
      "\tAnt 7 :\n",
      "\t\tPath: [60, 48, 38, 31, 63, 59, 12, 67, 25, 30, 33, 53, 6, 14, 45]\n",
      "\t\tAccuracy: 0.7238983050847458\n",
      "\tAnt 8 :\n",
      "\t\tPath: [22, 13, 18, 45, 42, 67, 24, 38, 21, 63, 68, 12, 15, 11, 31]\n",
      "\t\tAccuracy: 0.6327683615819208\n",
      "\tAnt 9 :\n",
      "\t\tPath: [58, 43, 28, 33, 18, 50, 57, 19, 20, 60, 54, 52, 8, 15, 25]\n",
      "\t\tAccuracy: 0.7174011299435029\n",
      "\tAnt 10 :\n",
      "\t\tPath: [43, 59, 63, 27, 48, 25, 40, 9, 54, 37, 16, 57, 50, 38, 44]\n",
      "\t\tAccuracy: 0.6802824858757062\n",
      "\tAnt 11 :\n",
      "\t\tPath: [43, 49, 8, 65, 11, 37, 52, 18, 38, 63, 23, 9, 20, 58, 5]\n",
      "\t\tAccuracy: 0.713954802259887\n",
      "\tAnt 12 :\n",
      "\t\tPath: [52, 28, 49, 10, 34, 65, 60, 67, 68, 22, 57, 63, 51, 9, 50]\n",
      "\t\tAccuracy: 0.6665536723163841\n",
      "\tAnt 13 :\n",
      "\t\tPath: [44, 63, 60, 16, 12, 31, 57, 53, 67, 36, 5, 47, 37, 49, 50]\n",
      "\t\tAccuracy: 0.6868361581920903\n",
      "\tAnt 14 :\n",
      "\t\tPath: [51, 10, 68, 12, 27, 33, 14, 6, 8, 43, 15, 49, 67, 44, 11]\n",
      "\t\tAccuracy: 0.6870056497175141\n",
      "\tAnt 15 :\n",
      "\t\tPath: [43, 10, 53, 65, 52, 46, 57, 68, 23, 18, 50, 37, 56, 13, 25]\n",
      "\t\tAccuracy: 0.71045197740113\n",
      "\tAnt 16 :\n",
      "\t\tPath: [50, 46, 54, 16, 12, 9, 57, 19, 43, 18, 5, 49, 28, 25, 63]\n",
      "\t\tAccuracy: 0.713728813559322\n",
      "\tAnt 17 :\n",
      "\t\tPath: [15, 18, 40, 8, 58, 25, 30, 57, 53, 44, 63, 16, 12, 59, 19]\n",
      "\t\tAccuracy: 0.6867231638418078\n",
      "\tAnt 18 :\n",
      "\t\tPath: [49, 43, 12, 68, 27, 57, 10, 9, 11, 14, 44, 53, 67, 30, 63]\n",
      "\t\tAccuracy: 0.7070056497175141\n",
      "\tAnt 19 :\n",
      "\t\tPath: [50, 6, 48, 16, 25, 44, 11, 49, 2, 38, 59, 12, 57, 28, 14]\n",
      "\t\tAccuracy: 0.7067796610169491\n",
      "Colony 10 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [58, 22, 51, 13, 38, 63, 59, 16, 67, 37, 44, 19, 60, 14, 52]\n",
      "\t\tAccuracy: 0.7070621468926552\n",
      "\tAnt 1 :\n",
      "\t\tPath: [50, 56, 10, 9, 65, 37, 25, 57, 59, 19, 58, 38, 18, 27, 29]\n",
      "\t\tAccuracy: 0.6768361581920905\n",
      "\tAnt 2 :\n",
      "\t\tPath: [63, 44, 26, 54, 19, 6, 49, 43, 68, 25, 53, 2, 33, 21, 50]\n",
      "\t\tAccuracy: 0.7037853107344633\n",
      "\tAnt 3 :\n",
      "\t\tPath: [8, 63, 57, 67, 14, 10, 37, 60, 42, 59, 12, 43, 31, 25, 50]\n",
      "\t\tAccuracy: 0.7037853107344633\n",
      "\tAnt 4 :\n",
      "\t\tPath: [22, 50, 52, 9, 23, 59, 43, 63, 57, 58, 31, 12, 36, 27, 5]\n",
      "\t\tAccuracy: 0.71\n",
      "\tAnt 5 :\n",
      "\t\tPath: [29, 12, 59, 38, 11, 60, 8, 14, 54, 25, 52, 37, 43, 15, 68]\n",
      "\t\tAccuracy: 0.7340677966101694\n",
      "\tAnt 6 :\n",
      "\t\tPath: [21, 48, 9, 57, 28, 38, 30, 18, 31, 12, 63, 20, 34, 53, 40]\n",
      "\t\tAccuracy: 0.700225988700565\n",
      "\tAnt 7 :\n",
      "\t\tPath: [19, 38, 10, 14, 0, 44, 24, 60, 5, 57, 29, 67, 63, 8, 42]\n",
      "\t\tAccuracy: 0.7205084745762712\n",
      "\tAnt 8 :\n",
      "\t\tPath: [67, 40, 30, 18, 12, 10, 50, 51, 57, 14, 25, 59, 54, 63, 11]\n",
      "\t\tAccuracy: 0.6665536723163843\n",
      "\tAnt 9 :\n",
      "\t\tPath: [27, 34, 44, 10, 60, 29, 68, 18, 19, 52, 65, 57, 43, 2, 8]\n",
      "\t\tAccuracy: 0.7309604519774011\n",
      "\tAnt 10 :\n",
      "\t\tPath: [9, 12, 40, 52, 44, 68, 60, 10, 19, 38, 59, 36, 43, 30, 49]\n",
      "\t\tAccuracy: 0.7105084745762712\n",
      "\tAnt 11 :\n",
      "\t\tPath: [40, 56, 23, 41, 16, 21, 57, 65, 49, 12, 42, 36, 43, 3, 25]\n",
      "\t\tAccuracy: 0.6700000000000002\n",
      "\tAnt 12 :\n",
      "\t\tPath: [52, 14, 43, 64, 25, 59, 50, 67, 38, 37, 18, 63, 36, 10, 31]\n",
      "\t\tAccuracy: 0.7070056497175141\n",
      "\tAnt 13 :\n",
      "\t\tPath: [43, 59, 24, 52, 67, 46, 54, 12, 63, 68, 38, 11, 14, 13, 10]\n",
      "\t\tAccuracy: 0.6834463276836158\n",
      "\tAnt 14 :\n",
      "\t\tPath: [63, 59, 41, 10, 57, 44, 58, 49, 0, 9, 23, 54, 18, 52, 60]\n",
      "\t\tAccuracy: 0.7073446327683616\n",
      "\tAnt 15 :\n",
      "\t\tPath: [68, 53, 11, 12, 48, 43, 63, 34, 65, 37, 60, 38, 18, 67, 5]\n",
      "\t\tAccuracy: 0.7071186440677966\n",
      "\tAnt 16 :\n",
      "\t\tPath: [30, 12, 48, 38, 29, 14, 57, 47, 11, 52, 23, 63, 37, 43, 34]\n",
      "\t\tAccuracy: 0.7138418079096045\n",
      "\tAnt 17 :\n",
      "\t\tPath: [19, 57, 12, 59, 34, 14, 50, 51, 9, 56, 8, 16, 10, 7, 22]\n",
      "\t\tAccuracy: 0.7303954802259887\n",
      "\tAnt 18 :\n",
      "\t\tPath: [42, 54, 57, 28, 19, 10, 11, 43, 14, 25, 38, 59, 49, 50, 21]\n",
      "\t\tAccuracy: 0.6836158192090396\n",
      "\tAnt 19 :\n",
      "\t\tPath: [58, 63, 49, 65, 46, 52, 23, 11, 40, 6, 38, 21, 31, 33, 25]\n",
      "\t\tAccuracy: 0.6736723163841807\n",
      "Colony 11 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [41, 60, 38, 48, 37, 11, 12, 59, 50, 9, 30, 10, 6, 25, 14]\n",
      "\t\tAccuracy: 0.6936158192090396\n",
      "\tAnt 1 :\n",
      "\t\tPath: [60, 11, 38, 50, 52, 63, 42, 9, 12, 67, 65, 49, 44, 10, 14]\n",
      "\t\tAccuracy: 0.6968361581920904\n",
      "\tAnt 2 :\n",
      "\t\tPath: [30, 52, 38, 50, 19, 53, 18, 63, 37, 29, 16, 68, 21, 40, 33]\n",
      "\t\tAccuracy: 0.6837853107344631\n",
      "\tAnt 3 :\n",
      "\t\tPath: [20, 37, 38, 48, 25, 12, 63, 42, 65, 52, 60, 18, 50, 47, 14]\n",
      "\t\tAccuracy: 0.71045197740113\n",
      "\tAnt 4 :\n",
      "\t\tPath: [50, 49, 59, 15, 44, 42, 14, 18, 34, 67, 20, 13, 11, 0, 10]\n",
      "\t\tAccuracy: 0.6698870056497175\n",
      "\tAnt 5 :\n",
      "\t\tPath: [0, 57, 25, 49, 44, 38, 18, 37, 9, 46, 67, 54, 22, 11, 52]\n",
      "\t\tAccuracy: 0.7172881355932204\n",
      "\tAnt 6 :\n",
      "\t\tPath: [58, 44, 38, 63, 36, 42, 37, 9, 25, 51, 57, 2, 10, 15, 46]\n",
      "\t\tAccuracy: 0.7342372881355932\n",
      "\tAnt 7 :\n",
      "\t\tPath: [2, 14, 56, 9, 46, 23, 50, 37, 53, 52, 8, 12, 59, 16, 67]\n",
      "\t\tAccuracy: 0.6935593220338984\n",
      "\tAnt 8 :\n",
      "\t\tPath: [28, 38, 44, 59, 43, 65, 52, 53, 15, 27, 29, 2, 12, 50, 25]\n",
      "\t\tAccuracy: 0.7172881355932204\n",
      "\tAnt 9 :\n",
      "\t\tPath: [16, 18, 12, 9, 11, 67, 21, 38, 58, 10, 43, 19, 51, 63, 7]\n",
      "\t\tAccuracy: 0.6396045197740113\n",
      "\tAnt 10 :\n",
      "\t\tPath: [41, 52, 49, 67, 40, 6, 53, 37, 9, 65, 60, 22, 10, 38, 57]\n",
      "\t\tAccuracy: 0.7037853107344632\n",
      "\tAnt 11 :\n",
      "\t\tPath: [57, 19, 63, 20, 46, 8, 50, 59, 58, 10, 12, 44, 37, 49, 14]\n",
      "\t\tAccuracy: 0.7238418079096045\n",
      "\tAnt 12 :\n",
      "\t\tPath: [14, 38, 60, 18, 10, 12, 57, 0, 65, 20, 49, 59, 25, 23, 31]\n",
      "\t\tAccuracy: 0.7136723163841807\n",
      "\tAnt 13 :\n",
      "\t\tPath: [37, 20, 65, 33, 9, 34, 44, 12, 58, 63, 6, 19, 14, 42, 10]\n",
      "\t\tAccuracy: 0.6931073446327684\n",
      "\tAnt 14 :\n",
      "\t\tPath: [29, 9, 37, 67, 28, 44, 54, 14, 63, 24, 65, 49, 34, 21, 52]\n",
      "\t\tAccuracy: 0.6868361581920903\n",
      "\tAnt 15 :\n",
      "\t\tPath: [50, 23, 22, 2, 60, 44, 14, 11, 8, 33, 67, 10, 38, 59, 37]\n",
      "\t\tAccuracy: 0.7240677966101694\n",
      "\tAnt 16 :\n",
      "\t\tPath: [44, 65, 29, 15, 10, 38, 7, 12, 30, 9, 46, 49, 63, 5, 43]\n",
      "\t\tAccuracy: 0.69045197740113\n",
      "\tAnt 17 :\n",
      "\t\tPath: [47, 8, 42, 63, 43, 9, 59, 54, 21, 65, 12, 33, 10, 29, 67]\n",
      "\t\tAccuracy: 0.7073446327683616\n",
      "\tAnt 18 :\n",
      "\t\tPath: [53, 57, 8, 34, 44, 16, 67, 52, 37, 68, 42, 2, 47, 50, 25]\n",
      "\t\tAccuracy: 0.6933898305084745\n",
      "\tAnt 19 :\n",
      "\t\tPath: [12, 56, 13, 2, 63, 37, 18, 30, 1, 38, 50, 19, 67, 23, 11]\n",
      "\t\tAccuracy: 0.6532203389830509\n",
      "Colony 12 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [68, 22, 37, 38, 52, 67, 44, 65, 48, 50, 36, 59, 24, 21, 28]\n",
      "\t\tAccuracy: 0.7170621468926553\n",
      "\tAnt 1 :\n",
      "\t\tPath: [44, 36, 49, 32, 1, 34, 14, 33, 37, 10, 24, 12, 19, 25, 11]\n",
      "\t\tAccuracy: 0.6766666666666666\n",
      "\tAnt 2 :\n",
      "\t\tPath: [65, 9, 19, 8, 23, 10, 28, 33, 16, 63, 53, 57, 18, 30, 14]\n",
      "\t\tAccuracy: 0.7037288135593219\n",
      "\tAnt 3 :\n",
      "\t\tPath: [57, 52, 40, 19, 44, 2, 56, 25, 49, 63, 60, 53, 65, 18, 50]\n",
      "\t\tAccuracy: 0.713728813559322\n",
      "\tAnt 4 :\n",
      "\t\tPath: [22, 18, 25, 28, 14, 67, 24, 51, 63, 16, 11, 44, 69, 38, 2]\n",
      "\t\tAccuracy: 0.7103954802259886\n",
      "\tAnt 5 :\n",
      "\t\tPath: [16, 67, 37, 68, 10, 53, 8, 11, 44, 30, 25, 42, 59, 57, 48]\n",
      "\t\tAccuracy: 0.6566666666666666\n",
      "\tAnt 6 :\n",
      "\t\tPath: [16, 44, 45, 13, 5, 14, 33, 10, 49, 38, 22, 50, 54, 51, 65]\n",
      "\t\tAccuracy: 0.6628813559322034\n",
      "\tAnt 7 :\n",
      "\t\tPath: [14, 54, 16, 49, 22, 5, 9, 63, 67, 36, 50, 64, 59, 37, 58]\n",
      "\t\tAccuracy: 0.6767231638418079\n",
      "\tAnt 8 :\n",
      "\t\tPath: [57, 44, 9, 10, 52, 8, 25, 60, 58, 65, 38, 16, 67, 63, 47]\n",
      "\t\tAccuracy: 0.6900564971751413\n",
      "\tAnt 9 :\n",
      "\t\tPath: [8, 14, 67, 9, 12, 30, 16, 65, 18, 25, 63, 2, 38, 44, 50]\n",
      "\t\tAccuracy: 0.6833333333333333\n",
      "\tAnt 10 :\n",
      "\t\tPath: [12, 14, 10, 52, 43, 60, 65, 21, 23, 44, 16, 11, 42, 58, 0]\n",
      "\t\tAccuracy: 0.649774011299435\n",
      "\tAnt 11 :\n",
      "\t\tPath: [15, 38, 57, 19, 54, 35, 14, 63, 16, 9, 21, 37, 41, 49, 40]\n",
      "\t\tAccuracy: 0.6798305084745763\n",
      "\tAnt 12 :\n",
      "\t\tPath: [52, 59, 38, 30, 40, 21, 11, 53, 20, 60, 49, 33, 9, 15, 22]\n",
      "\t\tAccuracy: 0.6903389830508475\n",
      "\tAnt 13 :\n",
      "\t\tPath: [6, 50, 30, 19, 43, 52, 2, 38, 67, 63, 51, 14, 9, 21, 12]\n",
      "\t\tAccuracy: 0.683502824858757\n",
      "\tAnt 14 :\n",
      "\t\tPath: [10, 33, 12, 19, 31, 58, 42, 36, 11, 38, 37, 44, 14, 59, 48]\n",
      "\t\tAccuracy: 0.7003954802259886\n",
      "\tAnt 15 :\n",
      "\t\tPath: [9, 19, 15, 52, 50, 44, 18, 47, 41, 20, 8, 10, 28, 42, 53]\n",
      "\t\tAccuracy: 0.6632768361581921\n",
      "\tAnt 16 :\n",
      "\t\tPath: [49, 43, 68, 18, 48, 31, 12, 46, 58, 54, 37, 57, 38, 19, 59]\n",
      "\t\tAccuracy: 0.7171186440677966\n",
      "\tAnt 17 :\n",
      "\t\tPath: [47, 8, 50, 44, 15, 20, 14, 30, 12, 57, 49, 9, 42, 65, 23]\n",
      "\t\tAccuracy: 0.7070056497175141\n",
      "\tAnt 18 :\n",
      "\t\tPath: [52, 54, 6, 29, 48, 2, 16, 20, 21, 67, 12, 63, 38, 53, 22]\n",
      "\t\tAccuracy: 0.7101129943502824\n",
      "\tAnt 19 :\n",
      "\t\tPath: [46, 20, 12, 28, 53, 37, 29, 52, 48, 11, 44, 42, 27, 59, 40]\n",
      "\t\tAccuracy: 0.7002824858757062\n",
      "Colony 13 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [9, 42, 59, 41, 53, 34, 49, 30, 8, 38, 18, 48, 28, 67, 43]\n",
      "\t\tAccuracy: 0.6800564971751413\n",
      "\tAnt 1 :\n",
      "\t\tPath: [63, 59, 51, 56, 50, 49, 64, 8, 29, 30, 41, 52, 11, 38, 12]\n",
      "\t\tAccuracy: 0.6866666666666666\n",
      "\tAnt 2 :\n",
      "\t\tPath: [65, 27, 59, 63, 40, 19, 18, 12, 67, 2, 60, 10, 21, 50, 45]\n",
      "\t\tAccuracy: 0.7037853107344633\n",
      "\tAnt 3 :\n",
      "\t\tPath: [67, 47, 57, 19, 49, 27, 53, 38, 15, 50, 48, 6, 21, 52, 37]\n",
      "\t\tAccuracy: 0.6935028248587571\n",
      "\tAnt 4 :\n",
      "\t\tPath: [38, 44, 6, 53, 19, 14, 50, 42, 63, 67, 65, 8, 18, 28, 58]\n",
      "\t\tAccuracy: 0.6665536723163842\n",
      "\tAnt 5 :\n",
      "\t\tPath: [63, 67, 59, 15, 28, 46, 27, 43, 52, 10, 30, 48, 38, 57, 31]\n",
      "\t\tAccuracy: 0.6970621468926554\n",
      "\tAnt 6 :\n",
      "\t\tPath: [23, 12, 59, 57, 42, 63, 58, 18, 25, 43, 21, 30, 8, 2, 49]\n",
      "\t\tAccuracy: 0.6900564971751413\n",
      "\tAnt 7 :\n",
      "\t\tPath: [63, 67, 9, 14, 43, 44, 11, 49, 30, 56, 64, 36, 23, 22, 59]\n",
      "\t\tAccuracy: 0.703728813559322\n",
      "\tAnt 8 :\n",
      "\t\tPath: [60, 12, 43, 44, 11, 50, 57, 19, 28, 7, 16, 38, 53, 67, 25]\n",
      "\t\tAccuracy: 0.6802824858757062\n",
      "\tAnt 9 :\n",
      "\t\tPath: [6, 54, 42, 27, 16, 15, 50, 68, 25, 52, 63, 43, 14, 59, 33]\n",
      "\t\tAccuracy: 0.676271186440678\n",
      "\tAnt 10 :\n",
      "\t\tPath: [65, 57, 60, 22, 28, 59, 9, 63, 47, 14, 12, 27, 20, 38, 33]\n",
      "\t\tAccuracy: 0.7003954802259886\n",
      "\tAnt 11 :\n",
      "\t\tPath: [57, 7, 27, 38, 49, 48, 9, 59, 64, 15, 43, 67, 42, 56, 50]\n",
      "\t\tAccuracy: 0.7105084745762712\n",
      "\tAnt 12 :\n",
      "\t\tPath: [57, 65, 59, 53, 27, 52, 8, 50, 2, 38, 10, 56, 67, 11, 54]\n",
      "\t\tAccuracy: 0.693728813559322\n",
      "\tAnt 13 :\n",
      "\t\tPath: [30, 37, 63, 67, 66, 38, 44, 7, 50, 59, 12, 15, 25, 58, 8]\n",
      "\t\tAccuracy: 0.7036158192090396\n",
      "\tAnt 14 :\n",
      "\t\tPath: [65, 48, 14, 67, 51, 50, 16, 21, 19, 38, 59, 60, 44, 63, 27]\n",
      "\t\tAccuracy: 0.6902824858757062\n",
      "\tAnt 15 :\n",
      "\t\tPath: [67, 58, 34, 44, 5, 65, 42, 12, 22, 63, 60, 64, 50, 38, 16]\n",
      "\t\tAccuracy: 0.717231638418079\n",
      "\tAnt 16 :\n",
      "\t\tPath: [38, 2, 14, 40, 16, 54, 59, 63, 9, 23, 68, 42, 47, 44, 4]\n",
      "\t\tAccuracy: 0.6968926553672317\n",
      "\tAnt 17 :\n",
      "\t\tPath: [18, 50, 30, 51, 57, 29, 34, 19, 69, 12, 2, 11, 52, 36, 43]\n",
      "\t\tAccuracy: 0.6766666666666666\n",
      "\tAnt 18 :\n",
      "\t\tPath: [32, 57, 9, 67, 65, 63, 48, 19, 56, 49, 51, 15, 37, 14, 12]\n",
      "\t\tAccuracy: 0.6867796610169491\n",
      "\tAnt 19 :\n",
      "\t\tPath: [9, 63, 6, 59, 65, 15, 38, 12, 10, 36, 25, 49, 14, 67, 37]\n",
      "\t\tAccuracy: 0.727457627118644\n",
      "Colony 14 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [19, 25, 11, 57, 14, 0, 9, 53, 68, 15, 5, 65, 67, 37, 8]\n",
      "\t\tAccuracy: 0.683276836158192\n",
      "\tAnt 1 :\n",
      "\t\tPath: [53, 31, 58, 47, 24, 38, 0, 12, 37, 25, 63, 11, 43, 48, 2]\n",
      "\t\tAccuracy: 0.636271186440678\n",
      "\tAnt 2 :\n",
      "\t\tPath: [50, 52, 57, 30, 38, 22, 44, 21, 10, 12, 37, 43, 65, 63, 23]\n",
      "\t\tAccuracy: 0.7273446327683617\n",
      "\tAnt 3 :\n",
      "\t\tPath: [11, 43, 49, 38, 33, 51, 13, 28, 63, 42, 30, 19, 40, 12, 16]\n",
      "\t\tAccuracy: 0.6901694915254237\n",
      "\tAnt 4 :\n",
      "\t\tPath: [43, 22, 59, 15, 38, 56, 16, 37, 63, 53, 36, 35, 48, 57, 31]\n",
      "\t\tAccuracy: 0.6701129943502825\n",
      "\tAnt 5 :\n",
      "\t\tPath: [30, 12, 34, 52, 37, 5, 53, 14, 22, 64, 10, 49, 59, 63, 68]\n",
      "\t\tAccuracy: 0.6498870056497175\n",
      "\tAnt 6 :\n",
      "\t\tPath: [44, 42, 36, 5, 38, 9, 57, 59, 30, 6, 63, 49, 14, 37, 67]\n",
      "\t\tAccuracy: 0.7375141242937853\n",
      "\tAnt 7 :\n",
      "\t\tPath: [2, 22, 28, 11, 59, 49, 57, 8, 38, 37, 54, 19, 20, 50, 10]\n",
      "\t\tAccuracy: 0.7074011299435028\n",
      "\tAnt 8 :\n",
      "\t\tPath: [40, 54, 63, 57, 18, 49, 48, 14, 9, 23, 30, 22, 44, 27, 52]\n",
      "\t\tAccuracy: 0.7338983050847457\n",
      "\tAnt 9 :\n",
      "\t\tPath: [58, 37, 38, 20, 13, 19, 12, 45, 4, 50, 25, 63, 43, 41, 51]\n",
      "\t\tAccuracy: 0.6967231638418079\n",
      "\tAnt 10 :\n",
      "\t\tPath: [28, 10, 65, 38, 22, 49, 19, 5, 54, 25, 63, 57, 60, 37, 59]\n",
      "\t\tAccuracy: 0.7544632768361582\n",
      "\tAnt 11 :\n",
      "\t\tPath: [40, 53, 48, 52, 14, 43, 60, 63, 50, 65, 44, 42, 25, 59, 7]\n",
      "\t\tAccuracy: 0.7237853107344632\n",
      "\tAnt 12 :\n",
      "\t\tPath: [53, 19, 12, 9, 44, 38, 35, 65, 29, 28, 63, 37, 18, 57, 64]\n",
      "\t\tAccuracy: 0.6836723163841808\n",
      "\tAnt 13 :\n",
      "\t\tPath: [53, 63, 44, 49, 11, 65, 22, 10, 2, 64, 67, 38, 9, 23, 28]\n",
      "\t\tAccuracy: 0.6566101694915254\n",
      "\tAnt 14 :\n",
      "\t\tPath: [47, 52, 2, 65, 36, 53, 6, 42, 59, 12, 11, 22, 18, 29, 44]\n",
      "\t\tAccuracy: 0.7005084745762711\n",
      "\tAnt 15 :\n",
      "\t\tPath: [44, 63, 6, 48, 58, 45, 60, 40, 19, 37, 52, 65, 57, 14, 9]\n",
      "\t\tAccuracy: 0.7036723163841807\n",
      "\tAnt 16 :\n",
      "\t\tPath: [30, 31, 50, 63, 14, 23, 59, 65, 37, 36, 40, 60, 51, 43, 67]\n",
      "\t\tAccuracy: 0.6971186440677967\n",
      "\tAnt 17 :\n",
      "\t\tPath: [31, 54, 7, 22, 10, 2, 57, 12, 53, 51, 9, 0, 29, 16, 38]\n",
      "\t\tAccuracy: 0.6699435028248588\n",
      "\tAnt 18 :\n",
      "\t\tPath: [58, 18, 50, 67, 65, 47, 23, 2, 54, 52, 59, 12, 9, 38, 63]\n",
      "\t\tAccuracy: 0.6766666666666666\n",
      "\tAnt 19 :\n",
      "\t\tPath: [52, 50, 12, 11, 44, 46, 43, 63, 34, 59, 24, 67, 51, 38, 69]\n",
      "\t\tAccuracy: 0.679774011299435\n",
      "Colony 15 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [28, 51, 56, 33, 21, 37, 59, 9, 12, 27, 23, 65, 29, 44, 50]\n",
      "\t\tAccuracy: 0.6870621468926553\n",
      "\tAnt 1 :\n",
      "\t\tPath: [25, 49, 9, 43, 14, 31, 24, 18, 11, 38, 54, 10, 40, 65, 19]\n",
      "\t\tAccuracy: 0.71045197740113\n",
      "\tAnt 2 :\n",
      "\t\tPath: [65, 29, 56, 40, 59, 10, 14, 60, 8, 57, 43, 64, 27, 37, 15]\n",
      "\t\tAccuracy: 0.6971186440677967\n",
      "\tAnt 3 :\n",
      "\t\tPath: [45, 14, 43, 25, 52, 34, 65, 44, 51, 2, 19, 11, 59, 67, 56]\n",
      "\t\tAccuracy: 0.7067796610169491\n",
      "\tAnt 4 :\n",
      "\t\tPath: [18, 36, 63, 34, 50, 25, 48, 49, 22, 23, 38, 51, 14, 40, 16]\n",
      "\t\tAccuracy: 0.733954802259887\n",
      "\tAnt 5 :\n",
      "\t\tPath: [10, 51, 19, 65, 20, 15, 25, 30, 50, 54, 60, 68, 38, 36, 14]\n",
      "\t\tAccuracy: 0.6936723163841807\n",
      "\tAnt 6 :\n",
      "\t\tPath: [53, 65, 8, 18, 67, 38, 51, 57, 49, 10, 14, 52, 33, 30, 28]\n",
      "\t\tAccuracy: 0.6736723163841807\n",
      "\tAnt 7 :\n",
      "\t\tPath: [14, 22, 19, 43, 65, 51, 29, 21, 37, 23, 50, 53, 40, 42, 49]\n",
      "\t\tAccuracy: 0.6767796610169492\n",
      "\tAnt 8 :\n",
      "\t\tPath: [29, 42, 63, 6, 59, 9, 52, 51, 38, 64, 28, 14, 37, 19, 44]\n",
      "\t\tAccuracy: 0.7040112994350283\n",
      "\tAnt 9 :\n",
      "\t\tPath: [65, 48, 33, 59, 63, 43, 53, 44, 38, 54, 20, 2, 57, 1, 37]\n",
      "\t\tAccuracy: 0.7506779661016949\n",
      "\tAnt 10 :\n",
      "\t\tPath: [57, 56, 31, 38, 7, 9, 52, 16, 14, 59, 20, 19, 67, 30, 45]\n",
      "\t\tAccuracy: 0.7101694915254237\n",
      "\tAnt 11 :\n",
      "\t\tPath: [15, 49, 11, 53, 67, 20, 33, 54, 6, 52, 37, 48, 27, 57, 42]\n",
      "\t\tAccuracy: 0.6699435028248587\n",
      "\tAnt 12 :\n",
      "\t\tPath: [42, 8, 67, 28, 38, 18, 36, 16, 14, 19, 49, 53, 48, 44, 41]\n",
      "\t\tAccuracy: 0.6532203389830509\n",
      "\tAnt 13 :\n",
      "\t\tPath: [10, 65, 63, 37, 16, 41, 14, 59, 54, 33, 9, 50, 53, 25, 28]\n",
      "\t\tAccuracy: 0.7172881355932204\n",
      "\tAnt 14 :\n",
      "\t\tPath: [6, 22, 9, 44, 51, 29, 68, 58, 67, 59, 57, 63, 34, 2, 38]\n",
      "\t\tAccuracy: 0.7338983050847457\n",
      "\tAnt 15 :\n",
      "\t\tPath: [9, 15, 31, 26, 43, 33, 14, 10, 52, 44, 63, 49, 48, 67, 57]\n",
      "\t\tAccuracy: 0.7274011299435028\n",
      "\tAnt 16 :\n",
      "\t\tPath: [10, 60, 53, 15, 6, 51, 59, 38, 46, 11, 16, 50, 43, 58, 48]\n",
      "\t\tAccuracy: 0.7136723163841807\n",
      "\tAnt 17 :\n",
      "\t\tPath: [9, 10, 50, 44, 21, 14, 28, 29, 63, 65, 38, 42, 49, 68, 25]\n",
      "\t\tAccuracy: 0.6802259887005649\n",
      "\tAnt 18 :\n",
      "\t\tPath: [41, 42, 25, 37, 52, 9, 49, 38, 31, 30, 63, 10, 67, 22, 7]\n",
      "\t\tAccuracy: 0.6838983050847457\n",
      "\tAnt 19 :\n",
      "\t\tPath: [18, 65, 21, 56, 67, 37, 38, 53, 52, 31, 58, 10, 34, 57, 25]\n",
      "\t\tAccuracy: 0.6870621468926553\n",
      "Colony 16 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [50, 9, 18, 59, 5, 52, 47, 53, 43, 67, 11, 12, 38, 16, 68]\n",
      "\t\tAccuracy: 0.6967796610169492\n",
      "\tAnt 1 :\n",
      "\t\tPath: [48, 18, 0, 38, 16, 23, 9, 37, 63, 43, 50, 59, 19, 53, 44]\n",
      "\t\tAccuracy: 0.7171186440677966\n",
      "\tAnt 2 :\n",
      "\t\tPath: [59, 37, 68, 14, 67, 15, 28, 51, 8, 50, 54, 10, 23, 2, 40]\n",
      "\t\tAccuracy: 0.6869491525423729\n",
      "\tAnt 3 :\n",
      "\t\tPath: [9, 57, 49, 63, 67, 27, 52, 23, 18, 12, 33, 65, 40, 10, 22]\n",
      "\t\tAccuracy: 0.7305649717514123\n",
      "\tAnt 4 :\n",
      "\t\tPath: [59, 23, 44, 11, 0, 21, 30, 63, 34, 29, 25, 2, 56, 48, 10]\n",
      "\t\tAccuracy: 0.6801129943502825\n",
      "\tAnt 5 :\n",
      "\t\tPath: [30, 52, 63, 9, 24, 14, 18, 28, 54, 37, 49, 22, 68, 65, 41]\n",
      "\t\tAccuracy: 0.6498305084745762\n",
      "\tAnt 6 :\n",
      "\t\tPath: [48, 43, 11, 51, 47, 49, 30, 25, 44, 15, 57, 24, 21, 59, 53]\n",
      "\t\tAccuracy: 0.6801129943502825\n",
      "\tAnt 7 :\n",
      "\t\tPath: [60, 56, 12, 3, 42, 37, 0, 9, 5, 14, 46, 51, 2, 11, 59]\n",
      "\t\tAccuracy: 0.717231638418079\n",
      "\tAnt 8 :\n",
      "\t\tPath: [20, 33, 57, 12, 59, 14, 22, 19, 63, 52, 30, 18, 34, 44, 37]\n",
      "\t\tAccuracy: 0.7036723163841808\n",
      "\tAnt 9 :\n",
      "\t\tPath: [50, 63, 11, 59, 15, 12, 48, 30, 38, 16, 60, 31, 13, 44, 57]\n",
      "\t\tAccuracy: 0.7072316384180791\n",
      "\tAnt 10 :\n",
      "\t\tPath: [54, 63, 11, 14, 22, 57, 25, 58, 48, 67, 52, 15, 38, 37, 36]\n",
      "\t\tAccuracy: 0.7036723163841807\n",
      "\tAnt 11 :\n",
      "\t\tPath: [63, 52, 58, 21, 57, 36, 10, 22, 44, 8, 59, 48, 9, 18, 29]\n",
      "\t\tAccuracy: 0.717231638418079\n",
      "\tAnt 12 :\n",
      "\t\tPath: [28, 11, 10, 53, 19, 33, 13, 31, 63, 59, 58, 50, 18, 67, 37]\n",
      "\t\tAccuracy: 0.6700564971751413\n",
      "\tAnt 13 :\n",
      "\t\tPath: [9, 14, 44, 12, 25, 22, 0, 8, 59, 49, 6, 20, 63, 57, 56]\n",
      "\t\tAccuracy: 0.7307909604519773\n",
      "\tAnt 14 :\n",
      "\t\tPath: [68, 27, 9, 63, 40, 7, 37, 56, 57, 47, 33, 19, 22, 50, 6]\n",
      "\t\tAccuracy: 0.7070056497175141\n",
      "\tAnt 15 :\n",
      "\t\tPath: [59, 63, 30, 31, 43, 50, 7, 11, 57, 67, 12, 33, 48, 9, 2]\n",
      "\t\tAccuracy: 0.6799999999999999\n",
      "\tAnt 16 :\n",
      "\t\tPath: [29, 59, 57, 19, 38, 45, 51, 30, 56, 6, 43, 12, 34, 20, 0]\n",
      "\t\tAccuracy: 0.7303954802259887\n",
      "\tAnt 17 :\n",
      "\t\tPath: [59, 34, 60, 9, 22, 6, 21, 37, 67, 16, 68, 12, 30, 48, 38]\n",
      "\t\tAccuracy: 0.6902259887005651\n",
      "\tAnt 18 :\n",
      "\t\tPath: [2, 22, 16, 23, 30, 10, 63, 44, 18, 20, 48, 57, 8, 6, 59]\n",
      "\t\tAccuracy: 0.6901694915254237\n",
      "\tAnt 19 :\n",
      "\t\tPath: [29, 30, 36, 50, 43, 19, 38, 9, 25, 42, 63, 68, 44, 22, 16]\n",
      "\t\tAccuracy: 0.6968926553672317\n",
      "Colony 17 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [46, 27, 68, 21, 52, 11, 43, 19, 37, 42, 30, 65, 10, 41, 14]\n",
      "\t\tAccuracy: 0.6533333333333333\n",
      "\tAnt 1 :\n",
      "\t\tPath: [12, 11, 10, 49, 60, 31, 34, 18, 53, 67, 65, 63, 52, 51, 57]\n",
      "\t\tAccuracy: 0.6770621468926553\n",
      "\tAnt 2 :\n",
      "\t\tPath: [59, 67, 28, 63, 18, 6, 57, 43, 34, 7, 50, 37, 9, 31, 11]\n",
      "\t\tAccuracy: 0.7138418079096045\n",
      "\tAnt 3 :\n",
      "\t\tPath: [48, 30, 10, 37, 18, 56, 36, 14, 63, 11, 25, 9, 38, 20, 19]\n",
      "\t\tAccuracy: 0.6801129943502825\n",
      "\tAnt 4 :\n",
      "\t\tPath: [15, 29, 53, 44, 60, 1, 9, 34, 56, 49, 14, 67, 10, 43, 8]\n",
      "\t\tAccuracy: 0.6937853107344633\n",
      "\tAnt 5 :\n",
      "\t\tPath: [67, 51, 11, 27, 59, 8, 28, 60, 50, 57, 44, 30, 46, 33, 63]\n",
      "\t\tAccuracy: 0.6935593220338983\n",
      "\tAnt 6 :\n",
      "\t\tPath: [29, 25, 63, 43, 21, 6, 11, 68, 10, 12, 49, 2, 54, 59, 65]\n",
      "\t\tAccuracy: 0.7240677966101695\n",
      "\tAnt 7 :\n",
      "\t\tPath: [14, 67, 30, 59, 11, 9, 43, 49, 38, 10, 25, 44, 20, 63, 60]\n",
      "\t\tAccuracy: 0.7275141242937854\n",
      "\tAnt 8 :\n",
      "\t\tPath: [57, 12, 63, 10, 38, 68, 49, 51, 28, 48, 30, 19, 40, 42, 56]\n",
      "\t\tAccuracy: 0.683276836158192\n",
      "\tAnt 9 :\n",
      "\t\tPath: [48, 58, 9, 14, 52, 44, 33, 63, 38, 11, 19, 49, 10, 23, 46]\n",
      "\t\tAccuracy: 0.7038983050847458\n",
      "\tAnt 10 :\n",
      "\t\tPath: [12, 67, 44, 42, 58, 57, 53, 9, 52, 48, 11, 43, 63, 65, 37]\n",
      "\t\tAccuracy: 0.6901129943502824\n",
      "\tAnt 11 :\n",
      "\t\tPath: [59, 0, 52, 12, 2, 19, 16, 46, 47, 37, 67, 38, 50, 23, 15]\n",
      "\t\tAccuracy: 0.6870621468926554\n",
      "\tAnt 12 :\n",
      "\t\tPath: [31, 37, 57, 9, 14, 8, 10, 19, 63, 29, 27, 5, 11, 43, 16]\n",
      "\t\tAccuracy: 0.7102824858757062\n",
      "\tAnt 13 :\n",
      "\t\tPath: [53, 23, 65, 67, 58, 41, 52, 24, 42, 30, 57, 2, 14, 48, 59]\n",
      "\t\tAccuracy: 0.6900000000000001\n",
      "\tAnt 14 :\n",
      "\t\tPath: [57, 14, 63, 30, 37, 60, 13, 28, 59, 24, 43, 1, 49, 3, 36]\n",
      "\t\tAccuracy: 0.7007909604519774\n",
      "\tAnt 15 :\n",
      "\t\tPath: [34, 59, 63, 20, 38, 19, 57, 14, 58, 56, 43, 37, 12, 22, 46]\n",
      "\t\tAccuracy: 0.723728813559322\n",
      "\tAnt 16 :\n",
      "\t\tPath: [67, 56, 11, 14, 36, 31, 10, 49, 13, 30, 46, 58, 48, 8, 63]\n",
      "\t\tAccuracy: 0.7238418079096045\n",
      "\tAnt 17 :\n",
      "\t\tPath: [67, 50, 63, 22, 12, 46, 48, 49, 57, 58, 9, 29, 20, 14, 54]\n",
      "\t\tAccuracy: 0.6834463276836158\n",
      "\tAnt 18 :\n",
      "\t\tPath: [41, 42, 12, 52, 11, 50, 38, 56, 43, 37, 45, 25, 33, 58, 34]\n",
      "\t\tAccuracy: 0.6734463276836158\n",
      "\tAnt 19 :\n",
      "\t\tPath: [14, 25, 11, 51, 22, 63, 52, 49, 37, 53, 6, 8, 50, 36, 18]\n",
      "\t\tAccuracy: 0.690225988700565\n",
      "Colony 18 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [58, 14, 37, 38, 40, 10, 50, 65, 2, 23, 59, 22, 33, 48, 53]\n",
      "\t\tAccuracy: 0.7336723163841807\n",
      "\tAnt 1 :\n",
      "\t\tPath: [29, 31, 16, 44, 8, 57, 53, 2, 18, 14, 50, 12, 49, 15, 6]\n",
      "\t\tAccuracy: 0.7171186440677967\n",
      "\tAnt 2 :\n",
      "\t\tPath: [15, 12, 0, 59, 8, 9, 53, 26, 47, 68, 50, 29, 6, 28, 19]\n",
      "\t\tAccuracy: 0.6968361581920904\n",
      "\tAnt 3 :\n",
      "\t\tPath: [9, 67, 16, 29, 4, 36, 44, 63, 15, 6, 38, 18, 59, 19, 28]\n",
      "\t\tAccuracy: 0.6667796610169491\n",
      "\tAnt 4 :\n",
      "\t\tPath: [60, 27, 57, 10, 25, 67, 50, 30, 33, 51, 22, 44, 14, 43, 59]\n",
      "\t\tAccuracy: 0.6901129943502824\n",
      "\tAnt 5 :\n",
      "\t\tPath: [15, 50, 48, 55, 49, 59, 67, 33, 37, 34, 38, 56, 14, 63, 28]\n",
      "\t\tAccuracy: 0.6938983050847456\n",
      "\tAnt 6 :\n",
      "\t\tPath: [59, 15, 57, 60, 16, 14, 38, 23, 40, 22, 36, 29, 12, 58, 48]\n",
      "\t\tAccuracy: 0.7140112994350283\n",
      "\tAnt 7 :\n",
      "\t\tPath: [59, 38, 37, 58, 16, 12, 33, 14, 63, 28, 22, 7, 53, 30, 50]\n",
      "\t\tAccuracy: 0.73045197740113\n",
      "\tAnt 8 :\n",
      "\t\tPath: [14, 56, 44, 65, 49, 6, 40, 50, 11, 54, 36, 67, 19, 58, 37]\n",
      "\t\tAccuracy: 0.6903389830508474\n",
      "\tAnt 9 :\n",
      "\t\tPath: [10, 59, 25, 67, 22, 11, 19, 69, 9, 63, 8, 14, 12, 38, 53]\n",
      "\t\tAccuracy: 0.6734463276836158\n",
      "\tAnt 10 :\n",
      "\t\tPath: [30, 16, 33, 58, 37, 38, 43, 41, 8, 59, 36, 49, 48, 11, 67]\n",
      "\t\tAccuracy: 0.6836158192090396\n",
      "\tAnt 11 :\n",
      "\t\tPath: [46, 53, 38, 52, 59, 9, 0, 57, 58, 36, 42, 33, 28, 10, 44]\n",
      "\t\tAccuracy: 0.7003389830508475\n",
      "\tAnt 12 :\n",
      "\t\tPath: [56, 48, 49, 15, 53, 54, 37, 43, 44, 38, 16, 30, 59, 51, 12]\n",
      "\t\tAccuracy: 0.6870621468926553\n",
      "\tAnt 13 :\n",
      "\t\tPath: [60, 52, 58, 33, 12, 65, 20, 19, 29, 50, 51, 10, 54, 63, 44]\n",
      "\t\tAccuracy: 0.6901694915254237\n",
      "\tAnt 14 :\n",
      "\t\tPath: [14, 52, 43, 0, 38, 36, 53, 63, 20, 10, 19, 2, 59, 48, 51]\n",
      "\t\tAccuracy: 0.7271751412429378\n",
      "\tAnt 15 :\n",
      "\t\tPath: [43, 20, 14, 36, 9, 57, 59, 44, 52, 15, 38, 11, 8, 18, 63]\n",
      "\t\tAccuracy: 0.7006214689265537\n",
      "\tAnt 16 :\n",
      "\t\tPath: [12, 29, 37, 27, 14, 9, 23, 31, 63, 15, 33, 53, 16, 11, 43]\n",
      "\t\tAccuracy: 0.6599999999999999\n",
      "\tAnt 17 :\n",
      "\t\tPath: [38, 19, 63, 51, 47, 57, 59, 25, 18, 65, 46, 54, 56, 52, 21]\n",
      "\t\tAccuracy: 0.6935593220338984\n",
      "\tAnt 18 :\n",
      "\t\tPath: [12, 57, 51, 11, 34, 37, 16, 63, 25, 18, 43, 59, 14, 67, 9]\n",
      "\t\tAccuracy: 0.6937853107344634\n",
      "\tAnt 19 :\n",
      "\t\tPath: [20, 40, 8, 14, 59, 28, 44, 22, 57, 12, 63, 10, 33, 43, 56]\n",
      "\t\tAccuracy: 0.7102824858757062\n",
      "Colony 19 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [31, 63, 37, 29, 9, 40, 11, 67, 33, 57, 27, 12, 52, 43, 25]\n",
      "\t\tAccuracy: 0.6836723163841808\n",
      "\tAnt 1 :\n",
      "\t\tPath: [65, 16, 67, 43, 50, 9, 56, 63, 27, 44, 22, 12, 68, 58, 51]\n",
      "\t\tAccuracy: 0.7003389830508474\n",
      "\tAnt 2 :\n",
      "\t\tPath: [53, 29, 22, 38, 51, 60, 67, 44, 57, 30, 63, 9, 5, 25, 46]\n",
      "\t\tAccuracy: 0.7206779661016949\n",
      "\tAnt 3 :\n",
      "\t\tPath: [37, 47, 6, 52, 14, 12, 9, 25, 21, 49, 57, 53, 43, 65, 8]\n",
      "\t\tAccuracy: 0.7306779661016949\n",
      "\tAnt 4 :\n",
      "\t\tPath: [50, 57, 67, 44, 58, 52, 9, 12, 28, 65, 37, 43, 22, 30, 63]\n",
      "\t\tAccuracy: 0.6834463276836158\n",
      "\tAnt 5 :\n",
      "\t\tPath: [38, 29, 10, 12, 59, 51, 20, 67, 50, 15, 37, 9, 8, 30, 40]\n",
      "\t\tAccuracy: 0.6801129943502825\n",
      "\tAnt 6 :\n",
      "\t\tPath: [42, 67, 44, 57, 10, 14, 21, 50, 49, 5, 63, 48, 59, 60, 19]\n",
      "\t\tAccuracy: 0.6868361581920903\n",
      "\tAnt 7 :\n",
      "\t\tPath: [54, 41, 63, 19, 10, 60, 30, 12, 56, 64, 15, 37, 65, 27, 38]\n",
      "\t\tAccuracy: 0.6533333333333333\n",
      "\tAnt 8 :\n",
      "\t\tPath: [53, 67, 28, 44, 55, 49, 54, 50, 12, 34, 63, 22, 43, 51, 2]\n",
      "\t\tAccuracy: 0.6767231638418079\n",
      "\tAnt 9 :\n",
      "\t\tPath: [29, 30, 2, 18, 49, 22, 9, 10, 40, 59, 53, 19, 56, 63, 52]\n",
      "\t\tAccuracy: 0.7206779661016949\n",
      "\tAnt 10 :\n",
      "\t\tPath: [53, 54, 20, 68, 59, 6, 43, 38, 18, 19, 51, 63, 11, 14, 23]\n",
      "\t\tAccuracy: 0.6870056497175142\n",
      "\tAnt 11 :\n",
      "\t\tPath: [65, 63, 44, 6, 66, 25, 14, 37, 57, 10, 52, 24, 11, 50, 40]\n",
      "\t\tAccuracy: 0.6833333333333333\n",
      "\tAnt 12 :\n",
      "\t\tPath: [68, 65, 9, 12, 33, 60, 51, 38, 22, 8, 25, 59, 58, 30, 29]\n",
      "\t\tAccuracy: 0.6632768361581921\n",
      "\tAnt 13 :\n",
      "\t\tPath: [19, 57, 67, 3, 53, 60, 29, 18, 28, 14, 25, 11, 22, 52, 50]\n",
      "\t\tAccuracy: 0.6870056497175141\n",
      "\tAnt 14 :\n",
      "\t\tPath: [50, 2, 16, 20, 38, 53, 21, 23, 49, 43, 65, 42, 46, 59, 15]\n",
      "\t\tAccuracy: 0.7272316384180791\n",
      "\tAnt 15 :\n",
      "\t\tPath: [42, 63, 11, 9, 37, 34, 43, 12, 40, 18, 59, 53, 28, 60, 2]\n",
      "\t\tAccuracy: 0.720677966101695\n",
      "\tAnt 16 :\n",
      "\t\tPath: [5, 19, 67, 11, 23, 38, 30, 33, 48, 27, 44, 36, 6, 15, 49]\n",
      "\t\tAccuracy: 0.6802259887005648\n",
      "\tAnt 17 :\n",
      "\t\tPath: [2, 42, 63, 41, 14, 19, 52, 16, 22, 60, 23, 6, 44, 11, 18]\n",
      "\t\tAccuracy: 0.7103954802259886\n",
      "\tAnt 18 :\n",
      "\t\tPath: [9, 51, 43, 65, 63, 12, 18, 10, 8, 14, 25, 50, 37, 38, 54]\n",
      "\t\tAccuracy: 0.7308474576271186\n",
      "\tAnt 19 :\n",
      "\t\tPath: [15, 59, 27, 9, 65, 19, 37, 50, 64, 68, 30, 28, 48, 5, 2]\n",
      "\t\tAccuracy: 0.6935028248587571\n",
      "Colony 20 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [67, 52, 18, 59, 19, 10, 14, 28, 38, 16, 37, 43, 11, 57, 9]\n",
      "\t\tAccuracy: 0.7138418079096045\n",
      "\tAnt 1 :\n",
      "\t\tPath: [6, 63, 57, 13, 8, 43, 59, 23, 14, 29, 36, 44, 38, 10, 11]\n",
      "\t\tAccuracy: 0.6768361581920905\n",
      "\tAnt 2 :\n",
      "\t\tPath: [43, 13, 18, 65, 57, 51, 22, 44, 14, 53, 15, 58, 29, 19, 68]\n",
      "\t\tAccuracy: 0.6496045197740112\n",
      "\tAnt 3 :\n",
      "\t\tPath: [10, 63, 34, 41, 30, 38, 67, 15, 9, 59, 11, 14, 44, 20, 48]\n",
      "\t\tAccuracy: 0.727457627118644\n",
      "\tAnt 4 :\n",
      "\t\tPath: [21, 58, 67, 12, 9, 37, 57, 49, 38, 44, 20, 2, 10, 51, 27]\n",
      "\t\tAccuracy: 0.7308474576271187\n",
      "\tAnt 5 :\n",
      "\t\tPath: [43, 47, 63, 44, 52, 38, 33, 7, 50, 49, 12, 51, 56, 37, 67]\n",
      "\t\tAccuracy: 0.7207909604519773\n",
      "\tAnt 6 :\n",
      "\t\tPath: [63, 65, 10, 19, 38, 25, 64, 29, 6, 11, 67, 28, 31, 16, 37]\n",
      "\t\tAccuracy: 0.6938983050847458\n",
      "\tAnt 7 :\n",
      "\t\tPath: [38, 51, 44, 58, 20, 23, 53, 25, 48, 19, 16, 5, 67, 50, 43]\n",
      "\t\tAccuracy: 0.707005649717514\n",
      "\tAnt 8 :\n",
      "\t\tPath: [25, 57, 65, 63, 42, 59, 19, 51, 38, 50, 22, 16, 49, 10, 41]\n",
      "\t\tAccuracy: 0.7102824858757062\n",
      "\tAnt 9 :\n",
      "\t\tPath: [57, 29, 14, 12, 63, 10, 25, 49, 38, 52, 54, 68, 19, 18, 42]\n",
      "\t\tAccuracy: 0.7072316384180791\n",
      "\tAnt 10 :\n",
      "\t\tPath: [23, 38, 58, 18, 44, 10, 11, 65, 48, 63, 14, 52, 2, 56, 67]\n",
      "\t\tAccuracy: 0.707231638418079\n",
      "\tAnt 11 :\n",
      "\t\tPath: [45, 25, 10, 19, 5, 18, 64, 2, 48, 14, 49, 38, 6, 11, 53]\n",
      "\t\tAccuracy: 0.7241807909604521\n",
      "\tAnt 12 :\n",
      "\t\tPath: [12, 46, 57, 49, 11, 51, 33, 48, 44, 25, 20, 59, 8, 16, 43]\n",
      "\t\tAccuracy: 0.6903954802259886\n",
      "\tAnt 13 :\n",
      "\t\tPath: [44, 51, 47, 16, 57, 50, 43, 9, 29, 18, 20, 5, 6, 68, 14]\n",
      "\t\tAccuracy: 0.703502824858757\n",
      "\tAnt 14 :\n",
      "\t\tPath: [36, 18, 9, 67, 49, 69, 30, 40, 41, 0, 52, 15, 27, 57, 59]\n",
      "\t\tAccuracy: 0.6868926553672317\n",
      "\tAnt 15 :\n",
      "\t\tPath: [23, 30, 49, 59, 28, 27, 16, 52, 38, 53, 67, 65, 57, 42, 10]\n",
      "\t\tAccuracy: 0.6968926553672317\n",
      "\tAnt 16 :\n",
      "\t\tPath: [12, 25, 43, 63, 49, 50, 53, 16, 19, 65, 6, 59, 38, 57, 9]\n",
      "\t\tAccuracy: 0.7170621468926555\n",
      "\tAnt 17 :\n",
      "\t\tPath: [33, 30, 15, 42, 46, 34, 11, 14, 53, 12, 65, 25, 52, 27, 37]\n",
      "\t\tAccuracy: 0.6701694915254237\n",
      "\tAnt 18 :\n",
      "\t\tPath: [12, 44, 33, 37, 49, 25, 14, 52, 16, 18, 47, 21, 43, 10, 63]\n",
      "\t\tAccuracy: 0.6938418079096046\n",
      "\tAnt 19 :\n",
      "\t\tPath: [25, 23, 54, 14, 19, 18, 8, 49, 64, 11, 42, 7, 10, 46, 15]\n",
      "\t\tAccuracy: 0.6770621468926553\n",
      "Colony 21 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [38, 59, 14, 44, 37, 29, 12, 41, 58, 45, 9, 31, 67, 20, 48]\n",
      "\t\tAccuracy: 0.7100564971751412\n",
      "\tAnt 1 :\n",
      "\t\tPath: [9, 16, 18, 8, 38, 68, 52, 27, 13, 42, 65, 57, 63, 30, 59]\n",
      "\t\tAccuracy: 0.6900000000000001\n",
      "\tAnt 2 :\n",
      "\t\tPath: [67, 53, 46, 54, 59, 18, 65, 57, 12, 9, 26, 30, 10, 29, 16]\n",
      "\t\tAccuracy: 0.6903954802259887\n",
      "\tAnt 3 :\n",
      "\t\tPath: [22, 56, 53, 4, 18, 8, 58, 38, 49, 51, 59, 19, 36, 44, 40]\n",
      "\t\tAccuracy: 0.6768361581920904\n",
      "\tAnt 4 :\n",
      "\t\tPath: [57, 9, 48, 14, 19, 63, 58, 16, 28, 22, 46, 12, 37, 13, 40]\n",
      "\t\tAccuracy: 0.67\n",
      "\tAnt 5 :\n",
      "\t\tPath: [56, 59, 3, 68, 16, 50, 20, 2, 19, 32, 51, 38, 43, 53, 57]\n",
      "\t\tAccuracy: 0.7067796610169491\n",
      "\tAnt 6 :\n",
      "\t\tPath: [34, 59, 16, 42, 52, 63, 65, 49, 57, 44, 22, 25, 10, 38, 18]\n",
      "\t\tAccuracy: 0.7002824858757062\n",
      "\tAnt 7 :\n",
      "\t\tPath: [65, 11, 67, 5, 33, 20, 59, 9, 37, 42, 14, 63, 56, 57, 34]\n",
      "\t\tAccuracy: 0.6866666666666665\n",
      "\tAnt 8 :\n",
      "\t\tPath: [53, 23, 29, 27, 49, 9, 38, 14, 2, 56, 43, 44, 37, 52, 57]\n",
      "\t\tAccuracy: 0.7206779661016949\n",
      "\tAnt 9 :\n",
      "\t\tPath: [9, 2, 46, 57, 38, 63, 50, 7, 53, 44, 31, 12, 48, 30, 11]\n",
      "\t\tAccuracy: 0.7071751412429379\n",
      "\tAnt 10 :\n",
      "\t\tPath: [56, 18, 30, 34, 11, 33, 14, 38, 54, 43, 24, 9, 57, 44, 16]\n",
      "\t\tAccuracy: 0.6631638418079095\n",
      "\tAnt 11 :\n",
      "\t\tPath: [59, 67, 40, 42, 12, 33, 11, 21, 63, 29, 28, 54, 44, 2, 38]\n",
      "\t\tAccuracy: 0.6768361581920904\n",
      "\tAnt 12 :\n",
      "\t\tPath: [27, 25, 57, 38, 68, 50, 10, 24, 18, 8, 11, 67, 63, 9, 51]\n",
      "\t\tAccuracy: 0.7071186440677966\n",
      "\tAnt 13 :\n",
      "\t\tPath: [22, 10, 65, 8, 41, 9, 46, 49, 52, 63, 27, 57, 12, 25, 43]\n",
      "\t\tAccuracy: 0.7405649717514123\n",
      "\tAnt 14 :\n",
      "\t\tPath: [20, 52, 60, 19, 14, 37, 43, 10, 30, 65, 38, 0, 12, 8, 63]\n",
      "\t\tAccuracy: 0.6938418079096046\n",
      "\tAnt 15 :\n",
      "\t\tPath: [25, 61, 63, 38, 18, 41, 54, 50, 59, 44, 57, 14, 37, 43, 2]\n",
      "\t\tAccuracy: 0.7404519774011299\n",
      "\tAnt 16 :\n",
      "\t\tPath: [30, 19, 38, 23, 16, 9, 14, 46, 2, 63, 44, 50, 65, 49, 12]\n",
      "\t\tAccuracy: 0.7173446327683616\n",
      "\tAnt 17 :\n",
      "\t\tPath: [9, 57, 21, 54, 56, 67, 2, 44, 51, 53, 22, 24, 20, 65, 42]\n",
      "\t\tAccuracy: 0.7001694915254237\n",
      "\tAnt 18 :\n",
      "\t\tPath: [33, 67, 59, 63, 10, 16, 38, 14, 2, 53, 12, 7, 22, 15, 51]\n",
      "\t\tAccuracy: 0.6906214689265536\n",
      "\tAnt 19 :\n",
      "\t\tPath: [51, 63, 10, 23, 36, 22, 2, 9, 29, 52, 65, 50, 11, 14, 45]\n",
      "\t\tAccuracy: 0.6867796610169492\n",
      "Colony 22 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [54, 33, 53, 49, 43, 37, 18, 12, 38, 59, 30, 0, 63, 44, 41]\n",
      "\t\tAccuracy: 0.7102824858757063\n",
      "\tAnt 1 :\n",
      "\t\tPath: [38, 47, 52, 46, 16, 18, 25, 12, 63, 49, 20, 21, 48, 44, 68]\n",
      "\t\tAccuracy: 0.6731638418079096\n",
      "\tAnt 2 :\n",
      "\t\tPath: [59, 54, 50, 25, 6, 44, 14, 30, 11, 34, 38, 12, 16, 28, 27]\n",
      "\t\tAccuracy: 0.6902259887005651\n",
      "\tAnt 3 :\n",
      "\t\tPath: [57, 44, 33, 7, 65, 24, 60, 67, 16, 11, 12, 48, 9, 51, 10]\n",
      "\t\tAccuracy: 0.6802824858757062\n",
      "\tAnt 4 :\n",
      "\t\tPath: [43, 23, 20, 49, 25, 38, 8, 56, 65, 59, 53, 63, 14, 46, 9]\n",
      "\t\tAccuracy: 0.7071751412429378\n",
      "\tAnt 5 :\n",
      "\t\tPath: [44, 56, 65, 8, 14, 30, 67, 12, 48, 37, 38, 11, 6, 10, 25]\n",
      "\t\tAccuracy: 0.6971751412429379\n",
      "\tAnt 6 :\n",
      "\t\tPath: [5, 2, 8, 38, 27, 14, 48, 10, 67, 50, 43, 12, 57, 52, 59]\n",
      "\t\tAccuracy: 0.7341242937853106\n",
      "\tAnt 7 :\n",
      "\t\tPath: [0, 49, 68, 44, 25, 38, 20, 67, 31, 16, 48, 6, 57, 52, 2]\n",
      "\t\tAccuracy: 0.727231638418079\n",
      "\tAnt 8 :\n",
      "\t\tPath: [38, 36, 57, 54, 60, 10, 30, 9, 15, 65, 63, 18, 46, 50, 34]\n",
      "\t\tAccuracy: 0.7005084745762712\n",
      "\tAnt 9 :\n",
      "\t\tPath: [68, 49, 9, 57, 43, 14, 63, 16, 59, 50, 19, 12, 67, 10, 38]\n",
      "\t\tAccuracy: 0.7003389830508474\n",
      "\tAnt 10 :\n",
      "\t\tPath: [59, 22, 57, 18, 3, 8, 44, 11, 25, 29, 46, 60, 49, 65, 31]\n",
      "\t\tAccuracy: 0.7105649717514124\n",
      "\tAnt 11 :\n",
      "\t\tPath: [21, 46, 34, 9, 52, 68, 36, 44, 59, 10, 48, 50, 53, 22, 63]\n",
      "\t\tAccuracy: 0.7003389830508474\n",
      "\tAnt 12 :\n",
      "\t\tPath: [57, 11, 56, 7, 16, 42, 14, 59, 67, 10, 18, 38, 9, 49, 12]\n",
      "\t\tAccuracy: 0.6903954802259886\n",
      "\tAnt 13 :\n",
      "\t\tPath: [21, 40, 53, 29, 46, 59, 33, 16, 52, 50, 54, 19, 65, 22, 37]\n",
      "\t\tAccuracy: 0.6935593220338983\n",
      "\tAnt 14 :\n",
      "\t\tPath: [57, 40, 36, 60, 25, 6, 23, 50, 12, 31, 2, 63, 43, 49, 20]\n",
      "\t\tAccuracy: 0.7238983050847457\n",
      "\tAnt 15 :\n",
      "\t\tPath: [19, 57, 65, 52, 42, 14, 22, 43, 54, 6, 45, 12, 59, 10, 33]\n",
      "\t\tAccuracy: 0.6834463276836158\n",
      "\tAnt 16 :\n",
      "\t\tPath: [38, 18, 11, 65, 21, 63, 57, 16, 41, 60, 53, 59, 50, 22, 51]\n",
      "\t\tAccuracy: 0.7035593220338983\n",
      "\tAnt 17 :\n",
      "\t\tPath: [38, 67, 11, 50, 52, 21, 1, 31, 6, 23, 49, 65, 68, 9, 42]\n",
      "\t\tAccuracy: 0.6870056497175141\n",
      "\tAnt 18 :\n",
      "\t\tPath: [38, 27, 11, 43, 14, 58, 65, 8, 54, 23, 44, 9, 40, 10, 67]\n",
      "\t\tAccuracy: 0.6766666666666666\n",
      "\tAnt 19 :\n",
      "\t\tPath: [12, 2, 30, 48, 50, 19, 59, 37, 44, 60, 28, 67, 38, 65, 29]\n",
      "\t\tAccuracy: 0.723954802259887\n",
      "Colony 23 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [14, 52, 63, 48, 12, 9, 33, 60, 58, 56, 38, 67, 57, 34, 25]\n",
      "\t\tAccuracy: 0.7307344632768361\n",
      "\tAnt 1 :\n",
      "\t\tPath: [63, 53, 65, 42, 5, 14, 56, 12, 31, 38, 37, 15, 43, 57, 22]\n",
      "\t\tAccuracy: 0.707005649717514\n",
      "\tAnt 2 :\n",
      "\t\tPath: [63, 65, 14, 41, 68, 8, 37, 57, 12, 18, 48, 59, 33, 38, 36]\n",
      "\t\tAccuracy: 0.7066666666666667\n",
      "\tAnt 3 :\n",
      "\t\tPath: [5, 50, 22, 33, 44, 19, 52, 38, 60, 67, 63, 9, 54, 57, 2]\n",
      "\t\tAccuracy: 0.7441242937853108\n",
      "\tAnt 4 :\n",
      "\t\tPath: [51, 54, 53, 57, 56, 9, 43, 60, 12, 31, 2, 15, 20, 38, 68]\n",
      "\t\tAccuracy: 0.7237853107344632\n",
      "\tAnt 5 :\n",
      "\t\tPath: [18, 44, 68, 22, 31, 60, 29, 50, 58, 67, 63, 52, 38, 12, 16]\n",
      "\t\tAccuracy: 0.6631638418079097\n",
      "\tAnt 6 :\n",
      "\t\tPath: [56, 57, 25, 23, 22, 16, 38, 50, 28, 12, 29, 58, 67, 49, 42]\n",
      "\t\tAccuracy: 0.7105084745762712\n",
      "\tAnt 7 :\n",
      "\t\tPath: [10, 53, 43, 47, 29, 12, 37, 8, 38, 11, 51, 41, 5, 2, 54]\n",
      "\t\tAccuracy: 0.7038983050847458\n",
      "\tAnt 8 :\n",
      "\t\tPath: [20, 42, 52, 12, 29, 67, 56, 41, 16, 19, 53, 11, 63, 8, 38]\n",
      "\t\tAccuracy: 0.6867796610169492\n",
      "\tAnt 9 :\n",
      "\t\tPath: [34, 44, 16, 43, 53, 38, 40, 10, 59, 31, 9, 57, 54, 63, 19]\n",
      "\t\tAccuracy: 0.7137853107344633\n",
      "\tAnt 10 :\n",
      "\t\tPath: [12, 6, 19, 45, 21, 38, 2, 37, 63, 60, 67, 49, 57, 59, 15]\n",
      "\t\tAccuracy: 0.7340112994350283\n",
      "\tAnt 11 :\n",
      "\t\tPath: [22, 37, 51, 67, 59, 18, 33, 40, 44, 30, 9, 38, 14, 8, 7]\n",
      "\t\tAccuracy: 0.727231638418079\n",
      "\tAnt 12 :\n",
      "\t\tPath: [63, 41, 38, 12, 22, 40, 59, 52, 57, 46, 68, 50, 9, 43, 23]\n",
      "\t\tAccuracy: 0.7001694915254236\n",
      "\tAnt 13 :\n",
      "\t\tPath: [57, 45, 63, 38, 9, 21, 42, 43, 15, 65, 27, 67, 59, 50, 11]\n",
      "\t\tAccuracy: 0.6970056497175141\n",
      "\tAnt 14 :\n",
      "\t\tPath: [3, 63, 64, 4, 19, 59, 38, 14, 29, 9, 23, 57, 10, 6, 65]\n",
      "\t\tAccuracy: 0.703728813559322\n",
      "\tAnt 15 :\n",
      "\t\tPath: [18, 52, 49, 10, 21, 48, 50, 44, 63, 9, 43, 22, 30, 58, 23]\n",
      "\t\tAccuracy: 0.7139548022598871\n",
      "\tAnt 16 :\n",
      "\t\tPath: [52, 5, 43, 50, 68, 14, 42, 41, 10, 58, 19, 46, 57, 36, 37]\n",
      "\t\tAccuracy: 0.723728813559322\n",
      "\tAnt 17 :\n",
      "\t\tPath: [7, 28, 11, 51, 43, 9, 63, 44, 38, 27, 10, 8, 23, 30, 37]\n",
      "\t\tAccuracy: 0.703954802259887\n",
      "\tAnt 18 :\n",
      "\t\tPath: [43, 63, 65, 59, 33, 42, 12, 52, 56, 5, 2, 20, 30, 19, 9]\n",
      "\t\tAccuracy: 0.7001694915254236\n",
      "\tAnt 19 :\n",
      "\t\tPath: [58, 6, 52, 50, 2, 57, 9, 49, 29, 19, 63, 31, 14, 33, 61]\n",
      "\t\tAccuracy: 0.7003389830508475\n",
      "Colony 24 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [18, 57, 50, 67, 19, 37, 63, 52, 51, 38, 23, 34, 9, 43, 12]\n",
      "\t\tAccuracy: 0.713954802259887\n",
      "\tAnt 1 :\n",
      "\t\tPath: [48, 15, 50, 44, 63, 23, 33, 22, 43, 16, 6, 14, 52, 41, 54]\n",
      "\t\tAccuracy: 0.6866666666666666\n",
      "\tAnt 2 :\n",
      "\t\tPath: [30, 63, 10, 23, 14, 51, 46, 18, 37, 50, 13, 67, 53, 2, 44]\n",
      "\t\tAccuracy: 0.690225988700565\n",
      "\tAnt 3 :\n",
      "\t\tPath: [25, 59, 29, 63, 44, 36, 7, 40, 16, 2, 54, 47, 9, 11, 67]\n",
      "\t\tAccuracy: 0.6601694915254237\n",
      "\tAnt 4 :\n",
      "\t\tPath: [5, 38, 30, 6, 31, 8, 33, 25, 44, 21, 10, 27, 52, 18, 60]\n",
      "\t\tAccuracy: 0.7109039548022599\n",
      "\tAnt 5 :\n",
      "\t\tPath: [10, 5, 52, 65, 14, 49, 56, 9, 53, 54, 38, 16, 29, 59, 18]\n",
      "\t\tAccuracy: 0.6801694915254237\n",
      "\tAnt 6 :\n",
      "\t\tPath: [42, 67, 50, 38, 63, 15, 65, 28, 49, 44, 11, 14, 47, 33, 57]\n",
      "\t\tAccuracy: 0.7135593220338983\n",
      "\tAnt 7 :\n",
      "\t\tPath: [42, 65, 10, 67, 9, 14, 56, 19, 33, 51, 11, 12, 30, 57, 49]\n",
      "\t\tAccuracy: 0.6799999999999999\n",
      "\tAnt 8 :\n",
      "\t\tPath: [59, 24, 44, 52, 28, 42, 12, 38, 6, 34, 51, 25, 22, 37, 64]\n",
      "\t\tAccuracy: 0.7373446327683617\n",
      "\tAnt 9 :\n",
      "\t\tPath: [28, 36, 43, 52, 38, 56, 45, 68, 34, 14, 18, 51, 9, 59, 63]\n",
      "\t\tAccuracy: 0.659774011299435\n",
      "\tAnt 10 :\n",
      "\t\tPath: [54, 14, 41, 58, 67, 12, 65, 50, 16, 19, 18, 31, 22, 11, 9]\n",
      "\t\tAccuracy: 0.6090395480225987\n",
      "\tAnt 11 :\n",
      "\t\tPath: [44, 59, 8, 9, 37, 4, 52, 29, 27, 23, 2, 12, 57, 38, 63]\n",
      "\t\tAccuracy: 0.7475141242937852\n",
      "\tAnt 12 :\n",
      "\t\tPath: [53, 50, 67, 19, 57, 63, 16, 36, 59, 9, 18, 14, 21, 60, 42]\n",
      "\t\tAccuracy: 0.7003389830508475\n",
      "\tAnt 13 :\n",
      "\t\tPath: [65, 11, 22, 36, 30, 28, 40, 2, 10, 42, 51, 25, 49, 52, 44]\n",
      "\t\tAccuracy: 0.6872316384180791\n",
      "\tAnt 14 :\n",
      "\t\tPath: [44, 14, 43, 41, 36, 31, 56, 33, 2, 37, 34, 13, 52, 29, 19]\n",
      "\t\tAccuracy: 0.6327118644067797\n",
      "\tAnt 15 :\n",
      "\t\tPath: [38, 65, 8, 42, 18, 19, 57, 10, 16, 52, 59, 37, 9, 60, 44]\n",
      "\t\tAccuracy: 0.6701129943502824\n",
      "\tAnt 16 :\n",
      "\t\tPath: [38, 63, 48, 15, 31, 67, 36, 50, 65, 42, 25, 19, 51, 60, 52]\n",
      "\t\tAccuracy: 0.6969491525423729\n",
      "\tAnt 17 :\n",
      "\t\tPath: [67, 54, 12, 50, 43, 9, 6, 53, 15, 18, 34, 65, 24, 25, 49]\n",
      "\t\tAccuracy: 0.6767231638418079\n",
      "\tAnt 18 :\n",
      "\t\tPath: [12, 18, 14, 31, 50, 57, 30, 44, 67, 23, 60, 27, 59, 63, 20]\n",
      "\t\tAccuracy: 0.6698305084745763\n",
      "\tAnt 19 :\n",
      "\t\tPath: [10, 44, 59, 68, 31, 38, 24, 8, 29, 13, 27, 45, 40, 23, 16]\n",
      "\t\tAccuracy: 0.6901129943502825\n",
      "Colony 25 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [14, 40, 50, 19, 21, 60, 43, 53, 52, 59, 38, 46, 57, 6, 42]\n",
      "\t\tAccuracy: 0.7306214689265536\n",
      "\tAnt 1 :\n",
      "\t\tPath: [19, 63, 44, 38, 0, 52, 49, 12, 51, 8, 14, 22, 13, 54, 59]\n",
      "\t\tAccuracy: 0.6497175141242938\n",
      "\tAnt 2 :\n",
      "\t\tPath: [10, 43, 52, 59, 63, 27, 31, 49, 57, 37, 65, 46, 45, 44, 54]\n",
      "\t\tAccuracy: 0.7240677966101694\n",
      "\tAnt 3 :\n",
      "\t\tPath: [65, 43, 10, 57, 16, 41, 37, 36, 59, 21, 34, 7, 60, 23, 49]\n",
      "\t\tAccuracy: 0.7105649717514124\n",
      "\tAnt 4 :\n",
      "\t\tPath: [43, 18, 63, 47, 22, 1, 12, 14, 40, 36, 58, 48, 52, 8, 50]\n",
      "\t\tAccuracy: 0.6970621468926553\n",
      "\tAnt 5 :\n",
      "\t\tPath: [60, 49, 11, 52, 47, 16, 28, 25, 30, 14, 42, 29, 50, 34, 59]\n",
      "\t\tAccuracy: 0.6736158192090395\n",
      "\tAnt 6 :\n",
      "\t\tPath: [58, 11, 16, 6, 51, 24, 46, 33, 49, 59, 28, 8, 63, 15, 18]\n",
      "\t\tAccuracy: 0.6901694915254237\n",
      "\tAnt 7 :\n",
      "\t\tPath: [15, 56, 40, 59, 51, 45, 46, 57, 44, 52, 36, 43, 42, 37, 38]\n",
      "\t\tAccuracy: 0.6769491525423728\n",
      "\tAnt 8 :\n",
      "\t\tPath: [12, 51, 63, 57, 43, 20, 8, 53, 40, 50, 38, 23, 27, 59, 16]\n",
      "\t\tAccuracy: 0.7441242937853108\n",
      "\tAnt 9 :\n",
      "\t\tPath: [42, 57, 65, 37, 56, 59, 8, 25, 50, 46, 9, 44, 49, 31, 15]\n",
      "\t\tAccuracy: 0.7307344632768361\n",
      "\tAnt 10 :\n",
      "\t\tPath: [58, 28, 41, 16, 15, 2, 57, 37, 14, 25, 6, 3, 12, 30, 51]\n",
      "\t\tAccuracy: 0.693728813559322\n",
      "\tAnt 11 :\n",
      "\t\tPath: [2, 65, 57, 63, 14, 42, 45, 43, 21, 15, 38, 30, 33, 49, 25]\n",
      "\t\tAccuracy: 0.7273446327683617\n",
      "\tAnt 12 :\n",
      "\t\tPath: [43, 50, 47, 18, 51, 21, 44, 10, 49, 38, 20, 63, 33, 53, 42]\n",
      "\t\tAccuracy: 0.7072316384180791\n",
      "\tAnt 13 :\n",
      "\t\tPath: [37, 27, 63, 42, 58, 38, 10, 12, 44, 11, 0, 8, 57, 51, 54]\n",
      "\t\tAccuracy: 0.6666101694915254\n",
      "\tAnt 14 :\n",
      "\t\tPath: [43, 52, 29, 59, 58, 37, 50, 30, 8, 57, 23, 22, 19, 63, 42]\n",
      "\t\tAccuracy: 0.6970056497175141\n",
      "\tAnt 15 :\n",
      "\t\tPath: [49, 18, 37, 38, 9, 44, 12, 25, 14, 31, 58, 63, 65, 54, 28]\n",
      "\t\tAccuracy: 0.6766101694915254\n",
      "\tAnt 16 :\n",
      "\t\tPath: [63, 67, 16, 30, 53, 64, 10, 60, 14, 57, 0, 43, 38, 59, 58]\n",
      "\t\tAccuracy: 0.6667231638418079\n",
      "\tAnt 17 :\n",
      "\t\tPath: [41, 67, 8, 64, 16, 25, 29, 9, 4, 63, 1, 53, 11, 22, 60]\n",
      "\t\tAccuracy: 0.6802259887005648\n",
      "\tAnt 18 :\n",
      "\t\tPath: [25, 9, 8, 19, 6, 16, 43, 11, 18, 63, 51, 2, 57, 10, 53]\n",
      "\t\tAccuracy: 0.7273446327683615\n",
      "\tAnt 19 :\n",
      "\t\tPath: [63, 52, 56, 9, 65, 16, 50, 14, 60, 59, 2, 6, 45, 37, 20]\n",
      "\t\tAccuracy: 0.7172881355932204\n",
      "Colony 26 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [38, 7, 51, 63, 19, 37, 28, 52, 46, 25, 14, 48, 18, 65, 41]\n",
      "\t\tAccuracy: 0.6871186440677965\n",
      "\tAnt 1 :\n",
      "\t\tPath: [38, 22, 53, 67, 68, 57, 14, 12, 59, 29, 18, 45, 11, 33, 58]\n",
      "\t\tAccuracy: 0.652994350282486\n",
      "\tAnt 2 :\n",
      "\t\tPath: [10, 42, 22, 58, 65, 52, 29, 20, 63, 40, 59, 38, 43, 33, 37]\n",
      "\t\tAccuracy: 0.713954802259887\n",
      "\tAnt 3 :\n",
      "\t\tPath: [57, 56, 15, 38, 59, 44, 10, 65, 23, 6, 68, 22, 28, 63, 29]\n",
      "\t\tAccuracy: 0.6733898305084747\n",
      "\tAnt 4 :\n",
      "\t\tPath: [12, 19, 52, 41, 38, 44, 57, 63, 28, 48, 51, 14, 53, 30, 18]\n",
      "\t\tAccuracy: 0.7101694915254237\n",
      "\tAnt 5 :\n",
      "\t\tPath: [51, 31, 56, 33, 57, 21, 37, 6, 13, 30, 14, 67, 18, 44, 50]\n",
      "\t\tAccuracy: 0.6800564971751413\n",
      "\tAnt 6 :\n",
      "\t\tPath: [49, 8, 63, 10, 12, 38, 20, 52, 40, 14, 43, 33, 65, 57, 47]\n",
      "\t\tAccuracy: 0.7072316384180791\n",
      "\tAnt 7 :\n",
      "\t\tPath: [21, 65, 68, 38, 53, 12, 19, 15, 51, 40, 59, 8, 67, 20, 57]\n",
      "\t\tAccuracy: 0.6531638418079095\n",
      "\tAnt 8 :\n",
      "\t\tPath: [49, 51, 63, 50, 30, 57, 19, 9, 13, 65, 2, 48, 18, 25, 37]\n",
      "\t\tAccuracy: 0.7374011299435028\n",
      "\tAnt 9 :\n",
      "\t\tPath: [58, 44, 2, 27, 25, 47, 59, 31, 9, 67, 38, 16, 49, 41, 7]\n",
      "\t\tAccuracy: 0.7306214689265536\n",
      "\tAnt 10 :\n",
      "\t\tPath: [49, 63, 27, 9, 58, 67, 37, 18, 25, 34, 30, 40, 54, 38, 50]\n",
      "\t\tAccuracy: 0.6900564971751412\n",
      "\tAnt 11 :\n",
      "\t\tPath: [15, 19, 16, 27, 37, 36, 42, 38, 13, 7, 10, 67, 52, 57, 54]\n",
      "\t\tAccuracy: 0.6968926553672317\n",
      "\tAnt 12 :\n",
      "\t\tPath: [33, 57, 21, 43, 2, 60, 67, 25, 59, 8, 46, 18, 9, 41, 36]\n",
      "\t\tAccuracy: 0.7103389830508474\n",
      "\tAnt 13 :\n",
      "\t\tPath: [49, 18, 57, 52, 25, 19, 12, 0, 63, 67, 53, 9, 38, 44, 40]\n",
      "\t\tAccuracy: 0.6769491525423728\n",
      "\tAnt 14 :\n",
      "\t\tPath: [67, 23, 65, 53, 20, 49, 64, 6, 19, 22, 59, 29, 14, 27, 48]\n",
      "\t\tAccuracy: 0.7069491525423729\n",
      "\tAnt 15 :\n",
      "\t\tPath: [40, 37, 63, 2, 12, 50, 16, 34, 14, 43, 20, 19, 28, 67, 59]\n",
      "\t\tAccuracy: 0.713954802259887\n",
      "\tAnt 16 :\n",
      "\t\tPath: [30, 47, 37, 9, 19, 63, 59, 12, 16, 50, 31, 42, 57, 51, 38]\n",
      "\t\tAccuracy: 0.7102259887005651\n",
      "\tAnt 17 :\n",
      "\t\tPath: [38, 57, 59, 37, 58, 15, 44, 18, 50, 63, 11, 54, 8, 14, 67]\n",
      "\t\tAccuracy: 0.6935593220338983\n",
      "\tAnt 18 :\n",
      "\t\tPath: [12, 10, 52, 11, 18, 67, 44, 6, 60, 63, 57, 50, 8, 29, 51]\n",
      "\t\tAccuracy: 0.6768926553672315\n",
      "\tAnt 19 :\n",
      "\t\tPath: [51, 53, 63, 67, 28, 37, 14, 8, 38, 9, 21, 13, 41, 68, 52]\n",
      "\t\tAccuracy: 0.6801129943502825\n",
      "Colony 27 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [6, 15, 38, 19, 59, 21, 11, 52, 65, 23, 41, 57, 16, 58, 48]\n",
      "\t\tAccuracy: 0.6429378531073446\n",
      "\tAnt 1 :\n",
      "\t\tPath: [44, 63, 12, 33, 51, 67, 59, 15, 16, 43, 9, 31, 57, 41, 38]\n",
      "\t\tAccuracy: 0.7035593220338983\n",
      "\tAnt 2 :\n",
      "\t\tPath: [19, 67, 65, 48, 11, 37, 57, 47, 43, 30, 38, 28, 41, 44, 50]\n",
      "\t\tAccuracy: 0.7105649717514123\n",
      "\tAnt 3 :\n",
      "\t\tPath: [50, 49, 19, 65, 9, 14, 53, 21, 38, 10, 6, 12, 30, 63, 22]\n",
      "\t\tAccuracy: 0.6870621468926552\n",
      "\tAnt 4 :\n",
      "\t\tPath: [51, 56, 20, 0, 38, 63, 50, 22, 13, 67, 37, 29, 11, 33, 31]\n",
      "\t\tAccuracy: 0.6565536723163842\n",
      "\tAnt 5 :\n",
      "\t\tPath: [69, 60, 63, 52, 10, 68, 59, 44, 48, 15, 19, 65, 57, 14, 43]\n",
      "\t\tAccuracy: 0.6833898305084746\n",
      "\tAnt 6 :\n",
      "\t\tPath: [23, 52, 27, 11, 22, 59, 63, 44, 49, 67, 58, 20, 19, 14, 38]\n",
      "\t\tAccuracy: 0.683276836158192\n",
      "\tAnt 7 :\n",
      "\t\tPath: [40, 49, 11, 50, 31, 44, 43, 68, 14, 59, 57, 22, 25, 9, 38]\n",
      "\t\tAccuracy: 0.6971186440677967\n",
      "\tAnt 8 :\n",
      "\t\tPath: [30, 48, 37, 11, 52, 4, 41, 40, 10, 18, 31, 58, 63, 59, 38]\n",
      "\t\tAccuracy: 0.6802259887005649\n",
      "\tAnt 9 :\n",
      "\t\tPath: [2, 56, 59, 38, 44, 53, 48, 18, 27, 14, 22, 28, 25, 37, 49]\n",
      "\t\tAccuracy: 0.723954802259887\n",
      "\tAnt 10 :\n",
      "\t\tPath: [65, 38, 63, 5, 57, 42, 44, 41, 14, 4, 53, 22, 29, 50, 67]\n",
      "\t\tAccuracy: 0.7000564971751413\n",
      "\tAnt 11 :\n",
      "\t\tPath: [42, 30, 63, 49, 65, 43, 25, 33, 53, 2, 12, 67, 20, 50, 38]\n",
      "\t\tAccuracy: 0.7273446327683615\n",
      "\tAnt 12 :\n",
      "\t\tPath: [27, 33, 67, 9, 38, 12, 58, 53, 59, 24, 37, 23, 28, 43, 41]\n",
      "\t\tAccuracy: 0.7171751412429378\n",
      "\tAnt 13 :\n",
      "\t\tPath: [42, 44, 49, 63, 36, 40, 25, 19, 33, 54, 58, 51, 6, 38, 30]\n",
      "\t\tAccuracy: 0.6698305084745763\n",
      "\tAnt 14 :\n",
      "\t\tPath: [44, 14, 60, 6, 12, 54, 9, 23, 38, 43, 19, 52, 65, 28, 3]\n",
      "\t\tAccuracy: 0.6971751412429377\n",
      "\tAnt 15 :\n",
      "\t\tPath: [50, 37, 8, 48, 10, 6, 9, 42, 25, 44, 16, 49, 12, 65, 11]\n",
      "\t\tAccuracy: 0.6970621468926554\n",
      "\tAnt 16 :\n",
      "\t\tPath: [36, 11, 4, 57, 44, 42, 43, 30, 59, 12, 50, 41, 18, 16, 67]\n",
      "\t\tAccuracy: 0.6667231638418079\n",
      "\tAnt 17 :\n",
      "\t\tPath: [10, 67, 31, 37, 49, 55, 42, 59, 47, 51, 38, 57, 13, 9, 14]\n",
      "\t\tAccuracy: 0.7071186440677966\n",
      "\tAnt 18 :\n",
      "\t\tPath: [44, 22, 59, 33, 57, 2, 14, 30, 47, 29, 48, 52, 50, 38, 18]\n",
      "\t\tAccuracy: 0.7508474576271186\n",
      "\tAnt 19 :\n",
      "\t\tPath: [2, 33, 12, 37, 51, 38, 44, 16, 57, 14, 65, 56, 43, 30, 18]\n",
      "\t\tAccuracy: 0.727231638418079\n",
      "Colony 28 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [31, 43, 2, 51, 13, 11, 36, 52, 59, 56, 53, 65, 67, 34, 14]\n",
      "\t\tAccuracy: 0.6664971751412428\n",
      "\tAnt 1 :\n",
      "\t\tPath: [49, 67, 44, 48, 63, 2, 60, 57, 61, 50, 10, 27, 54, 58, 38]\n",
      "\t\tAccuracy: 0.7107344632768362\n",
      "\tAnt 2 :\n",
      "\t\tPath: [42, 31, 43, 59, 40, 44, 63, 11, 50, 18, 52, 57, 67, 38, 30]\n",
      "\t\tAccuracy: 0.7003954802259886\n",
      "\tAnt 3 :\n",
      "\t\tPath: [31, 22, 38, 67, 21, 53, 20, 27, 63, 28, 65, 25, 54, 30, 49]\n",
      "\t\tAccuracy: 0.6837288135593219\n",
      "\tAnt 4 :\n",
      "\t\tPath: [9, 38, 19, 36, 56, 50, 11, 52, 37, 12, 46, 21, 64, 14, 2]\n",
      "\t\tAccuracy: 0.6734463276836159\n",
      "\tAnt 5 :\n",
      "\t\tPath: [59, 11, 25, 16, 65, 14, 37, 52, 47, 23, 33, 50, 20, 49, 2]\n",
      "\t\tAccuracy: 0.6736158192090395\n",
      "\tAnt 6 :\n",
      "\t\tPath: [63, 41, 20, 42, 51, 13, 60, 14, 49, 64, 25, 50, 11, 37, 28]\n",
      "\t\tAccuracy: 0.6936158192090395\n",
      "\tAnt 7 :\n",
      "\t\tPath: [10, 22, 68, 38, 11, 43, 52, 2, 14, 9, 63, 24, 25, 8, 67]\n",
      "\t\tAccuracy: 0.744180790960452\n",
      "\tAnt 8 :\n",
      "\t\tPath: [11, 30, 22, 59, 65, 9, 37, 38, 5, 25, 16, 40, 15, 10, 2]\n",
      "\t\tAccuracy: 0.6807344632768362\n",
      "\tAnt 9 :\n",
      "\t\tPath: [67, 12, 63, 52, 50, 9, 59, 38, 23, 16, 46, 45, 7, 37, 65]\n",
      "\t\tAccuracy: 0.7105649717514124\n",
      "\tAnt 10 :\n",
      "\t\tPath: [59, 6, 18, 57, 8, 15, 38, 63, 22, 68, 10, 19, 16, 67, 12]\n",
      "\t\tAccuracy: 0.693502824858757\n",
      "\tAnt 11 :\n",
      "\t\tPath: [59, 9, 11, 38, 65, 22, 51, 67, 2, 40, 48, 33, 23, 18, 63]\n",
      "\t\tAccuracy: 0.6699999999999999\n",
      "\tAnt 12 :\n",
      "\t\tPath: [25, 38, 54, 53, 10, 65, 22, 49, 63, 2, 9, 48, 11, 57, 41]\n",
      "\t\tAccuracy: 0.7273446327683615\n",
      "\tAnt 13 :\n",
      "\t\tPath: [9, 50, 0, 51, 2, 57, 12, 52, 63, 14, 36, 33, 10, 11, 20]\n",
      "\t\tAccuracy: 0.686949152542373\n",
      "\tAnt 14 :\n",
      "\t\tPath: [10, 18, 49, 50, 0, 6, 38, 25, 63, 22, 44, 20, 14, 65, 29]\n",
      "\t\tAccuracy: 0.733954802259887\n",
      "\tAnt 15 :\n",
      "\t\tPath: [46, 50, 28, 52, 59, 37, 51, 67, 43, 58, 9, 53, 18, 54, 33]\n",
      "\t\tAccuracy: 0.6970056497175141\n",
      "\tAnt 16 :\n",
      "\t\tPath: [24, 51, 5, 7, 9, 58, 29, 25, 49, 19, 14, 37, 34, 65, 42]\n",
      "\t\tAccuracy: 0.6699435028248588\n",
      "\tAnt 17 :\n",
      "\t\tPath: [25, 9, 11, 52, 7, 67, 22, 47, 60, 40, 63, 38, 65, 14, 12]\n",
      "\t\tAccuracy: 0.6733333333333333\n",
      "\tAnt 18 :\n",
      "\t\tPath: [11, 44, 12, 8, 65, 43, 21, 38, 63, 67, 42, 53, 16, 57, 30]\n",
      "\t\tAccuracy: 0.6933333333333334\n",
      "\tAnt 19 :\n",
      "\t\tPath: [2, 63, 47, 30, 19, 42, 41, 57, 9, 44, 37, 51, 13, 8, 14]\n",
      "\t\tAccuracy: 0.683276836158192\n",
      "Colony 29 :\n",
      "\tAnt 0 :\n",
      "\t\tPath: [49, 42, 51, 25, 63, 12, 14, 37, 36, 0, 10, 33, 59, 68, 58]\n",
      "\t\tAccuracy: 0.6870056497175142\n",
      "\tAnt 1 :\n",
      "\t\tPath: [53, 33, 52, 58, 49, 41, 43, 16, 30, 19, 50, 59, 28, 9, 67]\n",
      "\t\tAccuracy: 0.693502824858757\n",
      "\tAnt 2 :\n",
      "\t\tPath: [59, 67, 50, 65, 25, 43, 54, 37, 34, 9, 57, 0, 38, 44, 14]\n",
      "\t\tAccuracy: 0.6902259887005651\n",
      "\tAnt 3 :\n",
      "\t\tPath: [31, 8, 58, 51, 10, 53, 52, 49, 48, 37, 20, 57, 56, 43, 59]\n",
      "\t\tAccuracy: 0.7206214689265537\n",
      "\tAnt 4 :\n",
      "\t\tPath: [12, 18, 67, 53, 57, 44, 60, 63, 50, 43, 2, 46, 58, 33, 56]\n",
      "\t\tAccuracy: 0.7405649717514123\n",
      "\tAnt 5 :\n",
      "\t\tPath: [15, 63, 14, 42, 34, 27, 59, 36, 65, 60, 33, 9, 54, 18, 23]\n",
      "\t\tAccuracy: 0.6766666666666666\n",
      "\tAnt 6 :\n",
      "\t\tPath: [33, 9, 59, 38, 19, 18, 53, 16, 52, 40, 65, 42, 44, 45, 57]\n",
      "\t\tAccuracy: 0.6701694915254237\n",
      "\tAnt 7 :\n",
      "\t\tPath: [50, 42, 57, 40, 2, 41, 3, 56, 28, 44, 38, 1, 6, 0, 5]\n",
      "\t\tAccuracy: 0.7174011299435029\n",
      "\tAnt 8 :\n",
      "\t\tPath: [12, 57, 52, 42, 8, 44, 59, 55, 58, 29, 65, 38, 53, 37, 67]\n",
      "\t\tAccuracy: 0.6968926553672317\n",
      "\tAnt 9 :\n",
      "\t\tPath: [28, 67, 65, 21, 46, 37, 22, 33, 44, 19, 30, 18, 52, 63, 16]\n",
      "\t\tAccuracy: 0.7005084745762712\n",
      "\tAnt 10 :\n",
      "\t\tPath: [18, 12, 49, 33, 14, 19, 46, 16, 57, 37, 53, 8, 52, 31, 23]\n",
      "\t\tAccuracy: 0.7105649717514125\n",
      "\tAnt 11 :\n",
      "\t\tPath: [7, 9, 57, 34, 59, 38, 19, 63, 44, 42, 53, 31, 67, 65, 10]\n",
      "\t\tAccuracy: 0.6967796610169492\n",
      "\tAnt 12 :\n",
      "\t\tPath: [38, 57, 0, 67, 33, 36, 20, 34, 18, 50, 52, 13, 23, 14, 27]\n",
      "\t\tAccuracy: 0.6969491525423729\n",
      "\tAnt 13 :\n",
      "\t\tPath: [31, 16, 65, 60, 44, 59, 63, 29, 11, 43, 15, 9, 42, 53, 27]\n",
      "\t\tAccuracy: 0.6935028248587571\n",
      "\tAnt 14 :\n",
      "\t\tPath: [52, 43, 30, 56, 44, 28, 36, 25, 65, 31, 64, 12, 9, 18, 63]\n",
      "\t\tAccuracy: 0.6566666666666666\n",
      "\tAnt 15 :\n",
      "\t\tPath: [11, 14, 15, 30, 19, 9, 40, 18, 63, 57, 42, 2, 10, 53, 38]\n",
      "\t\tAccuracy: 0.7239548022598871\n",
      "\tAnt 16 :\n",
      "\t\tPath: [52, 0, 59, 8, 67, 21, 10, 19, 9, 41, 11, 49, 38, 65, 53]\n",
      "\t\tAccuracy: 0.663276836158192\n",
      "\tAnt 17 :\n",
      "\t\tPath: [50, 57, 29, 11, 41, 56, 43, 27, 65, 49, 59, 12, 67, 18, 53]\n",
      "\t\tAccuracy: 0.6800564971751413\n",
      "\tAnt 18 :\n",
      "\t\tPath: [38, 67, 42, 50, 22, 16, 14, 52, 8, 57, 46, 60, 2, 9, 37]\n",
      "\t\tAccuracy: 0.7439548022598869\n",
      "\tAnt 19 :\n",
      "\t\tPath: [22, 46, 23, 28, 59, 51, 56, 68, 43, 53, 36, 19, 42, 9, 52]\n",
      "\t\tAccuracy: 0.713728813559322\n"
     ]
    }
   ],
   "source": [
    "fs.acoFS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final subset of features is:  [38, 67, 42, 50, 22, 16, 14, 52, 8, 57, 46, 60, 2, 9, 37]\n",
      "Top 5 ants:\n",
      "Path:  [59, 67, 50, 65, 25, 43, 54, 37, 34, 9, 57, 0, 38, 44, 14] \t | IPC:  0.027273528776705677\n",
      "Path:  [11, 14, 15, 30, 19, 9, 40, 18, 63, 57, 42, 2, 10, 53, 38] \t | IPC:  0.026770199638497236\n",
      "Path:  [12, 18, 67, 53, 57, 44, 60, 63, 50, 43, 2, 46, 58, 33, 56] \t | IPC:  0.026065073313322733\n",
      "Path:  [38, 67, 42, 50, 22, 16, 14, 52, 8, 57, 46, 60, 2, 9, 37] \t | IPC:  0.024660340280569404\n",
      "Path:  [7, 9, 57, 34, 59, 38, 19, 63, 44, 42, 53, 31, 67, 65, 10] \t | IPC:  0.02419733026714063\n",
      "Number of features:  15\n",
      "Subset of features dataset accuracy:\n",
      "\t CV-Training set:  0.7439548022598869\n",
      "\t Testing set    :  0.6868686868686869\n",
      "\t Time elapsed reading data        :  0.12134122848510742\n",
      "\t Time elapsed in LUT compute      :  0.4102745056152344\n",
      "\t Time elapsed reseting values     :  7.898557662963867\n",
      "\t Time elapsed in local search     :  11.278206825256348\n",
      "\t Time elapsed updating pheromones :  0.003998517990112305\n",
      "\n",
      "TOTAL AUC FROM MODEL:  0.7765957446808511\n",
      "AUC Score ([59, 67, 50, 65, 25, 43, 54, 37, 34, 9, 57, 0, 38, 44, 14], 0.027273528776705677): 0.6217266775777415\n",
      "AUC Score ([11, 14, 15, 30, 19, 9, 40, 18, 63, 57, 42, 2, 10, 53, 38], 0.026770199638497236): 0.5869476268412439\n",
      "AUC Score ([12, 18, 67, 53, 57, 44, 60, 63, 50, 43, 2, 46, 58, 33, 56], 0.026065073313322733): 0.7037643207855974\n",
      "AUC Score ([38, 67, 42, 50, 22, 16, 14, 52, 8, 57, 46, 60, 2, 9, 37], 0.024660340280569404): 0.7765957446808511\n",
      "AUC Score ([7, 9, 57, 34, 59, 38, 19, 63, 44, 42, 53, 31, 67, 65, 10], 0.02419733026714063): 0.5984042553191489\n"
     ]
    }
   ],
   "source": [
    "fs.printTestingResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38, 67, 42, 50, 22, 16, 14, 52, 8, 57, 46, 60, 2, 9, 37], [12, 18, 67, 53, 57, 44, 60, 63, 50, 43, 2, 46, 58, 33, 56], [11, 14, 15, 30, 19, 9, 40, 18, 63, 57, 42, 2, 10, 53, 38], [31, 8, 58, 51, 10, 53, 52, 49, 48, 37, 20, 57, 56, 43, 59], [50, 42, 57, 40, 2, 41, 3, 56, 28, 44, 38, 1, 6, 0, 5], [22, 46, 23, 28, 59, 51, 56, 68, 43, 53, 36, 19, 42, 9, 52], [18, 12, 49, 33, 14, 19, 46, 16, 57, 37, 53, 8, 52, 31, 23], [28, 67, 65, 21, 46, 37, 22, 33, 44, 19, 30, 18, 52, 63, 16], [38, 57, 0, 67, 33, 36, 20, 34, 18, 50, 52, 13, 23, 14, 27], [12, 57, 52, 42, 8, 44, 59, 55, 58, 29, 65, 38, 53, 37, 67], [7, 9, 57, 34, 59, 38, 19, 63, 44, 42, 53, 31, 67, 65, 10], [31, 16, 65, 60, 44, 59, 63, 29, 11, 43, 15, 9, 42, 53, 27], [53, 33, 52, 58, 49, 41, 43, 16, 30, 19, 50, 59, 28, 9, 67], [59, 67, 50, 65, 25, 43, 54, 37, 34, 9, 57, 0, 38, 44, 14], [49, 42, 51, 25, 63, 12, 14, 37, 36, 0, 10, 33, 59, 68, 58], [50, 57, 29, 11, 41, 56, 43, 27, 65, 49, 59, 12, 67, 18, 53], [15, 63, 14, 42, 34, 27, 59, 36, 65, 60, 33, 9, 54, 18, 23], [33, 9, 59, 38, 19, 18, 53, 16, 52, 40, 65, 42, 44, 45, 57], [52, 0, 59, 8, 67, 21, 10, 19, 9, 41, 11, 49, 38, 65, 53], [52, 43, 30, 56, 44, 28, 36, 25, 65, 31, 64, 12, 9, 18, 63]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[38, 67, 42, 50, 22, 16, 14, 52, 8, 57, 46, 60, 2, 9, 37]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features = fs.selectedPaths()\n",
    "selected_features = selected_features[0]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from classifier import Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# classifier = Classifier(selected_features, './rtfDataSet.csv')\n",
    "# classifier.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision  Recall       AUC  \\\n",
      "0        1            0.1     1  0.517628   0.514106   0.995  0.507763   \n",
      "\n",
      "       Loss  \n",
      "0  0.777548  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1        1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "\n",
      "       Loss  \n",
      "0  0.777548  \n",
      "1  0.544762  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1        1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2        1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "\n",
      "       Loss  \n",
      "0  0.777548  \n",
      "1  0.544762  \n",
      "2  0.519720  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1        1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2        1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3        1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "\n",
      "       Loss  \n",
      "0  0.777548  \n",
      "1  0.544762  \n",
      "2  0.519720  \n",
      "3  0.510739  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1        1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2        1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3        1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4        1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "\n",
      "       Loss  \n",
      "0  0.777548  \n",
      "1  0.544762  \n",
      "2  0.519720  \n",
      "3  0.510739  \n",
      "4  0.519249  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1        1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2        1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3        1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4        1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5        1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "\n",
      "       Loss  \n",
      "0  0.777548  \n",
      "1  0.544762  \n",
      "2  0.519720  \n",
      "3  0.510739  \n",
      "4  0.519249  \n",
      "5  0.519249  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (301) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (301) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (301) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1        1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2        1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3        1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4        1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5        1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6        1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "\n",
      "       Loss  \n",
      "0  0.777548  \n",
      "1  0.544762  \n",
      "2  0.519720  \n",
      "3  0.510739  \n",
      "4  0.519249  \n",
      "5  0.519249  \n",
      "6  0.519249  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (351) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (351) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (351) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1        1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2        1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3        1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4        1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5        1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6        1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7        1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "\n",
      "       Loss  \n",
      "0  0.777548  \n",
      "1  0.544762  \n",
      "2  0.519720  \n",
      "3  0.510739  \n",
      "4  0.519249  \n",
      "5  0.519249  \n",
      "6  0.519249  \n",
      "7  0.519249  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1        1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2        1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3        1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4        1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5        1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6        1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7        1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8        1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "\n",
      "       Loss  \n",
      "0  0.777548  \n",
      "1  0.544762  \n",
      "2  0.519720  \n",
      "3  0.510739  \n",
      "4  0.519249  \n",
      "5  0.519249  \n",
      "6  0.519249  \n",
      "7  0.519249  \n",
      "8  0.519249  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0        1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1        1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2        1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3        1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4        1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5        1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6        1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7        1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8        1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9        1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "\n",
      "       Loss  \n",
      "0  0.777548  \n",
      "1  0.544762  \n",
      "2  0.519720  \n",
      "3  0.510739  \n",
      "4  0.519249  \n",
      "5  0.519249  \n",
      "6  0.519249  \n",
      "7  0.519249  \n",
      "8  0.519249  \n",
      "9  0.519249  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (251) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (301) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (301) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (301) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (301) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (301) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (301) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (351) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (351) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (351) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (351) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (351) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (401) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (401) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (401) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (401) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (401) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (451) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "48        2            0.3   201  0.699167   0.719707  0.706667  0.698728   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n",
      "48  0.489914  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "48        2            0.3   201  0.699167   0.719707  0.706667  0.698728   \n",
      "49        2            0.3   251  0.696667   0.720335  0.696905  0.696479   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n",
      "48  0.489914  \n",
      "49  0.489914  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "48        2            0.3   201  0.699167   0.719707  0.706667  0.698728   \n",
      "49        2            0.3   251  0.696667   0.720335  0.696905  0.696479   \n",
      "50        2            0.3   301  0.696667   0.720335  0.696905  0.696479   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n",
      "48  0.489914  \n",
      "49  0.489914  \n",
      "50  0.489914  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "48        2            0.3   201  0.699167   0.719707  0.706667  0.698728   \n",
      "49        2            0.3   251  0.696667   0.720335  0.696905  0.696479   \n",
      "50        2            0.3   301  0.696667   0.720335  0.696905  0.696479   \n",
      "51        2            0.3   351  0.696667   0.720335  0.696905  0.696479   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n",
      "48  0.489914  \n",
      "49  0.489914  \n",
      "50  0.489914  \n",
      "51  0.489914  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "48        2            0.3   201  0.699167   0.719707  0.706667  0.698728   \n",
      "49        2            0.3   251  0.696667   0.720335  0.696905  0.696479   \n",
      "50        2            0.3   301  0.696667   0.720335  0.696905  0.696479   \n",
      "51        2            0.3   351  0.696667   0.720335  0.696905  0.696479   \n",
      "52        2            0.3   401  0.696667   0.720335  0.696905  0.696479   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n",
      "48  0.489914  \n",
      "49  0.489914  \n",
      "50  0.489914  \n",
      "51  0.489914  \n",
      "52  0.489914  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "48        2            0.3   201  0.699167   0.719707  0.706667  0.698728   \n",
      "49        2            0.3   251  0.696667   0.720335  0.696905  0.696479   \n",
      "50        2            0.3   301  0.696667   0.720335  0.696905  0.696479   \n",
      "51        2            0.3   351  0.696667   0.720335  0.696905  0.696479   \n",
      "52        2            0.3   401  0.696667   0.720335  0.696905  0.696479   \n",
      "53        2            0.3   451  0.696667   0.720335  0.696905  0.696479   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n",
      "48  0.489914  \n",
      "49  0.489914  \n",
      "50  0.489914  \n",
      "51  0.489914  \n",
      "52  0.489914  \n",
      "53  0.489914  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "48        2            0.3   201  0.699167   0.719707  0.706667  0.698728   \n",
      "49        2            0.3   251  0.696667   0.720335  0.696905  0.696479   \n",
      "50        2            0.3   301  0.696667   0.720335  0.696905  0.696479   \n",
      "51        2            0.3   351  0.696667   0.720335  0.696905  0.696479   \n",
      "52        2            0.3   401  0.696667   0.720335  0.696905  0.696479   \n",
      "53        2            0.3   451  0.696667   0.720335  0.696905  0.696479   \n",
      "54        2            0.3   501  0.696667   0.720335  0.696905  0.696479   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n",
      "48  0.489914  \n",
      "49  0.489914  \n",
      "50  0.489914  \n",
      "51  0.489914  \n",
      "52  0.489914  \n",
      "53  0.489914  \n",
      "54  0.489914  \n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "48        2            0.3   201  0.699167   0.719707  0.706667  0.698728   \n",
      "49        2            0.3   251  0.696667   0.720335  0.696905  0.696479   \n",
      "50        2            0.3   301  0.696667   0.720335  0.696905  0.696479   \n",
      "51        2            0.3   351  0.696667   0.720335  0.696905  0.696479   \n",
      "52        2            0.3   401  0.696667   0.720335  0.696905  0.696479   \n",
      "53        2            0.3   451  0.696667   0.720335  0.696905  0.696479   \n",
      "54        2            0.3   501  0.696667   0.720335  0.696905  0.696479   \n",
      "55        2            0.5     1  0.489872   0.000000  0.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n",
      "48  0.489914  \n",
      "49  0.489914  \n",
      "50  0.489914  \n",
      "51  0.489914  \n",
      "52  0.489914  \n",
      "53  0.489914  \n",
      "54  0.489914  \n",
      "55  0.807361  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "48        2            0.3   201  0.699167   0.719707  0.706667  0.698728   \n",
      "49        2            0.3   251  0.696667   0.720335  0.696905  0.696479   \n",
      "50        2            0.3   301  0.696667   0.720335  0.696905  0.696479   \n",
      "51        2            0.3   351  0.696667   0.720335  0.696905  0.696479   \n",
      "52        2            0.3   401  0.696667   0.720335  0.696905  0.696479   \n",
      "53        2            0.3   451  0.696667   0.720335  0.696905  0.696479   \n",
      "54        2            0.3   501  0.696667   0.720335  0.696905  0.696479   \n",
      "55        2            0.5     1  0.489872   0.000000  0.000000  0.500000   \n",
      "56        2            0.5    51  0.739679   0.750087  0.746190  0.739806   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n",
      "48  0.489914  \n",
      "49  0.489914  \n",
      "50  0.489914  \n",
      "51  0.489914  \n",
      "52  0.489914  \n",
      "53  0.489914  \n",
      "54  0.489914  \n",
      "55  0.807361  \n",
      "56  0.536593  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "48        2            0.3   201  0.699167   0.719707  0.706667  0.698728   \n",
      "49        2            0.3   251  0.696667   0.720335  0.696905  0.696479   \n",
      "50        2            0.3   301  0.696667   0.720335  0.696905  0.696479   \n",
      "51        2            0.3   351  0.696667   0.720335  0.696905  0.696479   \n",
      "52        2            0.3   401  0.696667   0.720335  0.696905  0.696479   \n",
      "53        2            0.3   451  0.696667   0.720335  0.696905  0.696479   \n",
      "54        2            0.3   501  0.696667   0.720335  0.696905  0.696479   \n",
      "55        2            0.5     1  0.489872   0.000000  0.000000  0.500000   \n",
      "56        2            0.5    51  0.739679   0.750087  0.746190  0.739806   \n",
      "57        2            0.5   101  0.714487   0.739506  0.683095  0.715495   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n",
      "48  0.489914  \n",
      "49  0.489914  \n",
      "50  0.489914  \n",
      "51  0.489914  \n",
      "52  0.489914  \n",
      "53  0.489914  \n",
      "54  0.489914  \n",
      "55  0.807361  \n",
      "56  0.536593  \n",
      "57  0.485763  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "48        2            0.3   201  0.699167   0.719707  0.706667  0.698728   \n",
      "49        2            0.3   251  0.696667   0.720335  0.696905  0.696479   \n",
      "50        2            0.3   301  0.696667   0.720335  0.696905  0.696479   \n",
      "51        2            0.3   351  0.696667   0.720335  0.696905  0.696479   \n",
      "52        2            0.3   401  0.696667   0.720335  0.696905  0.696479   \n",
      "53        2            0.3   451  0.696667   0.720335  0.696905  0.696479   \n",
      "54        2            0.3   501  0.696667   0.720335  0.696905  0.696479   \n",
      "55        2            0.5     1  0.489872   0.000000  0.000000  0.500000   \n",
      "56        2            0.5    51  0.739679   0.750087  0.746190  0.739806   \n",
      "57        2            0.5   101  0.714487   0.739506  0.683095  0.715495   \n",
      "58        2            0.5   151  0.706987   0.717968  0.716667  0.706623   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n",
      "48  0.489914  \n",
      "49  0.489914  \n",
      "50  0.489914  \n",
      "51  0.489914  \n",
      "52  0.489914  \n",
      "53  0.489914  \n",
      "54  0.489914  \n",
      "55  0.807361  \n",
      "56  0.536593  \n",
      "57  0.485763  \n",
      "58  0.485763  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "5         1            0.1   251  0.724679   0.734664  0.730952  0.723634   \n",
      "6         1            0.1   301  0.717115   0.715411  0.755952  0.715871   \n",
      "7         1            0.1   351  0.724679   0.717408  0.770952  0.723503   \n",
      "8         1            0.1   401  0.727179   0.730698  0.746905  0.727268   \n",
      "9         1            0.1   451  0.727179   0.730698  0.746905  0.727268   \n",
      "10        1            0.1   501  0.727179   0.730698  0.746905  0.727268   \n",
      "11        1            0.3     1  0.472179   0.377143  0.084286  0.480301   \n",
      "12        1            0.3    51  0.696667   0.700778  0.731429  0.696372   \n",
      "13        1            0.3   101  0.722308   0.710796  0.781429  0.720583   \n",
      "14        1            0.3   151  0.717244   0.706912  0.775714  0.715620   \n",
      "15        1            0.3   201  0.714744   0.705835  0.770952  0.713239   \n",
      "16        1            0.3   251  0.714744   0.705835  0.770952  0.713239   \n",
      "17        1            0.3   301  0.714744   0.705835  0.770952  0.713239   \n",
      "18        1            0.3   351  0.714744   0.705835  0.770952  0.713239   \n",
      "19        1            0.3   401  0.714744   0.705835  0.770952  0.713239   \n",
      "20        1            0.3   451  0.714744   0.705835  0.770952  0.713239   \n",
      "21        1            0.3   501  0.714744   0.705835  0.770952  0.713239   \n",
      "22        1            0.5     1  0.482244   0.233333  0.029762  0.491723   \n",
      "23        1            0.5    51  0.712115   0.710230  0.765952  0.710871   \n",
      "24        1            0.5   101  0.729808   0.725326  0.781429  0.727951   \n",
      "25        1            0.5   151  0.734808   0.710033  0.831667  0.733070   \n",
      "26        1            0.5   201  0.734808   0.710033  0.831667  0.733070   \n",
      "27        1            0.5   251  0.734808   0.710033  0.831667  0.733070   \n",
      "28        1            0.5   301  0.734808   0.710033  0.831667  0.733070   \n",
      "29        1            0.5   351  0.734808   0.710033  0.831667  0.733070   \n",
      "30        1            0.5   401  0.734808   0.710033  0.831667  0.733070   \n",
      "31        1            0.5   451  0.734808   0.710033  0.831667  0.733070   \n",
      "32        1            0.5   501  0.734808   0.710033  0.831667  0.733070   \n",
      "33        2            0.1     1  0.563269   0.539613  0.995000  0.554342   \n",
      "34        2            0.1    51  0.712308   0.705733  0.746667  0.711228   \n",
      "35        2            0.1   101  0.734615   0.706869  0.821429  0.732951   \n",
      "36        2            0.1   151  0.754808   0.758938  0.765952  0.753897   \n",
      "37        2            0.1   201  0.759744   0.758797  0.780952  0.759292   \n",
      "38        2            0.1   251  0.757372   0.754946  0.800714  0.756673   \n",
      "39        2            0.1   301  0.749744   0.778055  0.720952  0.749292   \n",
      "40        2            0.1   351  0.749744   0.765680  0.740952  0.749424   \n",
      "41        2            0.1   401  0.757308   0.775885  0.746190  0.757043   \n",
      "42        2            0.1   451  0.744744   0.761059  0.736667  0.744781   \n",
      "43        2            0.1   501  0.747244   0.762142  0.741429  0.747162   \n",
      "44        2            0.3     1  0.565705   0.584412  0.503333  0.571009   \n",
      "45        2            0.3    51  0.747179   0.761170  0.746190  0.747174   \n",
      "46        2            0.3   101  0.731795   0.750203  0.716667  0.731886   \n",
      "47        2            0.3   151  0.704231   0.751092  0.637143  0.704624   \n",
      "48        2            0.3   201  0.699167   0.719707  0.706667  0.698728   \n",
      "49        2            0.3   251  0.696667   0.720335  0.696905  0.696479   \n",
      "50        2            0.3   301  0.696667   0.720335  0.696905  0.696479   \n",
      "51        2            0.3   351  0.696667   0.720335  0.696905  0.696479   \n",
      "52        2            0.3   401  0.696667   0.720335  0.696905  0.696479   \n",
      "53        2            0.3   451  0.696667   0.720335  0.696905  0.696479   \n",
      "54        2            0.3   501  0.696667   0.720335  0.696905  0.696479   \n",
      "55        2            0.5     1  0.489872   0.000000  0.000000  0.500000   \n",
      "56        2            0.5    51  0.739679   0.750087  0.746190  0.739806   \n",
      "57        2            0.5   101  0.714487   0.739506  0.683095  0.715495   \n",
      "58        2            0.5   151  0.706987   0.717968  0.716667  0.706623   \n",
      "59        2            0.5   201  0.714487   0.726497  0.716667  0.714518   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "5   0.519249  \n",
      "6   0.519249  \n",
      "7   0.519249  \n",
      "8   0.519249  \n",
      "9   0.519249  \n",
      "10  0.519249  \n",
      "11  0.764375  \n",
      "12  0.545873  \n",
      "13  0.542530  \n",
      "14  0.542530  \n",
      "15  0.542530  \n",
      "16  0.542530  \n",
      "17  0.542530  \n",
      "18  0.542530  \n",
      "19  0.542530  \n",
      "20  0.542530  \n",
      "21  0.542530  \n",
      "22  0.771907  \n",
      "23  0.533578  \n",
      "24  0.493937  \n",
      "25  0.492920  \n",
      "26  0.492920  \n",
      "27  0.492920  \n",
      "28  0.492920  \n",
      "29  0.492920  \n",
      "30  0.492920  \n",
      "31  0.492920  \n",
      "32  0.492920  \n",
      "33  0.721249  \n",
      "34  0.537762  \n",
      "35  0.488094  \n",
      "36  0.464884  \n",
      "37  0.419301  \n",
      "38  0.419301  \n",
      "39  0.419301  \n",
      "40  0.419301  \n",
      "41  0.419301  \n",
      "42  0.419301  \n",
      "43  0.419301  \n",
      "44  0.736818  \n",
      "45  0.523151  \n",
      "46  0.489914  \n",
      "47  0.489914  \n",
      "48  0.489914  \n",
      "49  0.489914  \n",
      "50  0.489914  \n",
      "51  0.489914  \n",
      "52  0.489914  \n",
      "53  0.489914  \n",
      "54  0.489914  \n",
      "55  0.807361  \n",
      "56  0.536593  \n",
      "57  0.485763  \n",
      "58  0.485763  \n",
      "59  0.485763  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "56        2            0.5    51  0.739679   0.750087  0.746190  0.739806   \n",
      "57        2            0.5   101  0.714487   0.739506  0.683095  0.715495   \n",
      "58        2            0.5   151  0.706987   0.717968  0.716667  0.706623   \n",
      "59        2            0.5   201  0.714487   0.726497  0.716667  0.714518   \n",
      "60        2            0.5   251  0.714487   0.726497  0.716667  0.714518   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "56  0.536593  \n",
      "57  0.485763  \n",
      "58  0.485763  \n",
      "59  0.485763  \n",
      "60  0.485763  \n",
      "\n",
      "[61 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "57        2            0.5   101  0.714487   0.739506  0.683095  0.715495   \n",
      "58        2            0.5   151  0.706987   0.717968  0.716667  0.706623   \n",
      "59        2            0.5   201  0.714487   0.726497  0.716667  0.714518   \n",
      "60        2            0.5   251  0.714487   0.726497  0.716667  0.714518   \n",
      "61        2            0.5   301  0.714487   0.726497  0.716667  0.714518   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "57  0.485763  \n",
      "58  0.485763  \n",
      "59  0.485763  \n",
      "60  0.485763  \n",
      "61  0.485763  \n",
      "\n",
      "[62 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "58        2            0.5   151  0.706987   0.717968  0.716667  0.706623   \n",
      "59        2            0.5   201  0.714487   0.726497  0.716667  0.714518   \n",
      "60        2            0.5   251  0.714487   0.726497  0.716667  0.714518   \n",
      "61        2            0.5   301  0.714487   0.726497  0.716667  0.714518   \n",
      "62        2            0.5   351  0.714487   0.726497  0.716667  0.714518   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "58  0.485763  \n",
      "59  0.485763  \n",
      "60  0.485763  \n",
      "61  0.485763  \n",
      "62  0.485763  \n",
      "\n",
      "[63 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "59        2            0.5   201  0.714487   0.726497  0.716667  0.714518   \n",
      "60        2            0.5   251  0.714487   0.726497  0.716667  0.714518   \n",
      "61        2            0.5   301  0.714487   0.726497  0.716667  0.714518   \n",
      "62        2            0.5   351  0.714487   0.726497  0.716667  0.714518   \n",
      "63        2            0.5   401  0.714487   0.726497  0.716667  0.714518   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "59  0.485763  \n",
      "60  0.485763  \n",
      "61  0.485763  \n",
      "62  0.485763  \n",
      "63  0.485763  \n",
      "\n",
      "[64 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "60        2            0.5   251  0.714487   0.726497  0.716667  0.714518   \n",
      "61        2            0.5   301  0.714487   0.726497  0.716667  0.714518   \n",
      "62        2            0.5   351  0.714487   0.726497  0.716667  0.714518   \n",
      "63        2            0.5   401  0.714487   0.726497  0.716667  0.714518   \n",
      "64        2            0.5   451  0.714487   0.726497  0.716667  0.714518   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "60  0.485763  \n",
      "61  0.485763  \n",
      "62  0.485763  \n",
      "63  0.485763  \n",
      "64  0.485763  \n",
      "\n",
      "[65 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "61        2            0.5   301  0.714487   0.726497  0.716667  0.714518   \n",
      "62        2            0.5   351  0.714487   0.726497  0.716667  0.714518   \n",
      "63        2            0.5   401  0.714487   0.726497  0.716667  0.714518   \n",
      "64        2            0.5   451  0.714487   0.726497  0.716667  0.714518   \n",
      "65        2            0.5   501  0.714487   0.726497  0.716667  0.714518   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "61  0.485763  \n",
      "62  0.485763  \n",
      "63  0.485763  \n",
      "64  0.485763  \n",
      "65  0.485763  \n",
      "\n",
      "[66 rows x 8 columns]\n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "62        2            0.5   351  0.714487   0.726497  0.716667  0.714518   \n",
      "63        2            0.5   401  0.714487   0.726497  0.716667  0.714518   \n",
      "64        2            0.5   451  0.714487   0.726497  0.716667  0.714518   \n",
      "65        2            0.5   501  0.714487   0.726497  0.716667  0.714518   \n",
      "66        3            0.1     1  0.489872   0.000000  0.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "62  0.485763  \n",
      "63  0.485763  \n",
      "64  0.485763  \n",
      "65  0.485763  \n",
      "66  0.722062  \n",
      "\n",
      "[67 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "63        2            0.5   401  0.714487   0.726497  0.716667  0.714518   \n",
      "64        2            0.5   451  0.714487   0.726497  0.716667  0.714518   \n",
      "65        2            0.5   501  0.714487   0.726497  0.716667  0.714518   \n",
      "66        3            0.1     1  0.489872   0.000000  0.000000  0.500000   \n",
      "67        3            0.1    51  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "63  0.485763  \n",
      "64  0.485763  \n",
      "65  0.485763  \n",
      "66  0.722062  \n",
      "67  0.693128  \n",
      "\n",
      "[68 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "64        2            0.5   451  0.714487   0.726497  0.716667  0.714518   \n",
      "65        2            0.5   501  0.714487   0.726497  0.716667  0.714518   \n",
      "66        3            0.1     1  0.489872   0.000000  0.000000  0.500000   \n",
      "67        3            0.1    51  0.510128   0.510128  1.000000  0.500000   \n",
      "68        3            0.1   101  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "64  0.485763  \n",
      "65  0.485763  \n",
      "66  0.722062  \n",
      "67  0.693128  \n",
      "68  0.693128  \n",
      "\n",
      "[69 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "65        2            0.5   501  0.714487   0.726497  0.716667  0.714518   \n",
      "66        3            0.1     1  0.489872   0.000000  0.000000  0.500000   \n",
      "67        3            0.1    51  0.510128   0.510128  1.000000  0.500000   \n",
      "68        3            0.1   101  0.510128   0.510128  1.000000  0.500000   \n",
      "69        3            0.1   151  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "65  0.485763  \n",
      "66  0.722062  \n",
      "67  0.693128  \n",
      "68  0.693128  \n",
      "69  0.693128  \n",
      "\n",
      "[70 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "66        3            0.1     1  0.489872   0.000000  0.000000  0.500000   \n",
      "67        3            0.1    51  0.510128   0.510128  1.000000  0.500000   \n",
      "68        3            0.1   101  0.510128   0.510128  1.000000  0.500000   \n",
      "69        3            0.1   151  0.510128   0.510128  1.000000  0.500000   \n",
      "70        3            0.1   201  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "66  0.722062  \n",
      "67  0.693128  \n",
      "68  0.693128  \n",
      "69  0.693128  \n",
      "70  0.693128  \n",
      "\n",
      "[71 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "67        3            0.1    51  0.510128   0.510128  1.000000  0.500000   \n",
      "68        3            0.1   101  0.510128   0.510128  1.000000  0.500000   \n",
      "69        3            0.1   151  0.510128   0.510128  1.000000  0.500000   \n",
      "70        3            0.1   201  0.510128   0.510128  1.000000  0.500000   \n",
      "71        3            0.1   251  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "67  0.693128  \n",
      "68  0.693128  \n",
      "69  0.693128  \n",
      "70  0.693128  \n",
      "71  0.693128  \n",
      "\n",
      "[72 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "68        3            0.1   101  0.510128   0.510128  1.000000  0.500000   \n",
      "69        3            0.1   151  0.510128   0.510128  1.000000  0.500000   \n",
      "70        3            0.1   201  0.510128   0.510128  1.000000  0.500000   \n",
      "71        3            0.1   251  0.510128   0.510128  1.000000  0.500000   \n",
      "72        3            0.1   301  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "68  0.693128  \n",
      "69  0.693128  \n",
      "70  0.693128  \n",
      "71  0.693128  \n",
      "72  0.693128  \n",
      "\n",
      "[73 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "69        3            0.1   151  0.510128   0.510128  1.000000  0.500000   \n",
      "70        3            0.1   201  0.510128   0.510128  1.000000  0.500000   \n",
      "71        3            0.1   251  0.510128   0.510128  1.000000  0.500000   \n",
      "72        3            0.1   301  0.510128   0.510128  1.000000  0.500000   \n",
      "73        3            0.1   351  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "69  0.693128  \n",
      "70  0.693128  \n",
      "71  0.693128  \n",
      "72  0.693128  \n",
      "73  0.693128  \n",
      "\n",
      "[74 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "70        3            0.1   201  0.510128   0.510128  1.000000  0.500000   \n",
      "71        3            0.1   251  0.510128   0.510128  1.000000  0.500000   \n",
      "72        3            0.1   301  0.510128   0.510128  1.000000  0.500000   \n",
      "73        3            0.1   351  0.510128   0.510128  1.000000  0.500000   \n",
      "74        3            0.1   401  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "70  0.693128  \n",
      "71  0.693128  \n",
      "72  0.693128  \n",
      "73  0.693128  \n",
      "74  0.693128  \n",
      "\n",
      "[75 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "71        3            0.1   251  0.510128   0.510128  1.000000  0.500000   \n",
      "72        3            0.1   301  0.510128   0.510128  1.000000  0.500000   \n",
      "73        3            0.1   351  0.510128   0.510128  1.000000  0.500000   \n",
      "74        3            0.1   401  0.510128   0.510128  1.000000  0.500000   \n",
      "75        3            0.1   451  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "71  0.693128  \n",
      "72  0.693128  \n",
      "73  0.693128  \n",
      "74  0.693128  \n",
      "75  0.693128  \n",
      "\n",
      "[76 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "72        3            0.1   301  0.510128   0.510128  1.000000  0.500000   \n",
      "73        3            0.1   351  0.510128   0.510128  1.000000  0.500000   \n",
      "74        3            0.1   401  0.510128   0.510128  1.000000  0.500000   \n",
      "75        3            0.1   451  0.510128   0.510128  1.000000  0.500000   \n",
      "76        3            0.1   501  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "72  0.693128  \n",
      "73  0.693128  \n",
      "74  0.693128  \n",
      "75  0.693128  \n",
      "76  0.693128  \n",
      "\n",
      "[77 rows x 8 columns]\n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "73        3            0.1   351  0.510128   0.510128  1.000000  0.500000   \n",
      "74        3            0.1   401  0.510128   0.510128  1.000000  0.500000   \n",
      "75        3            0.1   451  0.510128   0.510128  1.000000  0.500000   \n",
      "76        3            0.1   501  0.510128   0.510128  1.000000  0.500000   \n",
      "77        3            0.3     1  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "73  0.693128  \n",
      "74  0.693128  \n",
      "75  0.693128  \n",
      "76  0.693128  \n",
      "77  0.712408  \n",
      "\n",
      "[78 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "74        3            0.1   401  0.510128   0.510128  1.000000  0.500000   \n",
      "75        3            0.1   451  0.510128   0.510128  1.000000  0.500000   \n",
      "76        3            0.1   501  0.510128   0.510128  1.000000  0.500000   \n",
      "77        3            0.3     1  0.510128   0.510128  1.000000  0.500000   \n",
      "78        3            0.3    51  0.502436   0.356282  0.700000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "74  0.693128  \n",
      "75  0.693128  \n",
      "76  0.693128  \n",
      "77  0.712408  \n",
      "78  0.692922  \n",
      "\n",
      "[79 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "75        3            0.1   451  0.510128   0.510128  1.000000  0.500000   \n",
      "76        3            0.1   501  0.510128   0.510128  1.000000  0.500000   \n",
      "77        3            0.3     1  0.510128   0.510128  1.000000  0.500000   \n",
      "78        3            0.3    51  0.502436   0.356282  0.700000  0.500000   \n",
      "79        3            0.3   101  0.502436   0.356282  0.700000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "75  0.693128  \n",
      "76  0.693128  \n",
      "77  0.712408  \n",
      "78  0.692922  \n",
      "79  0.692922  \n",
      "\n",
      "[80 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "76        3            0.1   501  0.510128   0.510128  1.000000  0.500000   \n",
      "77        3            0.3     1  0.510128   0.510128  1.000000  0.500000   \n",
      "78        3            0.3    51  0.502436   0.356282  0.700000  0.500000   \n",
      "79        3            0.3   101  0.502436   0.356282  0.700000  0.500000   \n",
      "80        3            0.3   151  0.502436   0.356282  0.700000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "76  0.693128  \n",
      "77  0.712408  \n",
      "78  0.692922  \n",
      "79  0.692922  \n",
      "80  0.692922  \n",
      "\n",
      "[81 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "77        3            0.3     1  0.510128   0.510128  1.000000  0.500000   \n",
      "78        3            0.3    51  0.502436   0.356282  0.700000  0.500000   \n",
      "79        3            0.3   101  0.502436   0.356282  0.700000  0.500000   \n",
      "80        3            0.3   151  0.502436   0.356282  0.700000  0.500000   \n",
      "81        3            0.3   201  0.502436   0.356282  0.700000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "77  0.712408  \n",
      "78  0.692922  \n",
      "79  0.692922  \n",
      "80  0.692922  \n",
      "81  0.692922  \n",
      "\n",
      "[82 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "78        3            0.3    51  0.502436   0.356282  0.700000  0.500000   \n",
      "79        3            0.3   101  0.502436   0.356282  0.700000  0.500000   \n",
      "80        3            0.3   151  0.502436   0.356282  0.700000  0.500000   \n",
      "81        3            0.3   201  0.502436   0.356282  0.700000  0.500000   \n",
      "82        3            0.3   251  0.502436   0.356282  0.700000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "78  0.692922  \n",
      "79  0.692922  \n",
      "80  0.692922  \n",
      "81  0.692922  \n",
      "82  0.692922  \n",
      "\n",
      "[83 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "79        3            0.3   101  0.502436   0.356282  0.700000  0.500000   \n",
      "80        3            0.3   151  0.502436   0.356282  0.700000  0.500000   \n",
      "81        3            0.3   201  0.502436   0.356282  0.700000  0.500000   \n",
      "82        3            0.3   251  0.502436   0.356282  0.700000  0.500000   \n",
      "83        3            0.3   301  0.502436   0.356282  0.700000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "79  0.692922  \n",
      "80  0.692922  \n",
      "81  0.692922  \n",
      "82  0.692922  \n",
      "83  0.692922  \n",
      "\n",
      "[84 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "80        3            0.3   151  0.502436   0.356282  0.700000  0.500000   \n",
      "81        3            0.3   201  0.502436   0.356282  0.700000  0.500000   \n",
      "82        3            0.3   251  0.502436   0.356282  0.700000  0.500000   \n",
      "83        3            0.3   301  0.502436   0.356282  0.700000  0.500000   \n",
      "84        3            0.3   351  0.502436   0.356282  0.700000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "80  0.692922  \n",
      "81  0.692922  \n",
      "82  0.692922  \n",
      "83  0.692922  \n",
      "84  0.692922  \n",
      "\n",
      "[85 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "81        3            0.3   201  0.502436   0.356282  0.700000  0.500000   \n",
      "82        3            0.3   251  0.502436   0.356282  0.700000  0.500000   \n",
      "83        3            0.3   301  0.502436   0.356282  0.700000  0.500000   \n",
      "84        3            0.3   351  0.502436   0.356282  0.700000  0.500000   \n",
      "85        3            0.3   401  0.502436   0.356282  0.700000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "81  0.692922  \n",
      "82  0.692922  \n",
      "83  0.692922  \n",
      "84  0.692922  \n",
      "85  0.692922  \n",
      "\n",
      "[86 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "82        3            0.3   251  0.502436   0.356282  0.700000  0.500000   \n",
      "83        3            0.3   301  0.502436   0.356282  0.700000  0.500000   \n",
      "84        3            0.3   351  0.502436   0.356282  0.700000  0.500000   \n",
      "85        3            0.3   401  0.502436   0.356282  0.700000  0.500000   \n",
      "86        3            0.3   451  0.502436   0.356282  0.700000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "82  0.692922  \n",
      "83  0.692922  \n",
      "84  0.692922  \n",
      "85  0.692922  \n",
      "86  0.692922  \n",
      "\n",
      "[87 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "83        3            0.3   301  0.502436   0.356282  0.700000  0.500000   \n",
      "84        3            0.3   351  0.502436   0.356282  0.700000  0.500000   \n",
      "85        3            0.3   401  0.502436   0.356282  0.700000  0.500000   \n",
      "86        3            0.3   451  0.502436   0.356282  0.700000  0.500000   \n",
      "87        3            0.3   501  0.502436   0.356282  0.700000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "83  0.692922  \n",
      "84  0.692922  \n",
      "85  0.692922  \n",
      "86  0.692922  \n",
      "87  0.692922  \n",
      "\n",
      "[88 rows x 8 columns]\n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "84        3            0.3   351  0.502436   0.356282  0.700000  0.500000   \n",
      "85        3            0.3   401  0.502436   0.356282  0.700000  0.500000   \n",
      "86        3            0.3   451  0.502436   0.356282  0.700000  0.500000   \n",
      "87        3            0.3   501  0.502436   0.356282  0.700000  0.500000   \n",
      "88        3            0.5     1  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "84  0.692922  \n",
      "85  0.692922  \n",
      "86  0.692922  \n",
      "87  0.692922  \n",
      "88  0.708437  \n",
      "\n",
      "[89 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "85        3            0.3   401  0.502436   0.356282  0.700000  0.500000   \n",
      "86        3            0.3   451  0.502436   0.356282  0.700000  0.500000   \n",
      "87        3            0.3   501  0.502436   0.356282  0.700000  0.500000   \n",
      "88        3            0.5     1  0.510128   0.510128  1.000000  0.500000   \n",
      "89        3            0.5    51  0.500000   0.355064  0.605000  0.499868   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "85  0.692922  \n",
      "86  0.692922  \n",
      "87  0.692922  \n",
      "88  0.708437  \n",
      "89  0.694568  \n",
      "\n",
      "[90 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "86        3            0.3   451  0.502436   0.356282  0.700000  0.500000   \n",
      "87        3            0.3   501  0.502436   0.356282  0.700000  0.500000   \n",
      "88        3            0.5     1  0.510128   0.510128  1.000000  0.500000   \n",
      "89        3            0.5    51  0.500000   0.355064  0.605000  0.499868   \n",
      "90        3            0.5   101  0.500000   0.355064  0.605000  0.499868   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "86  0.692922  \n",
      "87  0.692922  \n",
      "88  0.708437  \n",
      "89  0.694568  \n",
      "90  0.694568  \n",
      "\n",
      "[91 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "87        3            0.3   501  0.502436   0.356282  0.700000  0.500000   \n",
      "88        3            0.5     1  0.510128   0.510128  1.000000  0.500000   \n",
      "89        3            0.5    51  0.500000   0.355064  0.605000  0.499868   \n",
      "90        3            0.5   101  0.500000   0.355064  0.605000  0.499868   \n",
      "91        3            0.5   151  0.500000   0.355064  0.605000  0.499868   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "87  0.692922  \n",
      "88  0.708437  \n",
      "89  0.694568  \n",
      "90  0.694568  \n",
      "91  0.694568  \n",
      "\n",
      "[92 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "88        3            0.5     1  0.510128   0.510128  1.000000  0.500000   \n",
      "89        3            0.5    51  0.500000   0.355064  0.605000  0.499868   \n",
      "90        3            0.5   101  0.500000   0.355064  0.605000  0.499868   \n",
      "91        3            0.5   151  0.500000   0.355064  0.605000  0.499868   \n",
      "92        3            0.5   201  0.500000   0.355064  0.605000  0.499868   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "88  0.708437  \n",
      "89  0.694568  \n",
      "90  0.694568  \n",
      "91  0.694568  \n",
      "92  0.694568  \n",
      "\n",
      "[93 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "89        3            0.5    51  0.500000   0.355064  0.605000  0.499868   \n",
      "90        3            0.5   101  0.500000   0.355064  0.605000  0.499868   \n",
      "91        3            0.5   151  0.500000   0.355064  0.605000  0.499868   \n",
      "92        3            0.5   201  0.500000   0.355064  0.605000  0.499868   \n",
      "93        3            0.5   251  0.500000   0.355064  0.605000  0.499868   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "89  0.694568  \n",
      "90  0.694568  \n",
      "91  0.694568  \n",
      "92  0.694568  \n",
      "93  0.694568  \n",
      "\n",
      "[94 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "90        3            0.5   101  0.500000   0.355064  0.605000  0.499868   \n",
      "91        3            0.5   151  0.500000   0.355064  0.605000  0.499868   \n",
      "92        3            0.5   201  0.500000   0.355064  0.605000  0.499868   \n",
      "93        3            0.5   251  0.500000   0.355064  0.605000  0.499868   \n",
      "94        3            0.5   301  0.500000   0.355064  0.605000  0.499868   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "90  0.694568  \n",
      "91  0.694568  \n",
      "92  0.694568  \n",
      "93  0.694568  \n",
      "94  0.694568  \n",
      "\n",
      "[95 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "91        3            0.5   151  0.500000   0.355064  0.605000  0.499868   \n",
      "92        3            0.5   201  0.500000   0.355064  0.605000  0.499868   \n",
      "93        3            0.5   251  0.500000   0.355064  0.605000  0.499868   \n",
      "94        3            0.5   301  0.500000   0.355064  0.605000  0.499868   \n",
      "95        3            0.5   351  0.500000   0.355064  0.605000  0.499868   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "91  0.694568  \n",
      "92  0.694568  \n",
      "93  0.694568  \n",
      "94  0.694568  \n",
      "95  0.694568  \n",
      "\n",
      "[96 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "92        3            0.5   201  0.500000   0.355064  0.605000  0.499868   \n",
      "93        3            0.5   251  0.500000   0.355064  0.605000  0.499868   \n",
      "94        3            0.5   301  0.500000   0.355064  0.605000  0.499868   \n",
      "95        3            0.5   351  0.500000   0.355064  0.605000  0.499868   \n",
      "96        3            0.5   401  0.500000   0.355064  0.605000  0.499868   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "92  0.694568  \n",
      "93  0.694568  \n",
      "94  0.694568  \n",
      "95  0.694568  \n",
      "96  0.694568  \n",
      "\n",
      "[97 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "93        3            0.5   251  0.500000   0.355064  0.605000  0.499868   \n",
      "94        3            0.5   301  0.500000   0.355064  0.605000  0.499868   \n",
      "95        3            0.5   351  0.500000   0.355064  0.605000  0.499868   \n",
      "96        3            0.5   401  0.500000   0.355064  0.605000  0.499868   \n",
      "97        3            0.5   451  0.500000   0.355064  0.605000  0.499868   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "93  0.694568  \n",
      "94  0.694568  \n",
      "95  0.694568  \n",
      "96  0.694568  \n",
      "97  0.694568  \n",
      "\n",
      "[98 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "94        3            0.5   301  0.500000   0.355064  0.605000  0.499868   \n",
      "95        3            0.5   351  0.500000   0.355064  0.605000  0.499868   \n",
      "96        3            0.5   401  0.500000   0.355064  0.605000  0.499868   \n",
      "97        3            0.5   451  0.500000   0.355064  0.605000  0.499868   \n",
      "98        3            0.5   501  0.500000   0.355064  0.605000  0.499868   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "94  0.694568  \n",
      "95  0.694568  \n",
      "96  0.694568  \n",
      "97  0.694568  \n",
      "98  0.694568  \n",
      "\n",
      "[99 rows x 8 columns]\n",
      "   Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0         1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1         1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2         1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3         1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4         1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..      ...            ...   ...       ...        ...       ...       ...   \n",
      "95        3            0.5   351  0.500000   0.355064  0.605000  0.499868   \n",
      "96        3            0.5   401  0.500000   0.355064  0.605000  0.499868   \n",
      "97        3            0.5   451  0.500000   0.355064  0.605000  0.499868   \n",
      "98        3            0.5   501  0.500000   0.355064  0.605000  0.499868   \n",
      "99        4            0.1     1  0.510128   0.510128  1.000000  0.500000   \n",
      "\n",
      "        Loss  \n",
      "0   0.777548  \n",
      "1   0.544762  \n",
      "2   0.519720  \n",
      "3   0.510739  \n",
      "4   0.519249  \n",
      "..       ...  \n",
      "95  0.694568  \n",
      "96  0.694568  \n",
      "97  0.694568  \n",
      "98  0.694568  \n",
      "99  0.698285  \n",
      "\n",
      "[100 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "96         3            0.5   401  0.500000   0.355064  0.605000  0.499868   \n",
      "97         3            0.5   451  0.500000   0.355064  0.605000  0.499868   \n",
      "98         3            0.5   501  0.500000   0.355064  0.605000  0.499868   \n",
      "99         4            0.1     1  0.510128   0.510128  1.000000  0.500000   \n",
      "100        4            0.1    51  0.742564   0.714440  0.835714  0.740357   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "96   0.694568  \n",
      "97   0.694568  \n",
      "98   0.694568  \n",
      "99   0.698285  \n",
      "100  0.489149  \n",
      "\n",
      "[101 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "97         3            0.5   451  0.500000   0.355064  0.605000  0.499868   \n",
      "98         3            0.5   501  0.500000   0.355064  0.605000  0.499868   \n",
      "99         4            0.1     1  0.510128   0.510128  1.000000  0.500000   \n",
      "100        4            0.1    51  0.742564   0.714440  0.835714  0.740357   \n",
      "101        4            0.1   101  0.747051   0.750293  0.767619  0.746967   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "97   0.694568  \n",
      "98   0.694568  \n",
      "99   0.698285  \n",
      "100  0.489149  \n",
      "101  0.432891  \n",
      "\n",
      "[102 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "98         3            0.5   501  0.500000   0.355064  0.605000  0.499868   \n",
      "99         4            0.1     1  0.510128   0.510128  1.000000  0.500000   \n",
      "100        4            0.1    51  0.742564   0.714440  0.835714  0.740357   \n",
      "101        4            0.1   101  0.747051   0.750293  0.767619  0.746967   \n",
      "102        4            0.1   151  0.754551   0.755620  0.782143  0.754492   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "98   0.694568  \n",
      "99   0.698285  \n",
      "100  0.489149  \n",
      "101  0.432891  \n",
      "102  0.432891  \n",
      "\n",
      "[103 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "99         4            0.1     1  0.510128   0.510128  1.000000  0.500000   \n",
      "100        4            0.1    51  0.742564   0.714440  0.835714  0.740357   \n",
      "101        4            0.1   101  0.747051   0.750293  0.767619  0.746967   \n",
      "102        4            0.1   151  0.754551   0.755620  0.782143  0.754492   \n",
      "103        4            0.1   201  0.754551   0.755620  0.782143  0.754492   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "99   0.698285  \n",
      "100  0.489149  \n",
      "101  0.432891  \n",
      "102  0.432891  \n",
      "103  0.432891  \n",
      "\n",
      "[104 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "100        4            0.1    51  0.742564   0.714440  0.835714  0.740357   \n",
      "101        4            0.1   101  0.747051   0.750293  0.767619  0.746967   \n",
      "102        4            0.1   151  0.754551   0.755620  0.782143  0.754492   \n",
      "103        4            0.1   201  0.754551   0.755620  0.782143  0.754492   \n",
      "104        4            0.1   251  0.754551   0.755620  0.782143  0.754492   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "100  0.489149  \n",
      "101  0.432891  \n",
      "102  0.432891  \n",
      "103  0.432891  \n",
      "104  0.432891  \n",
      "\n",
      "[105 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "101        4            0.1   101  0.747051   0.750293  0.767619  0.746967   \n",
      "102        4            0.1   151  0.754551   0.755620  0.782143  0.754492   \n",
      "103        4            0.1   201  0.754551   0.755620  0.782143  0.754492   \n",
      "104        4            0.1   251  0.754551   0.755620  0.782143  0.754492   \n",
      "105        4            0.1   301  0.754551   0.755620  0.782143  0.754492   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "101  0.432891  \n",
      "102  0.432891  \n",
      "103  0.432891  \n",
      "104  0.432891  \n",
      "105  0.432891  \n",
      "\n",
      "[106 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "102        4            0.1   151  0.754551   0.755620  0.782143  0.754492   \n",
      "103        4            0.1   201  0.754551   0.755620  0.782143  0.754492   \n",
      "104        4            0.1   251  0.754551   0.755620  0.782143  0.754492   \n",
      "105        4            0.1   301  0.754551   0.755620  0.782143  0.754492   \n",
      "106        4            0.1   351  0.754551   0.755620  0.782143  0.754492   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "102  0.432891  \n",
      "103  0.432891  \n",
      "104  0.432891  \n",
      "105  0.432891  \n",
      "106  0.432891  \n",
      "\n",
      "[107 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "103        4            0.1   201  0.754551   0.755620  0.782143  0.754492   \n",
      "104        4            0.1   251  0.754551   0.755620  0.782143  0.754492   \n",
      "105        4            0.1   301  0.754551   0.755620  0.782143  0.754492   \n",
      "106        4            0.1   351  0.754551   0.755620  0.782143  0.754492   \n",
      "107        4            0.1   401  0.754551   0.755620  0.782143  0.754492   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "103  0.432891  \n",
      "104  0.432891  \n",
      "105  0.432891  \n",
      "106  0.432891  \n",
      "107  0.432891  \n",
      "\n",
      "[108 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "104        4            0.1   251  0.754551   0.755620  0.782143  0.754492   \n",
      "105        4            0.1   301  0.754551   0.755620  0.782143  0.754492   \n",
      "106        4            0.1   351  0.754551   0.755620  0.782143  0.754492   \n",
      "107        4            0.1   401  0.754551   0.755620  0.782143  0.754492   \n",
      "108        4            0.1   451  0.754551   0.755620  0.782143  0.754492   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "104  0.432891  \n",
      "105  0.432891  \n",
      "106  0.432891  \n",
      "107  0.432891  \n",
      "108  0.432891  \n",
      "\n",
      "[109 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "105        4            0.1   301  0.754551   0.755620  0.782143  0.754492   \n",
      "106        4            0.1   351  0.754551   0.755620  0.782143  0.754492   \n",
      "107        4            0.1   401  0.754551   0.755620  0.782143  0.754492   \n",
      "108        4            0.1   451  0.754551   0.755620  0.782143  0.754492   \n",
      "109        4            0.1   501  0.754551   0.755620  0.782143  0.754492   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "105  0.432891  \n",
      "106  0.432891  \n",
      "107  0.432891  \n",
      "108  0.432891  \n",
      "109  0.432891  \n",
      "\n",
      "[110 rows x 8 columns]\n",
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "106        4            0.1   351  0.754551   0.755620  0.782143  0.754492   \n",
      "107        4            0.1   401  0.754551   0.755620  0.782143  0.754492   \n",
      "108        4            0.1   451  0.754551   0.755620  0.782143  0.754492   \n",
      "109        4            0.1   501  0.754551   0.755620  0.782143  0.754492   \n",
      "110        4            0.3     1  0.603397   0.551159  0.665238  0.602224   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "106  0.432891  \n",
      "107  0.432891  \n",
      "108  0.432891  \n",
      "109  0.432891  \n",
      "110  0.696610  \n",
      "\n",
      "[111 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "107        4            0.1   401  0.754551   0.755620  0.782143  0.754492   \n",
      "108        4            0.1   451  0.754551   0.755620  0.782143  0.754492   \n",
      "109        4            0.1   501  0.754551   0.755620  0.782143  0.754492   \n",
      "110        4            0.3     1  0.603397   0.551159  0.665238  0.602224   \n",
      "111        4            0.3    51  0.621474   0.503263  0.601905  0.623847   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "107  0.432891  \n",
      "108  0.432891  \n",
      "109  0.432891  \n",
      "110  0.696610  \n",
      "111  0.613342  \n",
      "\n",
      "[112 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "108        4            0.1   451  0.754551   0.755620  0.782143  0.754492   \n",
      "109        4            0.1   501  0.754551   0.755620  0.782143  0.754492   \n",
      "110        4            0.3     1  0.603397   0.551159  0.665238  0.602224   \n",
      "111        4            0.3    51  0.621474   0.503263  0.601905  0.623847   \n",
      "112        4            0.3   101  0.646346   0.524253  0.671905  0.648847   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "108  0.432891  \n",
      "109  0.432891  \n",
      "110  0.696610  \n",
      "111  0.613342  \n",
      "112  0.613342  \n",
      "\n",
      "[113 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "109        4            0.1   501  0.754551   0.755620  0.782143  0.754492   \n",
      "110        4            0.3     1  0.603397   0.551159  0.665238  0.602224   \n",
      "111        4            0.3    51  0.621474   0.503263  0.601905  0.623847   \n",
      "112        4            0.3   101  0.646346   0.524253  0.671905  0.648847   \n",
      "113        4            0.3   151  0.646346   0.524253  0.671905  0.648847   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "109  0.432891  \n",
      "110  0.696610  \n",
      "111  0.613342  \n",
      "112  0.613342  \n",
      "113  0.613342  \n",
      "\n",
      "[114 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "110        4            0.3     1  0.603397   0.551159  0.665238  0.602224   \n",
      "111        4            0.3    51  0.621474   0.503263  0.601905  0.623847   \n",
      "112        4            0.3   101  0.646346   0.524253  0.671905  0.648847   \n",
      "113        4            0.3   151  0.646346   0.524253  0.671905  0.648847   \n",
      "114        4            0.3   201  0.646346   0.524253  0.671905  0.648847   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "110  0.696610  \n",
      "111  0.613342  \n",
      "112  0.613342  \n",
      "113  0.613342  \n",
      "114  0.613342  \n",
      "\n",
      "[115 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "111        4            0.3    51  0.621474   0.503263  0.601905  0.623847   \n",
      "112        4            0.3   101  0.646346   0.524253  0.671905  0.648847   \n",
      "113        4            0.3   151  0.646346   0.524253  0.671905  0.648847   \n",
      "114        4            0.3   201  0.646346   0.524253  0.671905  0.648847   \n",
      "115        4            0.3   251  0.646346   0.524253  0.671905  0.648847   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "111  0.613342  \n",
      "112  0.613342  \n",
      "113  0.613342  \n",
      "114  0.613342  \n",
      "115  0.613342  \n",
      "\n",
      "[116 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "112        4            0.3   101  0.646346   0.524253  0.671905  0.648847   \n",
      "113        4            0.3   151  0.646346   0.524253  0.671905  0.648847   \n",
      "114        4            0.3   201  0.646346   0.524253  0.671905  0.648847   \n",
      "115        4            0.3   251  0.646346   0.524253  0.671905  0.648847   \n",
      "116        4            0.3   301  0.646346   0.524253  0.671905  0.648847   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "112  0.613342  \n",
      "113  0.613342  \n",
      "114  0.613342  \n",
      "115  0.613342  \n",
      "116  0.613342  \n",
      "\n",
      "[117 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "113        4            0.3   151  0.646346   0.524253  0.671905  0.648847   \n",
      "114        4            0.3   201  0.646346   0.524253  0.671905  0.648847   \n",
      "115        4            0.3   251  0.646346   0.524253  0.671905  0.648847   \n",
      "116        4            0.3   301  0.646346   0.524253  0.671905  0.648847   \n",
      "117        4            0.3   351  0.646346   0.524253  0.671905  0.648847   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "113  0.613342  \n",
      "114  0.613342  \n",
      "115  0.613342  \n",
      "116  0.613342  \n",
      "117  0.613342  \n",
      "\n",
      "[118 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "114        4            0.3   201  0.646346   0.524253  0.671905  0.648847   \n",
      "115        4            0.3   251  0.646346   0.524253  0.671905  0.648847   \n",
      "116        4            0.3   301  0.646346   0.524253  0.671905  0.648847   \n",
      "117        4            0.3   351  0.646346   0.524253  0.671905  0.648847   \n",
      "118        4            0.3   401  0.646346   0.524253  0.671905  0.648847   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "114  0.613342  \n",
      "115  0.613342  \n",
      "116  0.613342  \n",
      "117  0.613342  \n",
      "118  0.613342  \n",
      "\n",
      "[119 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "115        4            0.3   251  0.646346   0.524253  0.671905  0.648847   \n",
      "116        4            0.3   301  0.646346   0.524253  0.671905  0.648847   \n",
      "117        4            0.3   351  0.646346   0.524253  0.671905  0.648847   \n",
      "118        4            0.3   401  0.646346   0.524253  0.671905  0.648847   \n",
      "119        4            0.3   451  0.646346   0.524253  0.671905  0.648847   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "115  0.613342  \n",
      "116  0.613342  \n",
      "117  0.613342  \n",
      "118  0.613342  \n",
      "119  0.613342  \n",
      "\n",
      "[120 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "116        4            0.3   301  0.646346   0.524253  0.671905  0.648847   \n",
      "117        4            0.3   351  0.646346   0.524253  0.671905  0.648847   \n",
      "118        4            0.3   401  0.646346   0.524253  0.671905  0.648847   \n",
      "119        4            0.3   451  0.646346   0.524253  0.671905  0.648847   \n",
      "120        4            0.3   501  0.646346   0.524253  0.671905  0.648847   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "116  0.613342  \n",
      "117  0.613342  \n",
      "118  0.613342  \n",
      "119  0.613342  \n",
      "120  0.613342  \n",
      "\n",
      "[121 rows x 8 columns]\n",
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "117        4            0.3   351  0.646346   0.524253  0.671905  0.648847   \n",
      "118        4            0.3   401  0.646346   0.524253  0.671905  0.648847   \n",
      "119        4            0.3   451  0.646346   0.524253  0.671905  0.648847   \n",
      "120        4            0.3   501  0.646346   0.524253  0.671905  0.648847   \n",
      "121        4            0.5     1  0.534872   0.247738  0.269524  0.539630   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "117  0.613342  \n",
      "118  0.613342  \n",
      "119  0.613342  \n",
      "120  0.613342  \n",
      "121  0.695213  \n",
      "\n",
      "[122 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "118        4            0.3   401  0.646346   0.524253  0.671905  0.648847   \n",
      "119        4            0.3   451  0.646346   0.524253  0.671905  0.648847   \n",
      "120        4            0.3   501  0.646346   0.524253  0.671905  0.648847   \n",
      "121        4            0.5     1  0.534872   0.247738  0.269524  0.539630   \n",
      "122        4            0.5    51  0.616026   0.441518  0.625476  0.616949   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "118  0.613342  \n",
      "119  0.613342  \n",
      "120  0.613342  \n",
      "121  0.695213  \n",
      "122  0.567777  \n",
      "\n",
      "[123 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "119        4            0.3   451  0.646346   0.524253  0.671905  0.648847   \n",
      "120        4            0.3   501  0.646346   0.524253  0.671905  0.648847   \n",
      "121        4            0.5     1  0.534872   0.247738  0.269524  0.539630   \n",
      "122        4            0.5    51  0.616026   0.441518  0.625476  0.616949   \n",
      "123        4            0.5   101  0.611026   0.433260  0.635000  0.611184   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "119  0.613342  \n",
      "120  0.613342  \n",
      "121  0.695213  \n",
      "122  0.567777  \n",
      "123  0.567777  \n",
      "\n",
      "[124 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "120        4            0.3   501  0.646346   0.524253  0.671905  0.648847   \n",
      "121        4            0.5     1  0.534872   0.247738  0.269524  0.539630   \n",
      "122        4            0.5    51  0.616026   0.441518  0.625476  0.616949   \n",
      "123        4            0.5   101  0.611026   0.433260  0.635000  0.611184   \n",
      "124        4            0.5   151  0.611026   0.433260  0.635000  0.611184   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "120  0.613342  \n",
      "121  0.695213  \n",
      "122  0.567777  \n",
      "123  0.567777  \n",
      "124  0.567777  \n",
      "\n",
      "[125 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "121        4            0.5     1  0.534872   0.247738  0.269524  0.539630   \n",
      "122        4            0.5    51  0.616026   0.441518  0.625476  0.616949   \n",
      "123        4            0.5   101  0.611026   0.433260  0.635000  0.611184   \n",
      "124        4            0.5   151  0.611026   0.433260  0.635000  0.611184   \n",
      "125        4            0.5   201  0.611026   0.433260  0.635000  0.611184   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "121  0.695213  \n",
      "122  0.567777  \n",
      "123  0.567777  \n",
      "124  0.567777  \n",
      "125  0.567777  \n",
      "\n",
      "[126 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "122        4            0.5    51  0.616026   0.441518  0.625476  0.616949   \n",
      "123        4            0.5   101  0.611026   0.433260  0.635000  0.611184   \n",
      "124        4            0.5   151  0.611026   0.433260  0.635000  0.611184   \n",
      "125        4            0.5   201  0.611026   0.433260  0.635000  0.611184   \n",
      "126        4            0.5   251  0.611026   0.433260  0.635000  0.611184   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "122  0.567777  \n",
      "123  0.567777  \n",
      "124  0.567777  \n",
      "125  0.567777  \n",
      "126  0.567777  \n",
      "\n",
      "[127 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "123        4            0.5   101  0.611026   0.433260  0.635000  0.611184   \n",
      "124        4            0.5   151  0.611026   0.433260  0.635000  0.611184   \n",
      "125        4            0.5   201  0.611026   0.433260  0.635000  0.611184   \n",
      "126        4            0.5   251  0.611026   0.433260  0.635000  0.611184   \n",
      "127        4            0.5   301  0.611026   0.433260  0.635000  0.611184   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "123  0.567777  \n",
      "124  0.567777  \n",
      "125  0.567777  \n",
      "126  0.567777  \n",
      "127  0.567777  \n",
      "\n",
      "[128 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "124        4            0.5   151  0.611026   0.433260  0.635000  0.611184   \n",
      "125        4            0.5   201  0.611026   0.433260  0.635000  0.611184   \n",
      "126        4            0.5   251  0.611026   0.433260  0.635000  0.611184   \n",
      "127        4            0.5   301  0.611026   0.433260  0.635000  0.611184   \n",
      "128        4            0.5   351  0.611026   0.433260  0.635000  0.611184   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "124  0.567777  \n",
      "125  0.567777  \n",
      "126  0.567777  \n",
      "127  0.567777  \n",
      "128  0.567777  \n",
      "\n",
      "[129 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "125        4            0.5   201  0.611026   0.433260  0.635000  0.611184   \n",
      "126        4            0.5   251  0.611026   0.433260  0.635000  0.611184   \n",
      "127        4            0.5   301  0.611026   0.433260  0.635000  0.611184   \n",
      "128        4            0.5   351  0.611026   0.433260  0.635000  0.611184   \n",
      "129        4            0.5   401  0.611026   0.433260  0.635000  0.611184   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "125  0.567777  \n",
      "126  0.567777  \n",
      "127  0.567777  \n",
      "128  0.567777  \n",
      "129  0.567777  \n",
      "\n",
      "[130 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "126        4            0.5   251  0.611026   0.433260  0.635000  0.611184   \n",
      "127        4            0.5   301  0.611026   0.433260  0.635000  0.611184   \n",
      "128        4            0.5   351  0.611026   0.433260  0.635000  0.611184   \n",
      "129        4            0.5   401  0.611026   0.433260  0.635000  0.611184   \n",
      "130        4            0.5   451  0.611026   0.433260  0.635000  0.611184   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "126  0.567777  \n",
      "127  0.567777  \n",
      "128  0.567777  \n",
      "129  0.567777  \n",
      "130  0.567777  \n",
      "\n",
      "[131 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topology  Learning Rate Epoch  Accuracy  Precision    Recall       AUC  \\\n",
      "0          1            0.1     1  0.517628   0.514106  0.995000  0.507763   \n",
      "1          1            0.1    51  0.712244   0.697077  0.777143  0.711071   \n",
      "2          1            0.1   101  0.707051   0.690100  0.776905  0.705689   \n",
      "3          1            0.1   151  0.732436   0.719749  0.796190  0.731385   \n",
      "4          1            0.1   201  0.719615   0.703911  0.771429  0.718346   \n",
      "..       ...            ...   ...       ...        ...       ...       ...   \n",
      "127        4            0.5   301  0.611026   0.433260  0.635000  0.611184   \n",
      "128        4            0.5   351  0.611026   0.433260  0.635000  0.611184   \n",
      "129        4            0.5   401  0.611026   0.433260  0.635000  0.611184   \n",
      "130        4            0.5   451  0.611026   0.433260  0.635000  0.611184   \n",
      "131        4            0.5   501  0.611026   0.433260  0.635000  0.611184   \n",
      "\n",
      "         Loss  \n",
      "0    0.777548  \n",
      "1    0.544762  \n",
      "2    0.519720  \n",
      "3    0.510739  \n",
      "4    0.519249  \n",
      "..        ...  \n",
      "127  0.567777  \n",
      "128  0.567777  \n",
      "129  0.567777  \n",
      "130  0.567777  \n",
      "131  0.567777  \n",
      "\n",
      "[132 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulq\\AppData\\Local\\Temp\\ipykernel_41684\\4011705171.py:146: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append(add_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, matthews_corrcoef, roc_curve, auc, precision_recall_curve\n",
    "from statistics import mean, stdev\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('./rtfDataSet.csv')\n",
    "x = df.iloc[:, selected_features]\n",
    "\n",
    "# Input_ y_Target_Variable.\n",
    "y = df.iloc[:, -1].astype('int')\n",
    "\n",
    "# Normalize\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "all_y_test = []\n",
    "all_y_pred = []\n",
    "\n",
    "\n",
    "learning_rates = [0.1, 0.3, 0.5]\n",
    "epoch_ranges = range(1, 502, 50)\n",
    "input_dim = len(selected_features) \n",
    "output_neurons = 1\n",
    "output_activation = 'logistic' \n",
    "\n",
    "data_frame = pd.DataFrame(columns=['Topology','Learning Rate', 'Epoch', 'Accuracy', 'Precision', 'Recall', 'AUC','Loss'])\n",
    "\n",
    "\n",
    "activation_functions = ['identity', 'logistic', 'tanh', 'relu']\n",
    "topologies = [\n",
    "    {\n",
    "        'name': \"1\",\n",
    "        'input_neurons': input_dim,\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_layers': [{'neurons': 15, 'activation': 'relu'}],\n",
    "        'output_neurons': output_neurons,\n",
    "        'output_activation': output_activation,\n",
    "        'activation_function': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'name': \"2\",\n",
    "        'input_neurons': input_dim,\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_layers': [\n",
    "            {'neurons': 30, 'activation': 'tanh'},\n",
    "            {'neurons': 15, 'activation': 'tanh'}\n",
    "        ],\n",
    "        'output_neurons': output_neurons,\n",
    "        'output_activation': output_activation,\n",
    "        'activation_function': 'tanh'\n",
    "    },\n",
    "    {\n",
    "        'name': \"3\",\n",
    "        'input_neurons': input_dim,\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_layers': [\n",
    "            {'neurons': 60, 'activation': 'logistic'},\n",
    "            {'neurons': 30, 'activation': 'logistic'},\n",
    "            {'neurons': 15, 'activation': 'logistic'}\n",
    "        ],\n",
    "        'output_neurons': output_neurons,\n",
    "        'output_activation': output_activation,\n",
    "        'activation_function': 'logistic'\n",
    "    },\n",
    "    {\n",
    "        'name': \"4\",\n",
    "        'input_neurons': input_dim,\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_layers': [\n",
    "            {'neurons': 120, 'activation': 'relu'},\n",
    "            {'neurons': 60, 'activation': 'relu'},\n",
    "            {'neurons': 30, 'activation': 'relu'},\n",
    "            {'neurons': 15, 'activation': 'relu'}\n",
    "        ],\n",
    "        'output_neurons': output_neurons,\n",
    "        'output_activation': output_activation,\n",
    "        'activation_function': 'relu'\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for topology in topologies:    \n",
    "    for learning_rate in learning_rates:\n",
    "        hidden_layer_sizes = tuple(layer['neurons'] for layer in topology['hidden_layers'])\n",
    "        activation_function = topology['activation_function']\n",
    "        for epoch in epoch_ranges:\n",
    "            \n",
    "            accu_list = []\n",
    "            precision_list = []\n",
    "            recall_list = []\n",
    "            auc_list = []\n",
    "\n",
    "            model = MLPClassifier(learning_rate_init = learning_rate,\n",
    "                                hidden_layer_sizes = hidden_layer_sizes,\n",
    "                                max_iter = epoch,\n",
    "                                random_state = 42,\n",
    "                                activation = activation_function,\n",
    "                                solver = 'sgd',\n",
    "                                # out_activation_ = output_activation,\n",
    "                                # n_outputs_ = output_neurons\n",
    "                                #n_features_in_ = input_dim\n",
    "                                )\n",
    "            \n",
    "            for train_index, test_index in skf.split(x_scaled, y):\n",
    "                x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "                y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "                # Fit the classifier to the training data\n",
    "                model.fit(x_train_fold, y_train_fold)\n",
    "\n",
    "                # Make predictions on the test data\n",
    "                y_pred = model.predict(x_test_fold)\n",
    "                #all_y_pred.extend(model.predict(x_train_fold))\n",
    "\n",
    "                # Evaluate the classifier's performance \n",
    "                accu_list.append(accuracy_score(y_test_fold, y_pred))\n",
    "                precision_list.append(precision_score(y_test_fold, y_pred))\n",
    "                recall_list.append(recall_score(y_test_fold, y_pred))\n",
    "                auc_list.append(roc_auc_score(y_test_fold, y_pred))\n",
    "\n",
    "            add_data = {\n",
    "                'Topology': topology['name'],\n",
    "                'Learning Rate': learning_rate,\n",
    "                'Epoch': epoch,\n",
    "                'Accuracy': mean(accu_list),\n",
    "                'Precision': mean(precision_list),\n",
    "                'Recall': mean(recall_list),\n",
    "                'AUC': mean(auc_list),\n",
    "                'Loss': model.loss_,\n",
    "            }\n",
    "\n",
    "            data_frame = data_frame.append(add_data, ignore_index=True)\n",
    "            print(data_frame)\n",
    "            \n",
    "            # print('Metrics Results:')\n",
    "            # print('-'*30)\n",
    "            # print(f\"Accuracy: {mean(accu_list)*100:.2f}% ± {stdev(accu_list):.2f}\")\n",
    "            # print(f\"Precision: {mean(precision_list)*100:.2f}% ± {stdev(precision_list):.2f}\")\n",
    "            # print(f\"Recall: {mean(recall_list)*100:.2f}% ± {stdev(recall_list):.2f}\")\n",
    "            # print(f\"AUC: {mean(auc_list)*100:.2f}% ± {stdev(auc_list):.2f}\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Calcular sensibilidad y especificidad\n",
    "# tn, fp, fn, tp = confusion_matrix(y_test_fold, y_pred).ravel()\n",
    "        \n",
    "\n",
    "# # Plot Confusion Matrix \n",
    "# matrix = confusion_matrix(y_test_fold, y_pred)\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.set(font_scale=1.4)  # for label size\n",
    "# sns.heatmap(matrix, annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
    "# plt.xlabel('True')\n",
    "# plt.ylabel('Predicted')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot ROC Curve\n",
    "# fpr, tpr, _ = roc_curve(y_test_fold, y_pred)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# plt.figure()\n",
    "# lw = 2\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title(f'ROC - Criterion')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "\n",
    "# # Plot Precision vs Recall\n",
    "# precision, recall, thresholds = precision_recall_curve(y_test_fold, y_pred)\n",
    "# # precision, recall, thresholds = precision_recall_curve(all_y_test, all_y_pred)\n",
    "# plt.figure()\n",
    "# plt.plot(recall, precision, color='blue', lw=lw)\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title(f'Precision vs Recall - Criterion')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7592919799498747"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['AUC'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topology                2\n",
       "Learning Rate         0.1\n",
       "Epoch                 201\n",
       "Accuracy         0.759744\n",
       "Precision        0.758797\n",
       "Recall           0.780952\n",
       "AUC              0.759292\n",
       "Loss             0.419301\n",
       "Name: 37, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.loc[data_frame['AUC'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[164  30]\n",
      " [ 37 165]]\n",
      "AUC 0.8310962539552924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (201) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encontrar el modelo con el mayor valor promedio de AUC\n",
    "best_model_row = data_frame.loc[data_frame['AUC'].idxmax()]\n",
    "best_topology = int(best_model_row['Topology'])\n",
    "best_learning_rate = best_model_row['Learning Rate']\n",
    "best_epoch = int(best_model_row['Epoch'])\n",
    "\n",
    "x_train, y_train, x_test, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el mejor modelo con los parámetros encontrados\n",
    "best_topology_params = topologies[best_topology - 1]\n",
    "best_hidden_layer_sizes = tuple(layer['neurons'] for layer in best_topology_params['hidden_layers'])\n",
    "best_activation_function = best_topology_params['activation_function']\n",
    "\n",
    "best_model = MLPClassifier(learning_rate_init=best_learning_rate,\n",
    "                           hidden_layer_sizes=best_hidden_layer_sizes,\n",
    "                           max_iter=best_epoch,\n",
    "                           solver='sgd',\n",
    "                           random_state=42,\n",
    "                           activation=best_activation_function)\n",
    "\n",
    "\n",
    "# Entrenar el mejor modelo con todos los datos de entrenamiento\n",
    "best_model.fit(x_scaled, y)\n",
    "\n",
    "# 3. Mostrar la matriz de confusión obtenida para el modelo seleccionado\n",
    "y_pred = best_model.predict(x_scaled)\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"AUC\", roc_auc_score(y, y_pred))\n",
    "\n",
    "# # Plot Confusion Matrix \n",
    "plt.figure(figsize=(10,7))\n",
    "sns.set(font_scale=1.4)  # for label size\n",
    "sns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# 4. Mostrar el plot del área bajo la curva ROC para el modelo seleccionado\n",
    "fpr, tpr, _ = roc_curve(y, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Mostrar un plot de (precision vs recall) para el modelo seleccionado\n",
    "precision, recall, thresholds = precision_recall_curve(y, y_pred)\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs Recall')\n",
    "plt.show()\n",
    "\n",
    "# 6. Mostrar un plot de mean of loss function vs epochs para el modelo seleccionado\n",
    "loss_values = best_model.loss_curve_\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(loss_values, color='blue', linewidth=2, label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
